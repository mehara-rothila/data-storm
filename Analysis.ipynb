{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Improved Kaggle Winner Solution for Data Storm v6.0 competition\n",
    "Fixed data leakage issues and improved model robustness\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "4bf2e105c1ce42c5",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    start_time = time.time()\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"IMPROVED KAGGLE WINNER SOLUTION - FIXED DATA LEAKAGE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Starting at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = os.path.join('D:\\\\', 'DATA STORM', 'outputs')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Load data\n",
    "    print(\"Step 1: Loading data...\")\n",
    "    data_dir = os.path.join('D:\\\\', 'DATA STORM', 'dataset')\n",
    "    train_df = pd.read_csv(os.path.join(data_dir, 'train_storming_round.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, 'test_storming_round.csv'))\n",
    "\n",
    "    print(f\"Train data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "    # Basic preprocessing\n",
    "    print(\"\\nStep 2: Improved preprocessing...\")\n",
    "\n",
    "    # Create target column for train data (if agent sells next month)\n",
    "    # Important: we need to create this WITHOUT leaking future data\n",
    "    # Let's get unique agents and their monthly data\n",
    "    train_df['year_month'] = pd.to_datetime(train_df['year_month'])\n",
    "    train_df = train_df.sort_values(['agent_code', 'year_month'])\n",
    "\n",
    "    # Create target by looking ahead one month for each agent\n",
    "    train_df['target_column'] = 0  # Default to 0 (will go NILL)\n",
    "\n",
    "    # Get unique agents and process each\n",
    "    unique_agents = train_df['agent_code'].unique()\n",
    "    for agent in unique_agents:\n",
    "        agent_data = train_df[train_df['agent_code'] == agent].copy()\n",
    "        agent_data = agent_data.sort_values('year_month')\n",
    "\n",
    "        # For each month, check if agent sells anything in the next month\n",
    "        for i in range(len(agent_data) - 1):\n",
    "            current_row_id = agent_data.iloc[i]['row_id']\n",
    "            next_month_sales = agent_data.iloc[i+1]['new_policy_count']\n",
    "\n",
    "            # If they sell anything next month, target is 1 (not NILL)\n",
    "            if next_month_sales > 0:\n",
    "                train_df.loc[train_df['row_id'] == current_row_id, 'target_column'] = 1\n",
    "\n",
    "    # Remove the last month record for each agent as we don't have next month data\n",
    "    last_month_indices = []\n",
    "    for agent in unique_agents:\n",
    "        agent_data = train_df[train_df['agent_code'] == agent]\n",
    "        last_month_idx = agent_data.iloc[-1].name\n",
    "        last_month_indices.append(last_month_idx)\n",
    "\n",
    "    train_df = train_df.drop(last_month_indices)\n",
    "\n",
    "    print(f\"Processed training data shape: {train_df.shape}\")\n",
    "    print(f\"Target distribution: {train_df['target_column'].value_counts()}\")\n",
    "\n",
    "    # Convert date columns to datetime and extract features\n",
    "    date_columns = ['agent_join_month', 'first_policy_sold_month', 'year_month']\n",
    "    for df in [train_df, test_df]:\n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "                # Extract month and year\n",
    "                df[f'{col}_month'] = df[col].dt.month\n",
    "                df[f'{col}_year'] = df[col].dt.year\n",
    "\n",
    "    # Feature engineering\n",
    "    print(\"\\nStep 3: Leak-free feature engineering...\")\n",
    "\n",
    "    # Create features that don't leak target information\n",
    "    for df in [train_df, test_df]:\n",
    "        # Experience features\n",
    "        if all(col in df.columns for col in ['year_month', 'agent_join_month']):\n",
    "            df['months_with_company'] = ((df['year_month'].dt.year - df['agent_join_month'].dt.year) * 12 +\n",
    "                                        (df['year_month'].dt.month - df['agent_join_month'].dt.month))\n",
    "\n",
    "        if all(col in df.columns for col in ['first_policy_sold_month', 'agent_join_month']):\n",
    "            df['months_to_first_sale'] = ((df['first_policy_sold_month'].dt.year - df['agent_join_month'].dt.year) * 12 +\n",
    "                                        (df['first_policy_sold_month'].dt.month - df['agent_join_month'].dt.month))\n",
    "            # Fill if agent hasn't sold yet\n",
    "            df['months_to_first_sale'] = df['months_to_first_sale'].fillna(-1)\n",
    "\n",
    "        if all(col in df.columns for col in ['year_month', 'first_policy_sold_month']):\n",
    "            df['months_since_first_sale'] = ((df['year_month'].dt.year - df['first_policy_sold_month'].dt.year) * 12 +\n",
    "                                          (df['year_month'].dt.month - df['first_policy_sold_month'].dt.month))\n",
    "            # Fill if agent hasn't sold yet\n",
    "            df['months_since_first_sale'] = df['months_since_first_sale'].fillna(-1)\n",
    "\n",
    "        # Activity trend features (avoid using features derived from target)\n",
    "        if all(col in df.columns for col in ['unique_proposals_last_7_days', 'unique_proposals_last_15_days']):\n",
    "            df['proposal_trend_7_15'] = df['unique_proposals_last_7_days'] / np.maximum(df['unique_proposals_last_15_days'], 1)\n",
    "\n",
    "        if all(col in df.columns for col in ['unique_proposals_last_15_days', 'unique_proposals_last_21_days']):\n",
    "            df['proposal_trend_15_21'] = df['unique_proposals_last_15_days'] / np.maximum(df['unique_proposals_last_21_days'], 1)\n",
    "\n",
    "        if all(col in df.columns for col in ['unique_quotations_last_7_days', 'unique_quotations_last_15_days']):\n",
    "            df['quotation_trend_7_15'] = df['unique_quotations_last_7_days'] / np.maximum(df['unique_quotations_last_15_days'], 1)\n",
    "\n",
    "        if all(col in df.columns for col in ['unique_quotations_last_15_days', 'unique_quotations_last_21_days']):\n",
    "            df['quotation_trend_15_21'] = df['unique_quotations_last_15_days'] / np.maximum(df['unique_quotations_last_21_days'], 1)\n",
    "\n",
    "        # Activity consistency (variance-based)\n",
    "        if all(col in df.columns for col in ['unique_proposals_last_7_days', 'unique_proposals_last_15_days', 'unique_proposals_last_21_days']):\n",
    "            proposal_cols = ['unique_proposals_last_7_days', 'unique_proposals_last_15_days', 'unique_proposals_last_21_days']\n",
    "            df['proposal_variance'] = df[proposal_cols].var(axis=1)\n",
    "            df['proposal_consistency'] = 1 / (1 + df['proposal_variance'])\n",
    "\n",
    "        if all(col in df.columns for col in ['unique_quotations_last_7_days', 'unique_quotations_last_15_days', 'unique_quotations_last_21_days']):\n",
    "            quotation_cols = ['unique_quotations_last_7_days', 'unique_quotations_last_15_days', 'unique_quotations_last_21_days']\n",
    "            df['quotation_variance'] = df[quotation_cols].var(axis=1)\n",
    "            df['quotation_consistency'] = 1 / (1 + df['quotation_variance'])\n",
    "\n",
    "        # Current period activity rates\n",
    "        if all(col in df.columns for col in ['unique_customers', 'unique_proposal']):\n",
    "            df['proposals_per_customer'] = df['unique_proposal'] / np.maximum(df['unique_customers'], 1)\n",
    "\n",
    "        if all(col in df.columns for col in ['unique_customers', 'unique_quotations']):\n",
    "            df['quotations_per_customer'] = df['unique_quotations'] / np.maximum(df['unique_customers'], 1)\n",
    "\n",
    "        # Time-based seasonality features\n",
    "        if 'year_month_month' in df.columns:\n",
    "            df['is_quarter_end'] = df['year_month_month'].isin([3, 6, 9, 12]).astype(int)\n",
    "            df['month_sin'] = np.sin(2 * np.pi * df['year_month_month']/12)\n",
    "            df['month_cos'] = np.cos(2 * np.pi * df['year_month_month']/12)\n",
    "\n",
    "        # Ratios of activity metrics (without direct policy count info)\n",
    "        if all(col in df.columns for col in ['unique_proposal', 'unique_quotations']):\n",
    "            df['quotation_to_proposal_ratio'] = df['unique_quotations'] / np.maximum(df['unique_proposal'], 1)\n",
    "\n",
    "        # Cash payment ratio if available\n",
    "        if all(col in df.columns for col in ['number_of_cash_payment_policies', 'number_of_policy_holders']):\n",
    "            df['cash_payment_ratio'] = df['number_of_cash_payment_policies'] / np.maximum(df['number_of_policy_holders'], 1)\n",
    "\n",
    "        # Additional agent characteristic features\n",
    "        if 'agent_age' in df.columns:\n",
    "            df['agent_age_squared'] = df['agent_age'] ** 2\n",
    "\n",
    "        # Interaction features\n",
    "        if all(col in df.columns for col in ['agent_age', 'months_with_company']):\n",
    "            df['age_experience_interaction'] = df['agent_age'] * df['months_with_company']\n",
    "\n",
    "        # Agent velocity metrics (short term vs long term activity)\n",
    "        if all(col in df.columns for col in ['unique_proposals_last_7_days', 'unique_proposal']):\n",
    "            df['proposal_velocity'] = df['unique_proposals_last_7_days'] / np.maximum(df['unique_proposal'], 1)\n",
    "\n",
    "        if all(col in df.columns for col in ['unique_quotations_last_7_days', 'unique_quotations']):\n",
    "            df['quotation_velocity'] = df['unique_quotations_last_7_days'] / np.maximum(df['unique_quotations'], 1)\n",
    "\n",
    "        if all(col in df.columns for col in ['unique_customers_last_7_days', 'unique_customers']):\n",
    "            df['customer_velocity'] = df['unique_customers_last_7_days'] / np.maximum(df['unique_customers'], 1)\n",
    "\n",
    "        # Feature transformations for key metrics\n",
    "        for col in ['unique_proposal', 'unique_quotations', 'unique_customers']:\n",
    "            if col in df.columns:\n",
    "                df[f'log_{col}'] = np.log1p(df[col])\n",
    "\n",
    "    # Add agent historical performance features (without leakage)\n",
    "    # We'll use rolling windows of past performance\n",
    "\n",
    "    # Create historical features for train and test separately\n",
    "    full_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    full_data = full_data.sort_values(['agent_code', 'year_month'])\n",
    "\n",
    "    # Initialize columns to store historical features\n",
    "    historical_features = [\n",
    "        'hist_avg_proposals',\n",
    "        'hist_avg_quotations',\n",
    "        'hist_avg_customers',\n",
    "        'hist_proposal_growth',\n",
    "        'hist_quotation_growth',\n",
    "        'hist_customer_growth',\n",
    "        'hist_consistency_score',\n",
    "        'months_since_last_activity'\n",
    "    ]\n",
    "\n",
    "    for feature in historical_features:\n",
    "        full_data[feature] = 0\n",
    "\n",
    "    # Process each agent\n",
    "    for agent in full_data['agent_code'].unique():\n",
    "        agent_data = full_data[full_data['agent_code'] == agent].copy()\n",
    "        agent_data = agent_data.sort_values('year_month')\n",
    "\n",
    "        # Skip if agent has only one record\n",
    "        if len(agent_data) <= 1:\n",
    "            continue\n",
    "\n",
    "        for i in range(1, len(agent_data)):\n",
    "            # Get all previous months\n",
    "            past_data = agent_data.iloc[:i]\n",
    "\n",
    "            # Historical averages\n",
    "            full_data.loc[agent_data.iloc[i].name, 'hist_avg_proposals'] = past_data['unique_proposal'].mean()\n",
    "            full_data.loc[agent_data.iloc[i].name, 'hist_avg_quotations'] = past_data['unique_quotations'].mean()\n",
    "            full_data.loc[agent_data.iloc[i].name, 'hist_avg_customers'] = past_data['unique_customers'].mean()\n",
    "\n",
    "            # Growth metrics (vs previous month)\n",
    "            if i >= 2:\n",
    "                # Fix: use np.maximum instead of .replace\n",
    "                prev_proposal = agent_data.iloc[i-1]['unique_proposal']\n",
    "                prev_prev_proposal = np.maximum(agent_data.iloc[i-2]['unique_proposal'], 1)\n",
    "                full_data.loc[agent_data.iloc[i].name, 'hist_proposal_growth'] = (prev_proposal / prev_prev_proposal) - 1\n",
    "\n",
    "                prev_quotation = agent_data.iloc[i-1]['unique_quotations']\n",
    "                prev_prev_quotation = np.maximum(agent_data.iloc[i-2]['unique_quotations'], 1)\n",
    "                full_data.loc[agent_data.iloc[i].name, 'hist_quotation_growth'] = (prev_quotation / prev_prev_quotation) - 1\n",
    "\n",
    "                prev_customer = agent_data.iloc[i-1]['unique_customers']\n",
    "                prev_prev_customer = np.maximum(agent_data.iloc[i-2]['unique_customers'], 1)\n",
    "                full_data.loc[agent_data.iloc[i].name, 'hist_customer_growth'] = (prev_customer / prev_prev_customer) - 1\n",
    "\n",
    "            # Consistency score (coefficient of variation)\n",
    "            if len(past_data) >= 3:\n",
    "                proposal_cv = past_data['unique_proposal'].std() / (past_data['unique_proposal'].mean() + 1)\n",
    "                quotation_cv = past_data['unique_quotations'].std() / (past_data['unique_quotations'].mean() + 1)\n",
    "                full_data.loc[agent_data.iloc[i].name, 'hist_consistency_score'] = 1 / (1 + (proposal_cv + quotation_cv)/2)\n",
    "\n",
    "            # Months since last activity (proposals or quotations)\n",
    "            last_month_active = False\n",
    "            if i >= 1:\n",
    "                if agent_data.iloc[i-1]['unique_proposal'] > 0 or agent_data.iloc[i-1]['unique_quotations'] > 0:\n",
    "                    last_month_active = True\n",
    "\n",
    "            full_data.loc[agent_data.iloc[i].name, 'months_since_last_activity'] = 0 if last_month_active else 1\n",
    "\n",
    "    # Merge historical features back\n",
    "    train_df = pd.merge(train_df, full_data[['row_id'] + historical_features], on='row_id', how='left')\n",
    "    test_df = pd.merge(test_df, full_data[['row_id'] + historical_features], on='row_id', how='left')\n",
    "\n",
    "    # Fill NAs in historical features\n",
    "    for df in [train_df, test_df]:\n",
    "        for feature in historical_features:\n",
    "            df[feature] = df[feature].fillna(0)\n",
    "\n",
    "    # Model training with proper time-series cross-validation\n",
    "    print(\"\\nStep 4: Model training with time-series validation...\")\n",
    "\n",
    "    # Select features\n",
    "    base_features = [\n",
    "        'agent_age',\n",
    "        'agent_age_squared',\n",
    "        'unique_proposal',\n",
    "        'unique_quotations',\n",
    "        'unique_customers',\n",
    "        'unique_proposals_last_7_days',\n",
    "        'unique_proposals_last_15_days',\n",
    "        'unique_proposals_last_21_days',\n",
    "        'unique_quotations_last_7_days',\n",
    "        'unique_quotations_last_15_days',\n",
    "        'unique_quotations_last_21_days',\n",
    "        'unique_customers_last_7_days',\n",
    "        'unique_customers_last_15_days',\n",
    "        'unique_customers_last_21_days',\n",
    "        'ANBP_value',\n",
    "        'net_income',\n",
    "        'number_of_policy_holders',\n",
    "        'number_of_cash_payment_policies'\n",
    "    ]\n",
    "\n",
    "    # Add engineered features\n",
    "    engineered_features = [\n",
    "        'months_with_company',\n",
    "        'months_to_first_sale',\n",
    "        'months_since_first_sale',\n",
    "        'proposal_trend_7_15',\n",
    "        'proposal_trend_15_21',\n",
    "        'quotation_trend_7_15',\n",
    "        'quotation_trend_15_21',\n",
    "        'proposal_variance',\n",
    "        'proposal_consistency',\n",
    "        'quotation_variance',\n",
    "        'quotation_consistency',\n",
    "        'proposals_per_customer',\n",
    "        'quotations_per_customer',\n",
    "        'is_quarter_end',\n",
    "        'month_sin',\n",
    "        'month_cos',\n",
    "        'quotation_to_proposal_ratio',\n",
    "        'cash_payment_ratio',\n",
    "        'age_experience_interaction',\n",
    "        'log_unique_proposal',\n",
    "        'log_unique_quotations',\n",
    "        'log_unique_customers',\n",
    "        'proposal_velocity',\n",
    "        'quotation_velocity',\n",
    "        'customer_velocity'\n",
    "    ]\n",
    "\n",
    "    # Add historical features\n",
    "    combined_features = base_features + [f for f in engineered_features if f in train_df.columns] + historical_features\n",
    "\n",
    "    # Remove any features that might not exist after preprocessing\n",
    "    features_to_use = [f for f in combined_features if f in train_df.columns and f in test_df.columns]\n",
    "\n",
    "    print(f\"Using {len(features_to_use)} features for modeling\")\n",
    "\n",
    "    # Split data for training\n",
    "    X = train_df[features_to_use].copy()\n",
    "    y = train_df['target_column'].copy()\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().any():\n",
    "            if X[col].dtype == 'object':\n",
    "                X[col] = X[col].fillna('unknown')\n",
    "            else:\n",
    "                X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "    # Set up time-based cross-validation\n",
    "    # Use TimeSeriesSplit to simulate forecasting future months\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # Train models with time-series cross-validation\n",
    "    cv_scores = []\n",
    "    models = []\n",
    "    importance_dfs = []\n",
    "\n",
    "    fold = 1\n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        print(f\"\\nTraining fold {fold} with {len(X_train)} train samples, {len(X_val)} validation samples\")\n",
    "        print(f\"Validation target distribution: {y_val.value_counts()}\")\n",
    "        fold += 1\n",
    "\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "        # 1. Random Forest with balanced class weight\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=8,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=4,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "\n",
    "        # 2. Gradient Boosting with scale_pos_weight\n",
    "        gb_model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # 3. XGBoost with balanced class weights\n",
    "        pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            min_child_weight=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            gamma=0.1,\n",
    "            scale_pos_weight=pos_weight,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "\n",
    "        # 4. LightGBM with class balancing\n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            num_leaves=20,\n",
    "            min_child_samples=20,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            verbose=-1\n",
    "        )\n",
    "\n",
    "        # Create ensemble model with class balancing\n",
    "        ensemble_model = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_model),\n",
    "                ('gb', gb_model),\n",
    "                ('xgb', xgb_model),\n",
    "                ('lgb', lgb_model)\n",
    "            ],\n",
    "            voting='soft',\n",
    "            weights=[1, 1.5, 2, 1.5]  # Give more weight to boosting models\n",
    "        )\n",
    "\n",
    "        # Train ensemble on scaled data\n",
    "        ensemble_model.fit(X_train_scaled, y_train)\n",
    "        models.append((scaler, ensemble_model))\n",
    "\n",
    "        # Evaluate model with proper metrics\n",
    "        y_val_pred = ensemble_model.predict(X_val_scaled)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        val_precision = precision_score(y_val, y_val_pred, zero_division=0)\n",
    "        val_recall = recall_score(y_val, y_val_pred, zero_division=0)\n",
    "        val_f1 = f1_score(y_val, y_val_pred, zero_division=0)\n",
    "\n",
    "        # Calculate ROC AUC\n",
    "        y_val_proba = ensemble_model.predict_proba(X_val_scaled)[:, 1]\n",
    "        val_roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "        cv_scores.append({\n",
    "            'accuracy': val_accuracy,\n",
    "            'precision': val_precision,\n",
    "            'recall': val_recall,\n",
    "            'f1': val_f1,\n",
    "            'roc_auc': val_roc_auc\n",
    "        })\n",
    "\n",
    "        print(f\"Fold Results:\")\n",
    "        print(f\"  Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  Precision: {val_precision:.4f}\")\n",
    "        print(f\"  Recall: {val_recall:.4f}\")\n",
    "        print(f\"  F1 Score: {val_f1:.4f}\")\n",
    "        print(f\"  ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "        # Get feature importance from the RF model\n",
    "        rf_importance = pd.DataFrame({\n",
    "            'Feature': features_to_use,\n",
    "            'Importance': ensemble_model.named_estimators_['rf'].feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        importance_dfs.append(rf_importance)\n",
    "\n",
    "    # Compute average scores\n",
    "    avg_scores = {metric: np.mean([score[metric] for score in cv_scores]) for metric in cv_scores[0].keys()}\n",
    "    std_scores = {metric: np.std([score[metric] for score in cv_scores]) for metric in cv_scores[0].keys()}\n",
    "\n",
    "    print(\"\\nAverage Cross-Validation Scores:\")\n",
    "    for metric, value in avg_scores.items():\n",
    "        print(f\"  {metric}: {value:.4f} ± {std_scores[metric]:.4f}\")\n",
    "\n",
    "    # Combine feature importance from all folds\n",
    "    combined_importance = pd.concat(importance_dfs)\n",
    "    avg_importance = combined_importance.groupby('Feature')['Importance'].mean().reset_index()\n",
    "    avg_importance = avg_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "    print(\"\\nTop 15 important features:\")\n",
    "    print(avg_importance.head(15))\n",
    "\n",
    "    # Save feature importance plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=avg_importance.head(20))\n",
    "    plt.title('Feature Importance (Average Across Folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'feature_importance.png'))\n",
    "\n",
    "    # Final model training on all data\n",
    "    print(\"\\nStep 5: Training final model on all data...\")\n",
    "\n",
    "    # Scale all training data\n",
    "    final_scaler = StandardScaler()\n",
    "    X_scaled = final_scaler.fit_transform(X)\n",
    "\n",
    "    # Train the ensemble model on all data\n",
    "    final_rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        max_features='sqrt',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "    final_gb = GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    final_pos_weight = (y == 0).sum() / (y == 1).sum()\n",
    "    final_xgb = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        min_child_weight=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        gamma=0.1,\n",
    "        scale_pos_weight=final_pos_weight,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "    final_lgb = lgb.LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        num_leaves=20,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    final_ensemble = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('rf', final_rf),\n",
    "            ('gb', final_gb),\n",
    "            ('xgb', final_xgb),\n",
    "            ('lgb', final_lgb)\n",
    "        ],\n",
    "        voting='soft',\n",
    "        weights=[1, 1.5, 2, 1.5]\n",
    "    )\n",
    "\n",
    "    final_ensemble.fit(X_scaled, y)\n",
    "\n",
    "    # Generate predictions for test set\n",
    "    print(\"\\nStep 6: Generating test predictions...\")\n",
    "\n",
    "    # Prepare test features\n",
    "    X_test = test_df[features_to_use].copy()\n",
    "\n",
    "    # Fill missing values\n",
    "    for col in X_test.columns:\n",
    "        if X_test[col].isnull().any():\n",
    "            if X_test[col].dtype == 'object':\n",
    "                X_test[col] = X_test[col].fillna('unknown')\n",
    "            else:\n",
    "                X_test[col] = X_test[col].fillna(X[col].median())\n",
    "\n",
    "    # Scale test data\n",
    "    X_test_scaled = final_scaler.transform(X_test)\n",
    "\n",
    "    # Make probability predictions\n",
    "    test_proba = final_ensemble.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # Try different thresholds and create multiple submission files\n",
    "    thresholds = np.arange(0.3, 0.71, 0.05)  # 0.3, 0.35, 0.4, ..., 0.7\n",
    "    submission_template = pd.read_csv(os.path.join(data_dir, 'sample_submission_storming_round.csv'))\n",
    "\n",
    "    prediction_counts = {}\n",
    "    for threshold in thresholds:\n",
    "        # Apply threshold\n",
    "        test_predictions = (test_proba >= threshold).astype(int)\n",
    "\n",
    "        # Track counts\n",
    "        sell_count = test_predictions.sum()\n",
    "        nill_count = len(test_predictions) - sell_count\n",
    "        prediction_counts[threshold] = {'sell': sell_count, 'nill': nill_count}\n",
    "\n",
    "        # Create submission\n",
    "        submission = submission_template.copy()\n",
    "        submission['target_column'] = test_predictions\n",
    "\n",
    "        # Save submission\n",
    "        submission_path = os.path.join(output_dir, f'submission_threshold_{threshold:.2f}.csv')\n",
    "        submission.to_csv(submission_path, index=False)\n",
    "\n",
    "        print(f\"Threshold {threshold:.2f}: {sell_count} non-NILL ({sell_count/len(test_predictions):.1%}), {nill_count} NILL ({nill_count/len(test_predictions):.1%})\")\n",
    "\n",
    "    # Determine optimal threshold based on class distribution in training data\n",
    "    train_pos_rate = train_df['target_column'].mean()\n",
    "    print(f\"\\nTraining data positive rate: {train_pos_rate:.4f}\")\n",
    "\n",
    "    # Find threshold that gives closest match to training distribution\n",
    "    closest_threshold = min(thresholds, key=lambda x: abs(prediction_counts[x]['sell']/len(test_predictions) - train_pos_rate))\n",
    "    print(f\"Optimal threshold based on training distribution: {closest_threshold:.2f}\")\n",
    "\n",
    "    # Save optimal submission\n",
    "    optimal_predictions = (test_proba >= closest_threshold).astype(int)\n",
    "    optimal_submission = submission_template.copy()\n",
    "    optimal_submission['target_column'] = optimal_predictions\n",
    "    optimal_submission_path = os.path.join(output_dir, 'submission.csv')\n",
    "    optimal_submission.to_csv(optimal_submission_path, index=False)\n",
    "\n",
    "    print(f\"\\nOptimal submission file created: {optimal_submission_path}\")\n",
    "    print(f\"Optimal prediction counts: {pd.Series(optimal_predictions).value_counts()}\")\n",
    "    print(f\"Optimal prediction rate: {optimal_predictions.sum()/len(optimal_predictions):.2%} non-NILL\")\n",
    "\n",
    "    # Save models and probabilities for further analysis\n",
    "    joblib.dump((final_scaler, final_ensemble), os.path.join(output_dir, 'best_model.pkl'))\n",
    "    np.save(os.path.join(output_dir, 'test_probabilities.npy'), test_proba)\n",
    "\n",
    "    # Completion\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Improved Kaggle Solution completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total execution time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "    print(f\"Optimal submission: {optimal_submission_path}\")\n",
    "    print(f\"Additional submissions with different thresholds are in the outputs directory\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nSUBMIT THE OPTIMAL FILE TO KAGGLE FOR #1 POSITION!\")\n",
    "    print(\"=\" * 80)"
   ],
   "id": "a4b0d73938776fd9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
