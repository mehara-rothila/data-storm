{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T12:23:23.178469Z",
     "start_time": "2025-05-07T12:17:35.913611Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "CHAMPIONSHIP MODEL - Insurance Agent NILL Prediction\n",
    "Data Storm v6.0 - First Place Solution (with Optuna HPO)\n",
    "\n",
    "Key enhancements:\n",
    "1. Stratified time-series cross-validation with gap\n",
    "2. Feature importance-based selection with stability analysis\n",
    "3. CatBoost integration with custom loss function\n",
    "4. Agent-specific dynamic thresholding\n",
    "5. Recursive feature elimination with stability scores\n",
    "6. Optuna for Hyperparameter Optimization\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import optuna # Optuna import\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO) # Optuna verbosity\n",
    "\n",
    "# Get relative paths\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    script_dir = os.getcwd() # Fallback for interactive environments\n",
    "\n",
    "data_dir = os.path.join(script_dir, 'dataset')\n",
    "output_dir = os.path.join(script_dir, 'outputs')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"CHAMPIONSHIP KAGGLE SOLUTION - ADVANCED ENSEMBLE WITH CUSTOM AGENT PROFILING & OPTUNA HPO\")\n",
    "print(\"=\" * 100)\n",
    "start_time = time.time()\n",
    "print(f\"Starting at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Load data with integrity checks\n",
    "print(\"\\nStep 1: Loading data with enhanced checks...\")\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'train_storming_round.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'test_storming_round.csv'))\n",
    "submission_template = pd.read_csv(os.path.join(data_dir, 'sample_submission_storming_round.csv'))\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Submission template shape: {submission_template.shape}\")\n",
    "\n",
    "# Critical integrity checks and deduplications\n",
    "print(\"Performing data integrity checks...\")\n",
    "assert len(test_df) == len(submission_template), \"Test and submission sizes don't match!\"\n",
    "\n",
    "# Check for duplicates in train data\n",
    "dupes_train = train_df.duplicated().sum()\n",
    "if dupes_train > 0:\n",
    "    print(f\"WARNING: Found {dupes_train} duplicate rows in training data. Removing...\")\n",
    "    train_df = train_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Check for duplicates in test data\n",
    "dupes_test = test_df.duplicated().sum()\n",
    "if dupes_test > 0:\n",
    "    print(f\"WARNING: Found {dupes_test} duplicate rows in test data. Removing...\")\n",
    "    test_df = test_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Advanced preprocessing\n",
    "print(\"\\nStep 2: Enhanced preprocessing with domain expertise...\")\n",
    "date_columns = ['agent_join_month', 'first_policy_sold_month', 'year_month']\n",
    "for df in [train_df, test_df]:\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "# Create better target variable (looking ahead one month)\n",
    "train_df = train_df.sort_values(['agent_code', 'year_month'])\n",
    "train_df['target_column'] = 0  # Default to 0 (will go NILL)\n",
    "\n",
    "# Get unique agents and process each for sophisticated target creation\n",
    "unique_agents = train_df['agent_code'].unique()\n",
    "for agent in unique_agents:\n",
    "    agent_data = train_df[train_df['agent_code'] == agent].copy()\n",
    "    agent_data = agent_data.sort_values('year_month')\n",
    "\n",
    "    # For each month, check if agent sells anything in the next month\n",
    "    for i in range(len(agent_data) - 1):\n",
    "        current_row_id = agent_data.iloc[i]['row_id']\n",
    "        next_month_sales = agent_data.iloc[i+1]['new_policy_count']\n",
    "\n",
    "        # If they sell anything next month, target is 1 (not NILL)\n",
    "        if next_month_sales > 0:\n",
    "            train_df.loc[train_df['row_id'] == current_row_id, 'target_column'] = 1\n",
    "\n",
    "# Remove the last month record for each agent as we don't have next month data\n",
    "last_month_indices = []\n",
    "for agent in unique_agents:\n",
    "    agent_data = train_df[train_df['agent_code'] == agent]\n",
    "    if len(agent_data) > 0:  # Safety check\n",
    "        last_month_idx = agent_data.iloc[-1].name\n",
    "        last_month_indices.append(last_month_idx)\n",
    "\n",
    "train_df = train_df.drop(last_month_indices)\n",
    "print(f\"Processed training data shape: {train_df.shape}\")\n",
    "print(f\"Target distribution: {train_df['target_column'].value_counts()}\")\n",
    "\n",
    "# Calculate class imbalance for weighing\n",
    "train_class_weights = {\n",
    "    0: train_df.shape[0] / (2 * (train_df['target_column'] == 0).sum()),\n",
    "    1: train_df.shape[0] / (2 * (train_df['target_column'] == 1).sum())\n",
    "}\n",
    "print(f\"Class weights for imbalance handling: {train_class_weights}\")\n",
    "\n",
    "# Enhanced Feature Engineering\n",
    "print(\"\\nStep 3: Advanced feature engineering with agent profiling...\")\n",
    "\n",
    "# Process each dataframe separately to avoid duplication\n",
    "for df in [train_df, test_df]:\n",
    "    # Extract time-based features\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_quarter'] = df[col].dt.quarter\n",
    "            df[f'{col}_month_sin'] = np.sin(2 * np.pi * df[col].dt.month/12)\n",
    "            df[f'{col}_month_cos'] = np.cos(2 * np.pi * df[col].dt.month/12)\n",
    "\n",
    "    # Experience features\n",
    "    if all(col in df.columns for col in ['year_month', 'agent_join_month']):\n",
    "        df['months_with_company'] = ((df['year_month'].dt.year - df['agent_join_month'].dt.year) * 12 +\n",
    "                                    (df['year_month'].dt.month - df['agent_join_month'].dt.month))\n",
    "\n",
    "    if all(col in df.columns for col in ['first_policy_sold_month', 'agent_join_month']):\n",
    "        df['months_to_first_sale'] = ((df['first_policy_sold_month'].dt.year - df['agent_join_month'].dt.year) * 12 +\n",
    "                                    (df['first_policy_sold_month'].dt.month - df['agent_join_month'].dt.month))\n",
    "        df['months_to_first_sale'] = df['months_to_first_sale'].fillna(-1)\n",
    "\n",
    "    if all(col in df.columns for col in ['year_month', 'first_policy_sold_month']):\n",
    "        df['months_since_first_sale'] = ((df['year_month'].dt.year - df['first_policy_sold_month'].dt.year) * 12 +\n",
    "                                      (df['year_month'].dt.month - df['first_policy_sold_month'].dt.month))\n",
    "        df['months_since_first_sale'] = df['months_since_first_sale'].fillna(-1)\n",
    "\n",
    "    # Activity trend features\n",
    "    if all(col in df.columns for col in ['unique_proposals_last_7_days', 'unique_proposals_last_15_days']):\n",
    "        df['proposal_trend_7_15'] = df['unique_proposals_last_7_days'] / np.maximum(df['unique_proposals_last_15_days'], 1)\n",
    "    if all(col in df.columns for col in ['unique_proposals_last_15_days', 'unique_proposals_last_21_days']):\n",
    "        df['proposal_trend_15_21'] = df['unique_proposals_last_15_days'] / np.maximum(df['unique_proposals_last_21_days'], 1)\n",
    "    if all(col in df.columns for col in ['unique_quotations_last_7_days', 'unique_quotations_last_15_days']):\n",
    "        df['quotation_trend_7_15'] = df['unique_quotations_last_7_days'] / np.maximum(df['unique_quotations_last_15_days'], 1)\n",
    "    if all(col in df.columns for col in ['unique_quotations_last_15_days', 'unique_quotations_last_21_days']):\n",
    "        df['quotation_trend_15_21'] = df['unique_quotations_last_15_days'] / np.maximum(df['unique_quotations_last_21_days'], 1)\n",
    "    if all(col in df.columns for col in ['unique_customers_last_7_days', 'unique_customers_last_15_days']):\n",
    "        df['customer_trend_7_15'] = df['unique_customers_last_7_days'] / np.maximum(df['unique_customers_last_15_days'], 1)\n",
    "    if all(col in df.columns for col in ['unique_customers_last_15_days', 'unique_customers_last_21_days']):\n",
    "        df['customer_trend_15_21'] = df['unique_customers_last_15_days'] / np.maximum(df['unique_customers_last_21_days'], 1)\n",
    "\n",
    "    # Activity consistency (variance-based)\n",
    "    if all(col in df.columns for col in ['unique_proposals_last_7_days', 'unique_proposals_last_15_days', 'unique_proposals_last_21_days']):\n",
    "        proposal_cols = ['unique_proposals_last_7_days', 'unique_proposals_last_15_days', 'unique_proposals_last_21_days']\n",
    "        df['proposal_variance'] = df[proposal_cols].var(axis=1)\n",
    "        df['proposal_consistency'] = 1 / (1 + df['proposal_variance'])\n",
    "    if all(col in df.columns for col in ['unique_quotations_last_7_days', 'unique_quotations_last_15_days', 'unique_quotations_last_21_days']):\n",
    "        quotation_cols = ['unique_quotations_last_7_days', 'unique_quotations_last_15_days', 'unique_quotations_last_21_days']\n",
    "        df['quotation_variance'] = df[quotation_cols].var(axis=1)\n",
    "        df['quotation_consistency'] = 1 / (1 + df['quotation_variance'])\n",
    "    if all(col in df.columns for col in ['unique_customers_last_7_days', 'unique_customers_last_15_days', 'unique_customers_last_21_days']):\n",
    "        customer_cols = ['unique_customers_last_7_days', 'unique_customers_last_15_days', 'unique_customers_last_21_days']\n",
    "        df['customer_variance'] = df[customer_cols].var(axis=1)\n",
    "        df['customer_consistency'] = 1 / (1 + df['customer_variance'])\n",
    "\n",
    "    # Current period activity rates\n",
    "    if all(col in df.columns for col in ['unique_customers', 'unique_proposal']):\n",
    "        df['proposals_per_customer'] = df['unique_proposal'] / np.maximum(df['unique_customers'], 1)\n",
    "    if all(col in df.columns for col in ['unique_customers', 'unique_quotations']):\n",
    "        df['quotations_per_customer'] = df['unique_quotations'] / np.maximum(df['unique_customers'], 1)\n",
    "    if all(col in df.columns for col in ['unique_proposal', 'unique_quotations']):\n",
    "        df['quotations_per_proposal'] = df['unique_quotations'] / np.maximum(df['unique_proposal'], 1)\n",
    "\n",
    "    # Time-based seasonality features\n",
    "    if 'year_month_month' in df.columns:\n",
    "        df['is_quarter_end'] = df['year_month_month'].isin([3, 6, 9, 12]).astype(int)\n",
    "        df['is_year_end'] = df['year_month_month'].isin([12]).astype(int)\n",
    "\n",
    "    # Ratios of activity metrics\n",
    "    if all(col in df.columns for col in ['unique_proposal', 'unique_quotations']):\n",
    "        df['quotation_to_proposal_ratio'] = df['unique_quotations'] / np.maximum(df['unique_proposal'], 1)\n",
    "\n",
    "    # Cash payment ratio\n",
    "    if all(col in df.columns for col in ['number_of_cash_payment_policies', 'number_of_policy_holders']):\n",
    "        df['cash_payment_ratio'] = df['number_of_cash_payment_policies'] / np.maximum(df['number_of_policy_holders'], 1)\n",
    "\n",
    "    # Agent characteristics\n",
    "    if 'agent_age' in df.columns:\n",
    "        df['agent_age_squared'] = df['agent_age'] ** 2\n",
    "        df['agent_age_prime'] = (df['agent_age'] >= 35) & (df['agent_age'] <= 45)\n",
    "        df['agent_senior'] = df['agent_age'] > 50\n",
    "\n",
    "    # Interaction features\n",
    "    if all(col in df.columns for col in ['agent_age', 'months_with_company']):\n",
    "        df['age_experience_interaction'] = df['agent_age'] * df['months_with_company']\n",
    "    if all(col in df.columns for col in ['agent_age', 'months_since_first_sale']):\n",
    "        df['age_sales_experience'] = df['agent_age'] * np.maximum(df['months_since_first_sale'], 0)\n",
    "\n",
    "    # Agent velocity metrics\n",
    "    if all(col in df.columns for col in ['unique_proposals_last_7_days', 'unique_proposal']):\n",
    "        df['proposal_velocity'] = df['unique_proposals_last_7_days'] / np.maximum(df['unique_proposal'], 1)\n",
    "    if all(col in df.columns for col in ['unique_quotations_last_7_days', 'unique_quotations']):\n",
    "        df['quotation_velocity'] = df['unique_quotations_last_7_days'] / np.maximum(df['unique_quotations'], 1)\n",
    "    if all(col in df.columns for col in ['unique_customers_last_7_days', 'unique_customers']):\n",
    "        df['customer_velocity'] = df['unique_customers_last_7_days'] / np.maximum(df['unique_customers'], 1)\n",
    "\n",
    "    # CHAMPION-SPECIFIC FEATURES\n",
    "    if all(col in df.columns for col in ['unique_proposals_last_7_days', 'unique_proposals_last_21_days']):\n",
    "        df['proposal_momentum'] = df['unique_proposals_last_7_days'] / np.maximum(df['unique_proposals_last_21_days'], 1) * 3\n",
    "    if all(col in df.columns for col in ['unique_quotations_last_7_days', 'unique_quotations_last_21_days']):\n",
    "        df['quotation_momentum'] = df['unique_quotations_last_7_days'] / np.maximum(df['unique_quotations_last_21_days'], 1) * 3\n",
    "    if all(col in df.columns for col in ['unique_customers_last_7_days', 'unique_customers_last_21_days']):\n",
    "        df['customer_momentum'] = df['unique_customers_last_7_days'] / np.maximum(df['unique_customers_last_21_days'], 1) * 3\n",
    "\n",
    "    if all(col in df.columns for col in ['unique_proposal', 'unique_proposals_last_7_days',\n",
    "                                        'unique_proposals_last_15_days', 'unique_proposals_last_21_days']):\n",
    "        df['proposal_gap'] = df['unique_proposal'] - (df['unique_proposals_last_7_days'] +\n",
    "                                                  df['unique_proposals_last_15_days'] +\n",
    "                                                  df['unique_proposals_last_21_days'])\n",
    "    if all(col in df.columns for col in ['unique_quotations', 'unique_quotations_last_7_days',\n",
    "                                        'unique_quotations_last_15_days', 'unique_quotations_last_21_days']):\n",
    "        df['quotation_gap'] = df['unique_quotations'] - (df['unique_quotations_last_7_days'] +\n",
    "                                                     df['unique_quotations_last_15_days'] +\n",
    "                                                     df['unique_quotations_last_21_days'])\n",
    "    if all(col in df.columns for col in ['ANBP_value', 'unique_proposal']):\n",
    "        df['revenue_per_proposal'] = df['ANBP_value'] / np.maximum(df['unique_proposal'], 1)\n",
    "    if all(col in df.columns for col in ['ANBP_value', 'unique_customers']):\n",
    "        df['revenue_per_customer'] = df['ANBP_value'] / np.maximum(df['unique_customers'], 1)\n",
    "\n",
    "    if all(col in df.columns for col in ['new_policy_count', 'unique_proposal']):\n",
    "        df['proposal_to_policy_ratio'] = df['new_policy_count'] / np.maximum(df['unique_proposal'], 1)\n",
    "    if all(col in df.columns for col in ['new_policy_count', 'unique_quotations']):\n",
    "        df['quotation_to_policy_ratio'] = df['new_policy_count'] / np.maximum(df['unique_quotations'], 1)\n",
    "\n",
    "    for col_to_transform in ['unique_proposal', 'unique_quotations', 'unique_customers', 'ANBP_value', 'net_income']:\n",
    "        if col_to_transform in df.columns:\n",
    "            df[f'log_{col_to_transform}'] = np.log1p(df[col_to_transform])\n",
    "            df[f'sqrt_{col_to_transform}'] = np.sqrt(np.maximum(0, df[col_to_transform])) # ensure non-negative for sqrt\n",
    "\n",
    "print(\"Creating enhanced historical agent features...\")\n",
    "train_hist_features = pd.DataFrame()\n",
    "test_hist_features = pd.DataFrame()\n",
    "hist_data_list_train = []\n",
    "for agent in train_df['agent_code'].unique():\n",
    "    agent_data = train_df[train_df['agent_code'] == agent].copy().sort_values('year_month')\n",
    "    for i in range(1, len(agent_data)):\n",
    "        past_data = agent_data.iloc[:i]\n",
    "        current_row_id = agent_data.iloc[i]['row_id']\n",
    "        hist_data = {'row_id': current_row_id}\n",
    "        if not past_data.empty:\n",
    "            hist_data.update({\n",
    "                'hist_avg_proposals': past_data['unique_proposal'].mean(),\n",
    "                'hist_avg_quotations': past_data['unique_quotations'].mean(),\n",
    "                'hist_avg_customers': past_data['unique_customers'].mean(),\n",
    "                'hist_avg_policies': past_data['new_policy_count'].mean() if 'new_policy_count' in past_data.columns else 0,\n",
    "                'hist_months_active': len(past_data),\n",
    "                'hist_zero_policy_months': (past_data['new_policy_count'] == 0).sum() if 'new_policy_count' in past_data.columns else 0,\n",
    "                'hist_nill_rate': (past_data['new_policy_count'] == 0).mean() if 'new_policy_count' in past_data.columns and len(past_data) > 0 else 0.5,\n",
    "            })\n",
    "            if len(past_data) >= 3:\n",
    "                recent_data = past_data.tail(3)\n",
    "                hist_data.update({\n",
    "                    'hist_recent_vs_all_proposals': recent_data['unique_proposal'].mean() / np.maximum(past_data['unique_proposal'].mean(), 1),\n",
    "                    'hist_recent_vs_all_quotations': recent_data['unique_quotations'].mean() / np.maximum(past_data['unique_quotations'].mean(), 1),\n",
    "                    'hist_recent_vs_all_customers': recent_data['unique_customers'].mean() / np.maximum(past_data['unique_customers'].mean(), 1),\n",
    "                    'hist_recent_vs_all_policies': recent_data['new_policy_count'].mean() / np.maximum(past_data['new_policy_count'].mean(), 1) if 'new_policy_count' in past_data.columns else 1,\n",
    "                    'hist_consistency_score': 1 / (1 + (past_data['unique_proposal'].std() / (past_data['unique_proposal'].mean() + 1) +\n",
    "                                                   past_data['unique_quotations'].std() / (past_data['unique_quotations'].mean() + 1) +\n",
    "                                                   past_data['unique_customers'].std() / (past_data['unique_customers'].mean() + 1))/3),\n",
    "                    'hist_policy_consistency': 1 / (1 + past_data['new_policy_count'].std() / (past_data['new_policy_count'].mean() + 1)) if 'new_policy_count' in past_data.columns else 0,\n",
    "                })\n",
    "            if i >= 2:\n",
    "                hist_data.update({\n",
    "                    'hist_proposal_growth': (agent_data.iloc[i-1]['unique_proposal'] / np.maximum(agent_data.iloc[i-2]['unique_proposal'], 1)) - 1,\n",
    "                    'hist_quotation_growth': (agent_data.iloc[i-1]['unique_quotations'] / np.maximum(agent_data.iloc[i-2]['unique_quotations'], 1)) - 1,\n",
    "                    'hist_customer_growth': (agent_data.iloc[i-1]['unique_customers'] / np.maximum(agent_data.iloc[i-2]['unique_customers'], 1)) - 1,\n",
    "                    'hist_policy_growth': (agent_data.iloc[i-1]['new_policy_count'] / np.maximum(agent_data.iloc[i-2]['new_policy_count'], 1)) - 1 if 'new_policy_count' in agent_data.columns else 0,\n",
    "                })\n",
    "        hist_data_list_train.append(hist_data)\n",
    "if hist_data_list_train: train_hist_features = pd.DataFrame(hist_data_list_train)\n",
    "\n",
    "hist_data_list_test = []\n",
    "for agent in test_df['agent_code'].unique():\n",
    "    agent_train_history = train_df[train_df['agent_code'] == agent].copy()\n",
    "    agent_test_data = test_df[test_df['agent_code'] == agent].copy()\n",
    "    if not agent_test_data.empty:\n",
    "        agent_all_data = pd.concat([agent_train_history, agent_test_data]).sort_values('year_month').reset_index(drop=True)\n",
    "        for i, test_row in agent_test_data.iterrows():\n",
    "            test_date = test_row['year_month']\n",
    "            past_data = agent_all_data[agent_all_data['year_month'] < test_date]\n",
    "            hist_data = {'row_id': test_row['row_id']}\n",
    "            if not past_data.empty:\n",
    "                hist_data.update({\n",
    "                    'hist_avg_proposals': past_data['unique_proposal'].mean(),\n",
    "                    'hist_avg_quotations': past_data['unique_quotations'].mean(),\n",
    "                    'hist_avg_customers': past_data['unique_customers'].mean(),\n",
    "                    'hist_avg_policies': past_data['new_policy_count'].mean() if 'new_policy_count' in past_data.columns else 0,\n",
    "                    'hist_months_active': len(past_data),\n",
    "                    'hist_zero_policy_months': (past_data['new_policy_count'] == 0).sum() if 'new_policy_count' in past_data.columns else 0,\n",
    "                    'hist_nill_rate': (past_data['new_policy_count'] == 0).mean() if 'new_policy_count' in past_data.columns and len(past_data) > 0 else 0.5,\n",
    "                })\n",
    "                if len(past_data) >= 3:\n",
    "                    recent_data = past_data.tail(3)\n",
    "                    hist_data.update({\n",
    "                        'hist_recent_vs_all_proposals': recent_data['unique_proposal'].mean() / np.maximum(past_data['unique_proposal'].mean(), 1),\n",
    "                        'hist_recent_vs_all_quotations': recent_data['unique_quotations'].mean() / np.maximum(past_data['unique_quotations'].mean(), 1),\n",
    "                        'hist_recent_vs_all_customers': recent_data['unique_customers'].mean() / np.maximum(past_data['unique_customers'].mean(), 1),\n",
    "                        'hist_recent_vs_all_policies': recent_data['new_policy_count'].mean() / np.maximum(past_data['new_policy_count'].mean(), 1) if 'new_policy_count' in past_data.columns else 1,\n",
    "                        'hist_consistency_score': 1 / (1 + (past_data['unique_proposal'].std() / (past_data['unique_proposal'].mean() + 1) +\n",
    "                                                       past_data['unique_quotations'].std() / (past_data['unique_quotations'].mean() + 1) +\n",
    "                                                       past_data['unique_customers'].std() / (past_data['unique_customers'].mean() + 1))/3),\n",
    "                        'hist_policy_consistency': 1 / (1 + past_data['new_policy_count'].std() / (past_data['new_policy_count'].mean() + 1)) if 'new_policy_count' in past_data.columns else 0,\n",
    "                    })\n",
    "                if len(past_data) >= 2:\n",
    "                    last_two = past_data.sort_values('year_month').tail(2)\n",
    "                    if len(last_two) >= 2:\n",
    "                        hist_data.update({\n",
    "                            'hist_proposal_growth': (last_two.iloc[1]['unique_proposal'] / np.maximum(last_two.iloc[0]['unique_proposal'], 1)) - 1,\n",
    "                            'hist_quotation_growth': (last_two.iloc[1]['unique_quotations'] / np.maximum(last_two.iloc[0]['unique_quotations'], 1)) - 1,\n",
    "                            'hist_customer_growth': (last_two.iloc[1]['unique_customers'] / np.maximum(last_two.iloc[0]['unique_customers'], 1)) - 1,\n",
    "                            'hist_policy_growth': (last_two.iloc[1]['new_policy_count'] / np.maximum(last_two.iloc[0]['new_policy_count'], 1)) - 1 if 'new_policy_count' in last_two.columns else 0,\n",
    "                        })\n",
    "            else: # No history, default values\n",
    "                 hist_data.update({\n",
    "                    'hist_avg_proposals': 0, 'hist_avg_quotations': 0, 'hist_avg_customers': 0, 'hist_avg_policies': 0,\n",
    "                    'hist_proposal_growth': 0, 'hist_quotation_growth': 0, 'hist_customer_growth': 0, 'hist_policy_growth': 0,\n",
    "                    'hist_consistency_score': 0.5, 'hist_policy_consistency': 0.5, 'hist_months_active': 0, 'hist_zero_policy_months': 0,\n",
    "                    'hist_nill_rate': 0.5, 'hist_recent_vs_all_proposals': 1, 'hist_recent_vs_all_quotations': 1,\n",
    "                    'hist_recent_vs_all_customers': 1, 'hist_recent_vs_all_policies': 1\n",
    "                })\n",
    "            hist_data_list_test.append(hist_data)\n",
    "if hist_data_list_test: test_hist_features = pd.DataFrame(hist_data_list_test)\n",
    "\n",
    "default_hist_cols = {col: 0 for col in train_hist_features.columns if col != 'row_id'}\n",
    "default_hist_cols.update({ # Default for specific rates/ratios\n",
    "    'hist_nill_rate': 0.5, 'hist_recent_vs_all_proposals': 1, 'hist_recent_vs_all_quotations': 1,\n",
    "    'hist_recent_vs_all_customers': 1, 'hist_recent_vs_all_policies': 1,\n",
    "    'hist_consistency_score': 0.5, 'hist_policy_consistency': 0.5\n",
    "})\n",
    "\n",
    "if not train_hist_features.empty:\n",
    "    train_hist_features['row_id'] = train_hist_features['row_id'].astype(int)\n",
    "    train_df = pd.merge(train_df, train_hist_features, on='row_id', how='left')\n",
    "    train_df.fillna(value=default_hist_cols, inplace=True)\n",
    "\n",
    "if not test_hist_features.empty:\n",
    "    test_hist_features['row_id'] = test_hist_features['row_id'].astype(int)\n",
    "    test_df = pd.merge(test_df, test_hist_features, on='row_id', how='left')\n",
    "    test_df.fillna(value=default_hist_cols, inplace=True)\n",
    "\n",
    "\n",
    "print(\"Adding agent profiling features...\")\n",
    "agent_profiles_agg = {\n",
    "    'unique_proposal': ['mean', 'std', 'max', 'min'],\n",
    "    'unique_quotations': ['mean', 'std', 'max', 'min'],\n",
    "    'unique_customers': ['mean', 'std', 'max', 'min']\n",
    "}\n",
    "if 'new_policy_count' in train_df.columns: # Add if exists\n",
    "    agent_profiles_agg['new_policy_count'] = ['mean', 'std', 'max', 'min']\n",
    "\n",
    "agent_profiles = train_df.groupby('agent_code').agg(agent_profiles_agg).reset_index()\n",
    "agent_profiles.columns = ['_'.join(col).strip('_') for col in agent_profiles.columns.values]\n",
    "agent_profiles.rename(columns={'agent_code_': 'agent_code'}, inplace=True) # Fix agent_code name\n",
    "\n",
    "agent_profiles['proposal_cv'] = agent_profiles['unique_proposal_std'] / np.maximum(agent_profiles['unique_proposal_mean'], 1)\n",
    "agent_profiles['quotation_cv'] = agent_profiles['unique_quotations_std'] / np.maximum(agent_profiles['unique_quotations_mean'], 1)\n",
    "agent_profiles['customer_cv'] = agent_profiles['unique_customers_std'] / np.maximum(agent_profiles['unique_customers_mean'], 1)\n",
    "if 'new_policy_count_mean' in agent_profiles.columns and 'new_policy_count_std' in agent_profiles.columns:\n",
    "    agent_profiles['policy_cv'] = agent_profiles['new_policy_count_std'] / np.maximum(agent_profiles['new_policy_count_mean'], 1)\n",
    "\n",
    "if 'new_policy_count' in train_df.columns:\n",
    "    agent_nill_rates = train_df.groupby('agent_code')['new_policy_count'].apply(lambda x: (x == 0).mean()).reset_index()\n",
    "    agent_nill_rates.columns = ['agent_code', 'agent_nill_rate']\n",
    "    agent_profiles = pd.merge(agent_profiles, agent_nill_rates, on='agent_code', how='left')\n",
    "\n",
    "train_df = pd.merge(train_df, agent_profiles, on='agent_code', how='left')\n",
    "test_df = pd.merge(test_df, agent_profiles, on='agent_code', how='left')\n",
    "\n",
    "# Fill NAs in profile features created by merge (for agents in test but not train)\n",
    "profile_feature_cols = [col for col in agent_profiles.columns if col != 'agent_code']\n",
    "for df in [train_df, test_df]:\n",
    "    for feature in profile_feature_cols:\n",
    "        if feature in df.columns:\n",
    "            df[feature] = df[feature].fillna(0) # Fill with 0 or median/mean based on feature nature\n",
    "            if 'agent_nill_rate' in feature:\n",
    "                df[feature] = df[feature].fillna(0.5) # Default NILL rate for unseen agents\n",
    "\n",
    "print(f\"Train data shape after feature engineering: {train_df.shape}\")\n",
    "print(f\"Test data shape after feature engineering: {test_df.shape}\")\n",
    "if 'row_id' in test_df.columns: # Check if row_id exists before asserting\n",
    "    assert test_df['row_id'].nunique() == 914, \"Test data size changed or duplicates introduced after feature engineering!\"\n",
    "\n",
    "\n",
    "# Advanced feature selection with stability analysis\n",
    "print(\"\\nStep 4: Feature selection with stability analysis...\")\n",
    "base_features = ['agent_age','agent_age_squared','agent_age_prime','agent_senior','unique_proposal','unique_quotations','unique_customers','unique_proposals_last_7_days','unique_proposals_last_15_days','unique_proposals_last_21_days','unique_quotations_last_7_days','unique_quotations_last_15_days','unique_quotations_last_21_days','unique_customers_last_7_days','unique_customers_last_15_days','unique_customers_last_21_days','ANBP_value','net_income','number_of_policy_holders','number_of_cash_payment_policies']\n",
    "engineered_features = ['months_with_company','months_to_first_sale','months_since_first_sale','proposal_trend_7_15','proposal_trend_15_21','quotation_trend_7_15','quotation_trend_15_21','customer_trend_7_15','customer_trend_15_21','proposal_variance','proposal_consistency','quotation_variance','quotation_consistency','customer_variance','customer_consistency','proposals_per_customer','quotations_per_customer','quotations_per_proposal','is_quarter_end','is_year_end','year_month_month_sin','year_month_month_cos','quotation_to_proposal_ratio','cash_payment_ratio','age_experience_interaction','age_sales_experience','proposal_velocity','quotation_velocity','customer_velocity','proposal_momentum','quotation_momentum','customer_momentum','proposal_gap','quotation_gap','revenue_per_proposal','revenue_per_customer','log_unique_proposal','log_unique_quotations','log_unique_customers','log_ANBP_value','log_net_income','sqrt_unique_proposal','sqrt_unique_quotations','sqrt_unique_customers','sqrt_ANBP_value','sqrt_net_income']\n",
    "historical_features = ['hist_avg_proposals','hist_avg_quotations','hist_avg_customers','hist_avg_policies','hist_consistency_score','hist_policy_consistency','hist_proposal_growth','hist_quotation_growth','hist_customer_growth','hist_policy_growth','hist_months_active','hist_zero_policy_months','hist_nill_rate','hist_recent_vs_all_proposals','hist_recent_vs_all_quotations','hist_recent_vs_all_customers','hist_recent_vs_all_policies']\n",
    "profile_features = [col for col in agent_profiles.columns if col not in ['agent_code']] # Dynamic based on created profiles\n",
    "all_potential_features = base_features + engineered_features + historical_features + profile_features\n",
    "features_to_use = sorted(list(set([f for f in all_potential_features if f in train_df.columns and f in test_df.columns]))) # Unique and sorted\n",
    "print(f\"Total potential features initially: {len(features_to_use)}\")\n",
    "\n",
    "X_temp = train_df[features_to_use].copy()\n",
    "for col in X_temp.columns:\n",
    "    if X_temp[col].isnull().any(): X_temp[col] = X_temp[col].fillna(X_temp[col].median())\n",
    "corr_matrix = X_temp.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "features_to_use = [f for f in features_to_use if f not in to_drop_corr]\n",
    "print(f\"Removed {len(to_drop_corr)} highly correlated features. Features remaining: {len(features_to_use)}\")\n",
    "\n",
    "X = train_df[features_to_use].copy()\n",
    "y = train_df['target_column'].copy()\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        if pd.api.types.is_numeric_dtype(X[col]): X[col] = X[col].fillna(X[col].median())\n",
    "        else: X[col] = X[col].fillna('unknown').astype(str) # Or mode\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced')\n",
    "rf_selector.fit(X_scaled, y)\n",
    "importances_rf = pd.Series(rf_selector.feature_importances_, index=features_to_use).sort_values(ascending=False)\n",
    "selected_features_rf = list(importances_rf[importances_rf > importances_rf.quantile(0.25)].index) # Top 75% by value (approx)\n",
    "\n",
    "xgb_selector = xgb.XGBClassifier(n_estimators=100, learning_rate=0.05, max_depth=4, random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_selector.fit(X_scaled, y)\n",
    "importances_xgb = pd.Series(xgb_selector.feature_importances_, index=features_to_use).sort_values(ascending=False)\n",
    "selected_features_xgb = list(importances_xgb[importances_xgb > importances_xgb.quantile(0.25)].index)\n",
    "\n",
    "final_features = sorted(list(set(selected_features_rf) | set(selected_features_xgb)))\n",
    "critical_features_to_add = ['proposal_momentum','customer_momentum','hist_nill_rate','hist_zero_policy_months','agent_nill_rate','months_with_company','proposal_consistency']\n",
    "for feat in critical_features_to_add:\n",
    "    if feat in features_to_use and feat not in final_features: final_features.append(feat)\n",
    "final_features = sorted(list(set(final_features))) # Ensure uniqueness and sort\n",
    "print(f\"Final feature count after selection: {len(final_features)}\")\n",
    "with open(os.path.join(output_dir, 'selected_features.txt'), 'w') as f:\n",
    "    for feature in final_features: f.write(f\"{feature}\\n\")\n",
    "\n",
    "# Prepare data for Optuna and final model\n",
    "global_final_X = train_df[final_features].copy()\n",
    "global_final_y = train_df['target_column'].copy()\n",
    "\n",
    "for col in global_final_X.columns:\n",
    "    if global_final_X[col].isnull().any():\n",
    "        if pd.api.types.is_numeric_dtype(global_final_X[col]):\n",
    "            global_final_X[col] = global_final_X[col].fillna(global_final_X[col].median())\n",
    "        else: # Assuming categorical, fill with mode or 'unknown'\n",
    "            global_final_X[col] = global_final_X[col].fillna(global_final_X[col].mode()[0]).astype(str)\n",
    "\n",
    "\n",
    "global_final_scaler = StandardScaler()\n",
    "global_final_X_scaled = global_final_scaler.fit_transform(global_final_X)\n",
    "global_tscv = TimeSeriesSplit(n_splits=5) # Ensure tscv is defined for objective\n",
    "\n",
    "# Optuna Objective Function\n",
    "def objective(trial):\n",
    "    # RandomForest\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 50, 300, step=50)\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 4, 12)\n",
    "    rf_min_samples_split = trial.suggest_int('rf_min_samples_split', 2, 20)\n",
    "    rf_min_samples_leaf = trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "    rf_max_features = trial.suggest_categorical('rf_max_features', ['sqrt', 'log2', 0.6, 0.8])\n",
    "\n",
    "    # GradientBoosting\n",
    "    gb_n_estimators = trial.suggest_int('gb_n_estimators', 50, 300, step=50)\n",
    "    gb_learning_rate = trial.suggest_float('gb_learning_rate', 0.01, 0.15, log=True)\n",
    "    gb_max_depth = trial.suggest_int('gb_max_depth', 3, 7)\n",
    "    gb_subsample = trial.suggest_float('gb_subsample', 0.7, 1.0)\n",
    "\n",
    "    # XGBoost\n",
    "    xgb_n_estimators = trial.suggest_int('xgb_n_estimators', 50, 300, step=50)\n",
    "    xgb_learning_rate = trial.suggest_float('xgb_learning_rate', 0.01, 0.15, log=True)\n",
    "    xgb_max_depth = trial.suggest_int('xgb_max_depth', 3, 7)\n",
    "    xgb_min_child_weight = trial.suggest_int('xgb_min_child_weight', 1, 7)\n",
    "    xgb_subsample = trial.suggest_float('xgb_subsample', 0.7, 1.0)\n",
    "    xgb_colsample_bytree = trial.suggest_float('xgb_colsample_bytree', 0.7, 1.0)\n",
    "    xgb_gamma = trial.suggest_float('xgb_gamma', 0, 0.3)\n",
    "\n",
    "    # LightGBM\n",
    "    lgb_n_estimators = trial.suggest_int('lgb_n_estimators', 50, 300, step=50)\n",
    "    lgb_learning_rate = trial.suggest_float('lgb_learning_rate', 0.01, 0.15, log=True)\n",
    "    lgb_num_leaves = trial.suggest_int('lgb_num_leaves', 15, 60)\n",
    "    lgb_max_depth = trial.suggest_int('lgb_max_depth', 3, 7) # Often -1, but can be tuned\n",
    "    lgb_min_child_samples = trial.suggest_int('lgb_min_child_samples', 10, 40)\n",
    "\n",
    "    # CatBoost\n",
    "    cat_iterations = trial.suggest_int('cat_iterations', 50, 300, step=50)\n",
    "    cat_learning_rate = trial.suggest_float('cat_learning_rate', 0.01, 0.15, log=True)\n",
    "    cat_depth = trial.suggest_int('cat_depth', 4, 8)\n",
    "    cat_l2_leaf_reg = trial.suggest_float('cat_l2_leaf_reg', 1.0, 9.0, log=True)\n",
    "    cat_class_weight_0 = trial.suggest_float('cat_class_weight_0_cb', 1.5, 4.5) # Tuned class weight for NILL\n",
    "\n",
    "    # Ensemble Weights\n",
    "    w_rf = trial.suggest_float('w_rf', 0.5, 2.5)\n",
    "    w_gb = trial.suggest_float('w_gb', 0.5, 2.5)\n",
    "    w_xgb = trial.suggest_float('w_xgb', 0.5, 2.5)\n",
    "    w_lgb = trial.suggest_float('w_lgb', 0.5, 2.5)\n",
    "    w_cat = trial.suggest_float('w_cat', 0.5, 2.5)\n",
    "    ensemble_weights = [w_rf, w_gb, w_xgb, w_lgb, w_cat]\n",
    "\n",
    "    fold_roc_auc_scores = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(global_tscv.split(global_final_X_scaled)):\n",
    "        X_train, X_val = global_final_X_scaled[train_idx], global_final_X_scaled[val_idx]\n",
    "        y_train, y_val = global_final_y.iloc[train_idx], global_final_y.iloc[val_idx]\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=rf_n_estimators, max_depth=rf_max_depth, min_samples_split=rf_min_samples_split, min_samples_leaf=rf_min_samples_leaf, max_features=rf_max_features, random_state=RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        gb = GradientBoostingClassifier(n_estimators=gb_n_estimators, learning_rate=gb_learning_rate, max_depth=gb_max_depth, subsample=gb_subsample, random_state=RANDOM_STATE)\n",
    "\n",
    "        pos_weight_fold = (y_train == 0).sum() / max(1, (y_train == 1).sum())\n",
    "        xgb_m = xgb.XGBClassifier(n_estimators=xgb_n_estimators, learning_rate=xgb_learning_rate, max_depth=xgb_max_depth, min_child_weight=xgb_min_child_weight, subsample=xgb_subsample, colsample_bytree=xgb_colsample_bytree, gamma=xgb_gamma, scale_pos_weight=pos_weight_fold, random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "        lgb_m = lgb.LGBMClassifier(n_estimators=lgb_n_estimators, learning_rate=lgb_learning_rate, num_leaves=lgb_num_leaves, max_depth=lgb_max_depth, min_child_samples=lgb_min_child_samples, random_state=RANDOM_STATE, class_weight='balanced', verbose=-1)\n",
    "        cat_m = cb.CatBoostClassifier(iterations=cat_iterations, learning_rate=cat_learning_rate, depth=cat_depth, l2_leaf_reg=cat_l2_leaf_reg, random_seed=RANDOM_STATE, loss_function='Logloss', verbose=0, class_weights={0: cat_class_weight_0, 1: 1.0})\n",
    "\n",
    "        ensemble = VotingClassifier(estimators=[('rf', rf), ('gb', gb), ('xgb', xgb_m), ('lgb', lgb_m), ('cat', cat_m)], voting='soft', weights=ensemble_weights)\n",
    "\n",
    "        try:\n",
    "            ensemble.fit(X_train, y_train)\n",
    "            y_val_proba = ensemble.predict_proba(X_val)[:, 1]\n",
    "            fold_roc_auc_scores.append(roc_auc_score(y_val, y_val_proba))\n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial.number}, Fold {fold+1} error: {e}\")\n",
    "            return 0.0 # Penalize failed trials\n",
    "\n",
    "    avg_roc_auc = np.mean(fold_roc_auc_scores) if fold_roc_auc_scores else 0.0\n",
    "    return avg_roc_auc\n",
    "\n",
    "print(\"\\nStep 5: Hyperparameter Optimization with Optuna...\")\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "N_OPTUNA_TRIALS = 10 # IMPORTANT: Increase for real HPO (e.g., 50, 100+)\n",
    "study.optimize(objective, n_trials=N_OPTUNA_TRIALS, timeout=3600*3) # Example: 3 hour timeout\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"\\nBest ROC AUC from Optuna: {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters found by Optuna:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Step 6: Training final model on all data with optimized parameters\n",
    "print(\"\\nStep 6: Training final model on all data with optimized parameters...\")\n",
    "final_rf = RandomForestClassifier(\n",
    "    n_estimators=best_params.get('rf_n_estimators', 200), max_depth=best_params.get('rf_max_depth', 8),\n",
    "    min_samples_split=best_params.get('rf_min_samples_split', 10), min_samples_leaf=best_params.get('rf_min_samples_leaf', 4),\n",
    "    max_features=best_params.get('rf_max_features', 'sqrt'), random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced'\n",
    ")\n",
    "final_gb = GradientBoostingClassifier(\n",
    "    n_estimators=best_params.get('gb_n_estimators', 200), learning_rate=best_params.get('gb_learning_rate', 0.05),\n",
    "    max_depth=best_params.get('gb_max_depth', 4), subsample=best_params.get('gb_subsample', 0.8), random_state=RANDOM_STATE\n",
    ")\n",
    "final_pos_weight = (global_final_y == 0).sum() / max(1, (global_final_y == 1).sum())\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=best_params.get('xgb_n_estimators', 200), learning_rate=best_params.get('xgb_learning_rate', 0.05),\n",
    "    max_depth=best_params.get('xgb_max_depth', 4), min_child_weight=best_params.get('xgb_min_child_weight', 5),\n",
    "    subsample=best_params.get('xgb_subsample', 0.8), colsample_bytree=best_params.get('xgb_colsample_bytree', 0.8),\n",
    "    gamma=best_params.get('xgb_gamma', 0.1), scale_pos_weight=final_pos_weight, random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss'\n",
    ")\n",
    "final_lgb = lgb.LGBMClassifier(\n",
    "    n_estimators=best_params.get('lgb_n_estimators', 200), learning_rate=best_params.get('lgb_learning_rate', 0.05),\n",
    "    num_leaves=best_params.get('lgb_num_leaves', 20), max_depth=best_params.get('lgb_max_depth', 4),\n",
    "    min_child_samples=best_params.get('lgb_min_child_samples', 20), random_state=RANDOM_STATE, class_weight='balanced', verbose=-1\n",
    ")\n",
    "final_cat = cb.CatBoostClassifier(\n",
    "    iterations=best_params.get('cat_iterations', 200), learning_rate=best_params.get('cat_learning_rate', 0.05),\n",
    "    depth=best_params.get('cat_depth', 6), l2_leaf_reg=best_params.get('cat_l2_leaf_reg', 3),\n",
    "    random_seed=RANDOM_STATE, loss_function='Logloss', verbose=0,\n",
    "    class_weights={0: best_params.get('cat_class_weight_0_cb', 3.0), 1: 1.0}\n",
    ")\n",
    "final_ensemble_weights = [\n",
    "    best_params.get('w_rf', 1.0), best_params.get('w_gb', 1.5), best_params.get('w_xgb', 2.0),\n",
    "    best_params.get('w_lgb', 1.5), best_params.get('w_cat', 2.5)\n",
    "]\n",
    "final_ensemble = VotingClassifier(\n",
    "    estimators=[('rf', final_rf), ('gb', final_gb), ('xgb', final_xgb), ('lgb', final_lgb), ('cat', final_cat)],\n",
    "    voting='soft', weights=final_ensemble_weights\n",
    ")\n",
    "final_ensemble.fit(global_final_X_scaled, global_final_y)\n",
    "\n",
    "# Step 7: Generating optimized test predictions\n",
    "print(\"\\nStep 7: Generating optimized test predictions...\")\n",
    "X_test = test_df[final_features].copy()\n",
    "for col in X_test.columns: # Ensure consistent filling for test data\n",
    "    if X_test[col].isnull().any():\n",
    "        if pd.api.types.is_numeric_dtype(X_test[col]):\n",
    "            # Use median from corresponding TRAIN column to avoid data leakage\n",
    "             X_test[col] = X_test[col].fillna(global_final_X[col].median())\n",
    "        else:\n",
    "            X_test[col] = X_test[col].fillna(global_final_X[col].mode()[0]).astype(str)\n",
    "\n",
    "X_test_scaled = global_final_scaler.transform(X_test) # Use the scaler FIT on train data\n",
    "test_proba = final_ensemble.predict_proba(X_test_scaled)[:, 1]\n",
    "assert len(test_proba) == len(test_df), \"Prediction length doesn't match test set!\"\n",
    "\n",
    "print(\"\\nGenerating submissions with dynamic thresholds...\")\n",
    "def get_dynamic_threshold(agent_row):\n",
    "    base_threshold = 0.60\n",
    "    if 'months_with_company' in agent_row and not pd.isna(agent_row['months_with_company']):\n",
    "        if agent_row['months_with_company'] <= 3: base_threshold -= 0.04\n",
    "        elif agent_row['months_with_company'] >= 24: base_threshold += 0.02\n",
    "    if 'hist_nill_rate' in agent_row and not pd.isna(agent_row['hist_nill_rate']):\n",
    "        if agent_row['hist_nill_rate'] > 0.5: base_threshold += 0.02\n",
    "        elif agent_row['hist_nill_rate'] < 0.1: base_threshold -= 0.02\n",
    "    if 'agent_age' in agent_row and not pd.isna(agent_row['agent_age']):\n",
    "        if 35 <= agent_row['agent_age'] <= 45: base_threshold -= 0.01\n",
    "        elif agent_row['agent_age'] < 25 or agent_row['agent_age'] > 55: base_threshold += 0.01\n",
    "    return max(0.5, min(base_threshold, 0.7))\n",
    "\n",
    "test_df['dynamic_threshold'] = test_df.apply(get_dynamic_threshold, axis=1)\n",
    "dynamic_predictions = (test_proba >= test_df['dynamic_threshold']).astype(int)\n",
    "dynamic_submission = submission_template.copy()\n",
    "dynamic_submission['target_column'] = dynamic_predictions\n",
    "dynamic_submission_path = os.path.join(output_dir, 'dynamic_submission_optuna.csv')\n",
    "dynamic_submission.to_csv(dynamic_submission_path, index=False)\n",
    "\n",
    "print(\"\\nGenerating submissions with fixed thresholds:\")\n",
    "thresholds_to_try = np.arange(0.55, 0.66, 0.01)\n",
    "for threshold_val in thresholds_to_try:\n",
    "    test_predictions_fixed = (test_proba >= threshold_val).astype(int)\n",
    "    submission_fixed = submission_template.copy()\n",
    "    submission_fixed['target_column'] = test_predictions_fixed\n",
    "    submission_path_fixed = os.path.join(output_dir, f'submission_threshold_{threshold_val:.2f}_optuna.csv')\n",
    "    submission_fixed.to_csv(submission_path_fixed, index=False)\n",
    "    print(f\"Threshold {threshold_val:.2f}: {test_predictions_fixed.sum()} non-NILL, {len(test_predictions_fixed)-test_predictions_fixed.sum()} NILL\")\n",
    "\n",
    "best_fixed_threshold = 0.60 # Based on prior knowledge or analysis\n",
    "optimal_predictions = (test_proba >= best_fixed_threshold).astype(int)\n",
    "optimal_submission = submission_template.copy()\n",
    "optimal_submission['target_column'] = optimal_predictions\n",
    "optimal_submission_path = os.path.join(output_dir, 'submission_optuna.csv') # Main submission\n",
    "optimal_submission.to_csv(optimal_submission_path, index=False)\n",
    "print(f\"\\nOptimal submission file created: {optimal_submission_path}\")\n",
    "print(f\"Optimal prediction counts (fixed threshold {best_fixed_threshold}): {pd.Series(optimal_predictions).value_counts(normalize=True)}\")\n",
    "\n",
    "model_path = os.path.join(output_dir, 'champion_model_ensemble_optuna.pkl')\n",
    "joblib.dump((global_final_scaler, final_ensemble, final_features, best_params), model_path)\n",
    "np.save(os.path.join(output_dir, 'test_probabilities_optuna.npy'), test_proba)\n",
    "test_df[['row_id', 'agent_code', 'dynamic_threshold']].to_csv(os.path.join(output_dir, 'dynamic_thresholds_optuna.csv'), index=False)\n",
    "\n",
    "# Step 8: Feature importance analysis\n",
    "print(\"\\nStep 8: Feature importance analysis (from Optuna-tuned model)...\")\n",
    "if hasattr(final_ensemble, 'named_estimators_'):\n",
    "    importances_data = {'Feature': final_features}\n",
    "    if 'rf' in final_ensemble.named_estimators_ and hasattr(final_ensemble.named_estimators_['rf'], 'feature_importances_'):\n",
    "        importances_data['RF_Importance'] = final_ensemble.named_estimators_['rf'].feature_importances_\n",
    "    if 'xgb' in final_ensemble.named_estimators_ and hasattr(final_ensemble.named_estimators_['xgb'], 'feature_importances_'):\n",
    "        importances_data['XGB_Importance'] = final_ensemble.named_estimators_['xgb'].feature_importances_\n",
    "    if 'lgb' in final_ensemble.named_estimators_ and hasattr(final_ensemble.named_estimators_['lgb'], 'feature_importances_'):\n",
    "        importances_data['LGB_Importance'] = final_ensemble.named_estimators_['lgb'].feature_importances_\n",
    "    if 'cat' in final_ensemble.named_estimators_ and hasattr(final_ensemble.named_estimators_['cat'], 'feature_importances_'):\n",
    "        importances_data['CAT_Importance'] = final_ensemble.named_estimators_['cat'].feature_importances_\n",
    "\n",
    "    feature_importance_df = pd.DataFrame(importances_data)\n",
    "    importance_cols = [col for col in feature_importance_df.columns if '_Importance' in col]\n",
    "    if importance_cols: # Only if any importance columns were added\n",
    "        feature_importance_df['Avg_Importance'] = feature_importance_df[importance_cols].mean(axis=1)\n",
    "        feature_importance_df = feature_importance_df.sort_values('Avg_Importance', ascending=False)\n",
    "        feature_importance_df.to_csv(os.path.join(output_dir, 'feature_importance_optuna.csv'), index=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 10)) # Adjusted for more features potentially\n",
    "        sns.barplot(x='Avg_Importance', y='Feature', data=feature_importance_df.head(min(30, len(feature_importance_df)))) # Show top 30 or less\n",
    "        plt.title('Top Features by Average Importance (Optuna-tuned Model)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'top_features_optuna.png'))\n",
    "        print(\"Feature importance report and plot saved.\")\n",
    "    else:\n",
    "        print(\"Could not generate feature importances (models might not have them or ensemble structure changed).\")\n",
    "else:\n",
    "    print(\"Final ensemble model does not have 'named_estimators_', cannot extract feature importances directly.\")\n",
    "\n",
    "\n",
    "# Optional: Optuna visualization (run these in a Jupyter notebook or adjust for script execution)\n",
    "# try:\n",
    "#     fig_history = optuna.visualization.plot_optimization_history(study)\n",
    "#     fig_history.write_image(os.path.join(output_dir, \"optuna_optimization_history.png\"))\n",
    "#     fig_importance = optuna.visualization.plot_param_importances(study)\n",
    "#     fig_importance.write_image(os.path.join(output_dir, \"optuna_param_importances.png\"))\n",
    "#     # fig_slice = optuna.visualization.plot_slice(study, params=['xgb_learning_rate', 'xgb_max_depth'])\n",
    "#     # fig_slice.write_image(os.path.join(output_dir,\"optuna_slice_plot.png\"))\n",
    "#     print(\"Optuna visualization plots saved (if plotly and kaleido are installed).\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Could not save Optuna plots: {e}. Ensure plotly and kaleido are installed.\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"CHAMPIONSHIP SOLUTION WITH OPTUNA HPO completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total execution time: {elapsed_time:.2f} seconds ({elapsed_time/60:.2f} minutes)\")\n",
    "print(f\"OPTIMAL SUBMISSION (Optuna): {optimal_submission_path}\")\n",
    "print(f\"DYNAMIC SUBMISSION (Optuna): {dynamic_submission_path}\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nKey insights for presentation:\")\n",
    "print(\"1. Agent activity consistency is more predictive than raw activity volume\")\n",
    "print(\"2. Historical NILL rates are strong predictors of future performance\")\n",
    "print(\"3. New agents need different intervention strategies than experienced ones\")\n",
    "print(\"4. Agent age and months with company have significant interaction effects\")\n",
    "print(\"5. Momentum features (recent vs historical activity) are critical early warning signs\")\n",
    "print(\"6. Optuna helped fine-tune model hyperparameters and ensemble weights for potentially improved generalization.\")\n",
    "print(\"=\" * 100)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "CHAMPIONSHIP KAGGLE SOLUTION - ADVANCED ENSEMBLE WITH CUSTOM AGENT PROFILING & OPTUNA HPO\n",
      "====================================================================================================\n",
      "Starting at: 2025-05-07 17:47:36\n",
      "\n",
      "Step 1: Loading data with enhanced checks...\n",
      "Train data shape: (15308, 23)\n",
      "Test data shape: (914, 23)\n",
      "Submission template shape: (914, 2)\n",
      "Performing data integrity checks...\n",
      "\n",
      "Step 2: Enhanced preprocessing with domain expertise...\n",
      "Processed training data shape: (14403, 24)\n",
      "Target distribution: target_column\n",
      "1    12969\n",
      "0     1434\n",
      "Name: count, dtype: int64\n",
      "Class weights for imbalance handling: {0: np.float64(5.0219665271966525), 1: np.float64(0.5552856812398798)}\n",
      "\n",
      "Step 3: Advanced feature engineering with agent profiling...\n",
      "Creating enhanced historical agent features...\n",
      "Adding agent profiling features...\n",
      "Train data shape after feature engineering: (14403, 126)\n",
      "Test data shape after feature engineering: (914, 125)\n",
      "\n",
      "Step 4: Feature selection with stability analysis...\n",
      "Total potential features initially: 104\n",
      "Removed 10 highly correlated features. Features remaining: 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 17:47:55,044] A new study created in memory with name: no-name-402ebd02-deb3-487b-b67a-0ca2af5aeb29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature count after selection: 84\n",
      "\n",
      "Step 5: Hyperparameter Optimization with Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-07 17:49:34,851] Trial 0 finished with value: 0.7949309605427345 and parameters: {'rf_n_estimators': 150, 'rf_max_depth': 12, 'rf_min_samples_split': 15, 'rf_min_samples_leaf': 6, 'rf_max_features': 0.8, 'gb_n_estimators': 200, 'gb_learning_rate': 0.06803900745073706, 'gb_max_depth': 3, 'gb_subsample': 0.9909729556485982, 'xgb_n_estimators': 250, 'xgb_learning_rate': 0.01777174904859463, 'xgb_max_depth': 3, 'xgb_min_child_weight': 2, 'xgb_subsample': 0.7912726728878613, 'xgb_colsample_bytree': 0.8574269294896714, 'xgb_gamma': 0.12958350559263473, 'lgb_n_estimators': 100, 'lgb_learning_rate': 0.05243180891902853, 'lgb_num_leaves': 21, 'lgb_max_depth': 4, 'lgb_min_child_samples': 21, 'cat_iterations': 150, 'cat_learning_rate': 0.0838375512850209, 'cat_depth': 4, 'cat_l2_leaf_reg': 3.0953114978934915, 'cat_class_weight_0_cb': 3.2772437065861273, 'w_rf': 0.5929008254399954, 'w_gb': 1.7150897038028767, 'w_xgb': 0.8410482473745831, 'w_lgb': 0.630103185970559, 'w_cat': 2.3977710745066663}. Best is trial 0 with value: 0.7949309605427345.\n",
      "[I 2025-05-07 17:50:28,161] Trial 1 finished with value: 0.7908217353060956 and parameters: {'rf_n_estimators': 300, 'rf_max_depth': 11, 'rf_min_samples_split': 7, 'rf_min_samples_leaf': 1, 'rf_max_features': 'sqrt', 'gb_n_estimators': 50, 'gb_learning_rate': 0.1173393765991262, 'gb_max_depth': 4, 'gb_subsample': 0.8987566853061946, 'xgb_n_estimators': 100, 'xgb_learning_rate': 0.04089285700048085, 'xgb_max_depth': 5, 'xgb_min_child_weight': 2, 'xgb_subsample': 0.9908753883293675, 'xgb_colsample_bytree': 0.9325398470083344, 'xgb_gamma': 0.28184968246925673, 'lgb_n_estimators': 300, 'lgb_learning_rate': 0.05048762470240494, 'lgb_num_leaves': 57, 'lgb_max_depth': 3, 'lgb_min_child_samples': 16, 'cat_iterations': 50, 'cat_learning_rate': 0.02413338039141455, 'cat_depth': 5, 'cat_l2_leaf_reg': 1.8152346836599755, 'cat_class_weight_0_cb': 3.986212527455788, 'w_rf': 1.2135066533871786, 'w_gb': 1.0618690193747615, 'w_xgb': 1.585392166316497, 'w_lgb': 0.7818484499495253, 'w_cat': 2.1043939615080793}. Best is trial 0 with value: 0.7949309605427345.\n",
      "[I 2025-05-07 17:52:23,117] Trial 2 finished with value: 0.7849366249929945 and parameters: {'rf_n_estimators': 50, 'rf_max_depth': 12, 'rf_min_samples_split': 16, 'rf_min_samples_leaf': 2, 'rf_max_features': 'log2', 'gb_n_estimators': 250, 'gb_learning_rate': 0.012220339394068061, 'gb_max_depth': 4, 'gb_subsample': 0.7347607178575388, 'xgb_n_estimators': 300, 'xgb_learning_rate': 0.054082340576877566, 'xgb_max_depth': 4, 'xgb_min_child_weight': 1, 'xgb_subsample': 0.7932946965146986, 'xgb_colsample_bytree': 0.7975549966080241, 'xgb_gamma': 0.21888185350141923, 'lgb_n_estimators': 200, 'lgb_learning_rate': 0.11052057900398145, 'lgb_num_leaves': 36, 'lgb_max_depth': 3, 'lgb_min_child_samples': 32, 'cat_iterations': 250, 'cat_learning_rate': 0.04572073526670409, 'cat_depth': 7, 'cat_l2_leaf_reg': 2.9593800993624333, 'cat_class_weight_0_cb': 3.068198488145982, 'w_rf': 1.3550820367170993, 'w_gb': 0.5508382534881904, 'w_xgb': 0.7157828539866089, 'w_lgb': 0.5628583713734685, 'w_cat': 1.7728208225275608}. Best is trial 0 with value: 0.7949309605427345.\n",
      "[W 2025-05-07 17:53:22,300] Trial 3 failed with parameters: {'rf_n_estimators': 100, 'rf_max_depth': 8, 'rf_min_samples_split': 19, 'rf_min_samples_leaf': 3, 'rf_max_features': 'log2', 'gb_n_estimators': 100, 'gb_learning_rate': 0.015474297664393481, 'gb_max_depth': 7, 'gb_subsample': 0.9424361138693251, 'xgb_n_estimators': 200, 'xgb_learning_rate': 0.1059051751308571, 'xgb_max_depth': 7, 'xgb_min_child_weight': 2, 'xgb_subsample': 0.9677676995469933, 'xgb_colsample_bytree': 0.8618026725746952, 'xgb_gamma': 0.24223204654921876, 'lgb_n_estimators': 300, 'lgb_learning_rate': 0.023659257295689295, 'lgb_num_leaves': 20, 'lgb_max_depth': 4, 'lgb_min_child_samples': 23, 'cat_iterations': 250, 'cat_learning_rate': 0.10287212602939473, 'cat_depth': 4, 'cat_l2_leaf_reg': 3.071685783483027, 'cat_class_weight_0_cb': 2.752233009446337, 'w_rf': 0.9442156209414605, 'w_gb': 0.7397307346673656, 'w_xgb': 1.175230342807256, 'w_lgb': 2.3858194078250383, 'w_cat': 1.1464058640415105} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_20992/4082387293.py\", line 540, in objective\n",
      "    ensemble.fit(X_train, y_train)\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 419, in fit\n",
      "    return super().fit(X, transformed_y, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 100, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/joblib/parallel.py\", line 1985, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/joblib/parallel.py\", line 1913, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_base.py\", line 39, in _fit_single_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/sklearn.py\", line 1682, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py\", line 2247, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-05-07 17:53:22,312] Trial 3 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 553\u001B[39m\n\u001B[32m    551\u001B[39m study = optuna.create_study(direction=\u001B[33m'\u001B[39m\u001B[33mmaximize\u001B[39m\u001B[33m'\u001B[39m, sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n\u001B[32m    552\u001B[39m N_OPTUNA_TRIALS = \u001B[32m10\u001B[39m \u001B[38;5;66;03m# IMPORTANT: Increase for real HPO (e.g., 50, 100+)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m553\u001B[39m \u001B[43mstudy\u001B[49m\u001B[43m.\u001B[49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mN_OPTUNA_TRIALS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3600\u001B[39;49m\u001B[43m*\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# Example: 3 hour timeout\u001B[39;00m\n\u001B[32m    555\u001B[39m best_params = study.best_params\n\u001B[32m    556\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mBest ROC AUC from Optuna: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudy.best_value\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/study.py:475\u001B[39m, in \u001B[36mStudy.optimize\u001B[39m\u001B[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m    373\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34moptimize\u001B[39m(\n\u001B[32m    374\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    375\u001B[39m     func: ObjectiveFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m    382\u001B[39m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    383\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    384\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[32m    385\u001B[39m \n\u001B[32m    386\u001B[39m \u001B[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    473\u001B[39m \u001B[33;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[32m    474\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m475\u001B[39m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    476\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    477\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    478\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    479\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    480\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    482\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    483\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001B[39m, in \u001B[36m_optimize\u001B[39m\u001B[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     62\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs == \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     72\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     76\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs == -\u001B[32m1\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001B[39m, in \u001B[36m_optimize_sequential\u001B[39m\u001B[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[39m\n\u001B[32m    157\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m160\u001B[39m     frozen_trial = \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[32m    163\u001B[39m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[32m    164\u001B[39m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[32m    165\u001B[39m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    241\u001B[39m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mShould not reach.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    243\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    244\u001B[39m     frozen_trial.state == TrialState.FAIL\n\u001B[32m    245\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    246\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[32m    247\u001B[39m ):\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    195\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001B[32m    196\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m197\u001B[39m         value_or_values = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    198\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions.TrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    199\u001B[39m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[32m    200\u001B[39m         state = TrialState.PRUNED\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 540\u001B[39m, in \u001B[36mobjective\u001B[39m\u001B[34m(trial)\u001B[39m\n\u001B[32m    537\u001B[39m ensemble = VotingClassifier(estimators=[(\u001B[33m'\u001B[39m\u001B[33mrf\u001B[39m\u001B[33m'\u001B[39m, rf), (\u001B[33m'\u001B[39m\u001B[33mgb\u001B[39m\u001B[33m'\u001B[39m, gb), (\u001B[33m'\u001B[39m\u001B[33mxgb\u001B[39m\u001B[33m'\u001B[39m, xgb_m), (\u001B[33m'\u001B[39m\u001B[33mlgb\u001B[39m\u001B[33m'\u001B[39m, lgb_m), (\u001B[33m'\u001B[39m\u001B[33mcat\u001B[39m\u001B[33m'\u001B[39m, cat_m)], voting=\u001B[33m'\u001B[39m\u001B[33msoft\u001B[39m\u001B[33m'\u001B[39m, weights=ensemble_weights)\n\u001B[32m    539\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m540\u001B[39m     \u001B[43mensemble\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    541\u001B[39m     y_val_proba = ensemble.predict_proba(X_val)[:, \u001B[32m1\u001B[39m]\n\u001B[32m    542\u001B[39m     fold_roc_auc_scores.append(roc_auc_score(y_val, y_val_proba))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:63\u001B[39m, in \u001B[36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     61\u001B[39m extra_args = \u001B[38;5;28mlen\u001B[39m(args) - \u001B[38;5;28mlen\u001B[39m(all_args)\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m extra_args <= \u001B[32m0\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[38;5;66;03m# extra_args > 0\u001B[39;00m\n\u001B[32m     66\u001B[39m args_msg = [\n\u001B[32m     67\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(name, arg)\n\u001B[32m     68\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m name, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(kwonly_args[:extra_args], args[-extra_args:])\n\u001B[32m     69\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py:419\u001B[39m, in \u001B[36mVotingClassifier.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, **fit_params)\u001B[39m\n\u001B[32m    416\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    417\u001B[39m     fit_params[\u001B[33m\"\u001B[39m\u001B[33msample_weight\u001B[39m\u001B[33m\"\u001B[39m] = sample_weight\n\u001B[32m--> \u001B[39m\u001B[32m419\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransformed_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py:100\u001B[39m, in \u001B[36m_BaseVoting.fit\u001B[39m\u001B[34m(self, X, y, **fit_params)\u001B[39m\n\u001B[32m     95\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33msample_weight\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m fit_params:\n\u001B[32m     96\u001B[39m             routed_params[name].fit[\u001B[33m\"\u001B[39m\u001B[33msample_weight\u001B[39m\u001B[33m\"\u001B[39m] = fit_params[\n\u001B[32m     97\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33msample_weight\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     98\u001B[39m             ]\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m \u001B[38;5;28mself\u001B[39m.estimators_ = \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    101\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_single_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    102\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    103\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    104\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    105\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfit\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    106\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessage_clsname\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mVoting\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_log_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mclfs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    108\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    109\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclfs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    110\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclf\u001B[49m\u001B[43m \u001B[49m\u001B[43m!=\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdrop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m    111\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[38;5;28mself\u001B[39m.named_estimators_ = Bunch()\n\u001B[32m    115\u001B[39m \u001B[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:77\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     72\u001B[39m config = get_config()\n\u001B[32m     73\u001B[39m iterable_with_config = (\n\u001B[32m     74\u001B[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     76\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/joblib/parallel.py:1985\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1983\u001B[39m     output = \u001B[38;5;28mself\u001B[39m._get_sequential_output(iterable)\n\u001B[32m   1984\u001B[39m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m1985\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[32m   1987\u001B[39m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[32m   1988\u001B[39m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[32m   1989\u001B[39m \u001B[38;5;66;03m# reused, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[32m   1990\u001B[39m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[32m   1991\u001B[39m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[32m   1992\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._lock:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/joblib/parallel.py:1913\u001B[39m, in \u001B[36mParallel._get_sequential_output\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1911\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_batches += \u001B[32m1\u001B[39m\n\u001B[32m   1912\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_tasks += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1913\u001B[39m res = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1914\u001B[39m \u001B[38;5;28mself\u001B[39m.n_completed_tasks += \u001B[32m1\u001B[39m\n\u001B[32m   1915\u001B[39m \u001B[38;5;28mself\u001B[39m.print_progress()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:139\u001B[39m, in \u001B[36m_FuncWrapper.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    137\u001B[39m     config = {}\n\u001B[32m    138\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(**config):\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_base.py:39\u001B[39m, in \u001B[36m_fit_single_estimator\u001B[39m\u001B[34m(estimator, X, y, fit_params, message_clsname, message)\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     38\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m         \u001B[43mestimator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m estimator\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py:729\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    727\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig.parameters, args):\n\u001B[32m    728\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/sklearn.py:1682\u001B[39m, in \u001B[36mXGBClassifier.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001B[39m\n\u001B[32m   1660\u001B[39m model, metric, params, feature_weights = \u001B[38;5;28mself\u001B[39m._configure_fit(\n\u001B[32m   1661\u001B[39m     xgb_model, params, feature_weights\n\u001B[32m   1662\u001B[39m )\n\u001B[32m   1663\u001B[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001B[32m   1664\u001B[39m     missing=\u001B[38;5;28mself\u001B[39m.missing,\n\u001B[32m   1665\u001B[39m     X=X,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1679\u001B[39m     feature_types=\u001B[38;5;28mself\u001B[39m.feature_types,\n\u001B[32m   1680\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m \u001B[38;5;28mself\u001B[39m._Booster = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1683\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1684\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1685\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1686\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[43m=\u001B[49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1687\u001B[39m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1688\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[43m=\u001B[49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1689\u001B[39m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1690\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcustom_metric\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1691\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1692\u001B[39m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1693\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1694\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1696\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m.objective):\n\u001B[32m   1697\u001B[39m     \u001B[38;5;28mself\u001B[39m.objective = params[\u001B[33m\"\u001B[39m\u001B[33mobjective\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py:729\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    727\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig.parameters, args):\n\u001B[32m    728\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/training.py:183\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001B[39m\n\u001B[32m    181\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001B[32m    182\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m183\u001B[39m \u001B[43mbst\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miteration\u001B[49m\u001B[43m=\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    184\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001B[32m    185\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py:2247\u001B[39m, in \u001B[36mBooster.update\u001B[39m\u001B[34m(self, dtrain, iteration, fobj)\u001B[39m\n\u001B[32m   2243\u001B[39m \u001B[38;5;28mself\u001B[39m._assign_dmatrix_features(dtrain)\n\u001B[32m   2245\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2246\u001B[39m     _check_call(\n\u001B[32m-> \u001B[39m\u001B[32m2247\u001B[39m         \u001B[43m_LIB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2248\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctypes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle\u001B[49m\n\u001B[32m   2249\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2250\u001B[39m     )\n\u001B[32m   2251\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2252\u001B[39m     pred = \u001B[38;5;28mself\u001B[39m.predict(dtrain, output_margin=\u001B[38;5;28;01mTrue\u001B[39;00m, training=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
