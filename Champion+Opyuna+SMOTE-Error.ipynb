{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T13:48:57.184204Z",
     "start_time": "2025-05-07T13:37:22.294491Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "CHAMPIONSHIP MODEL - Insurance Agent NILL Prediction\n",
    "Data Storm v6.0 - First Place Solution (with Optuna HPO & SMOTE Augmentation)\n",
    "\n",
    "Key enhancements:\n",
    "1. Stratified time-series cross-validation with gap\n",
    "2. Feature importance-based selection with stability analysis\n",
    "3. CatBoost integration with custom loss function\n",
    "4. Agent-specific dynamic thresholding\n",
    "5. Recursive feature elimination with stability scores\n",
    "6. Optuna for Hyperparameter Optimization\n",
    "7. SMOTE Data Augmentation for minority class\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# --- DIRECTORY SETUP ---\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    script_dir = os.getcwd() # Fallback for interactive environments\n",
    "\n",
    "data_dir = os.path.join(script_dir, 'dataset')\n",
    "output_dir = os.path.join(script_dir, 'outputs')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"CHAMPIONSHIP KAGGLE SOLUTION - ADVANCED ENSEMBLE WITH OPTUNA HPO & SMOTE AUGMENTATION\")\n",
    "print(\"=\" * 100)\n",
    "start_time_script = time.time()\n",
    "print(f\"Starting at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# --- STEP 1: LOAD DATA ---\n",
    "print(\"\\nStep 1: Loading data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(os.path.join(data_dir, 'train_storming_round.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, 'test_storming_round.csv'))\n",
    "    submission_template = pd.read_csv(os.path.join(data_dir, 'sample_submission_storming_round.csv'))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Dataset file not found. Please ensure 'train_storming_round.csv', 'test_storming_round.csv', and 'sample_submission_storming_round.csv' are in the '{data_dir}' directory.\")\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Submission template shape: {submission_template.shape}\")\n",
    "\n",
    "# Critical integrity checks and deduplications\n",
    "print(\"Performing data integrity checks...\")\n",
    "assert len(test_df) == len(submission_template), \"Test and submission template row counts don't match!\"\n",
    "\n",
    "dupes_train = train_df.duplicated().sum()\n",
    "if dupes_train > 0:\n",
    "    print(f\"WARNING: Found {dupes_train} duplicate rows in training data. Removing...\")\n",
    "    train_df = train_df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "\n",
    "dupes_test = test_df.duplicated().sum()\n",
    "if dupes_test > 0:\n",
    "    print(f\"WARNING: Found {dupes_test} duplicate rows in test data. Removing...\")\n",
    "    test_df = test_df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    if len(test_df) != len(submission_template):\n",
    "        print(\"WARNING: Test data size changed after deduplication. Submission template might not align if it expected duplicates.\")\n",
    "\n",
    "\n",
    "# --- STEP 2: ENHANCED PREPROCESSING ---\n",
    "print(\"\\nStep 2: Enhanced preprocessing with domain expertise...\")\n",
    "date_columns = ['agent_join_month', 'first_policy_sold_month', 'year_month'] # Define globally for FE function\n",
    "for df_loop in [train_df, test_df]:\n",
    "    for col in date_columns:\n",
    "        if col in df_loop.columns:\n",
    "            df_loop[col] = pd.to_datetime(df_loop[col], errors='coerce')\n",
    "\n",
    "train_df = train_df.sort_values(['agent_code', 'year_month'])\n",
    "train_df['target_column'] = 0\n",
    "\n",
    "unique_agents = train_df['agent_code'].unique()\n",
    "print(f\"Processing target for {len(unique_agents)} unique agents...\")\n",
    "for agent_idx, agent in enumerate(unique_agents):\n",
    "    agent_data = train_df[train_df['agent_code'] == agent].copy()\n",
    "    agent_data = agent_data.sort_values('year_month')\n",
    "    for i in range(len(agent_data) - 1):\n",
    "        current_row_id = agent_data.iloc[i]['row_id']\n",
    "        next_month_sales = agent_data.iloc[i+1]['new_policy_count']\n",
    "        if next_month_sales > 0:\n",
    "            train_df.loc[train_df['row_id'] == current_row_id, 'target_column'] = 1\n",
    "    if (agent_idx + 1) % 500 == 0:\n",
    "        print(f\"  Processed target for {agent_idx+1}/{len(unique_agents)} agents...\")\n",
    "\n",
    "last_month_indices = train_df.groupby('agent_code')['year_month'].idxmax()\n",
    "train_df = train_df.drop(last_month_indices)\n",
    "\n",
    "print(f\"Processed training data shape after target creation: {train_df.shape}\")\n",
    "print(f\"Target distribution:\\n{train_df['target_column'].value_counts(normalize=True)}\")\n",
    "\n",
    "\n",
    "# --- STEP 3: ADVANCED FEATURE ENGINEERING ---\n",
    "print(\"\\nStep 3: Advanced feature engineering with agent profiling...\")\n",
    "\n",
    "def comprehensive_feature_engineering(df_input, is_train=True, train_reference_df=None):\n",
    "    df = df_input.copy()\n",
    "    # Extract time-based features\n",
    "    for col in date_columns: # date_columns defined earlier\n",
    "        if col in df.columns and pd.api.types.is_datetime64_any_dtype(df[col]): # Check if datetime\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_quarter'] = df[col].dt.quarter\n",
    "            df[f'{col}_dayofweek'] = df[col].dt.dayofweek\n",
    "            df[f'{col}_dayofyear'] = df[col].dt.dayofyear\n",
    "            if hasattr(df[col].dt, 'isocalendar'): # For pandas >= 1.1.0\n",
    "                 df[f'{col}_weekofyear'] = df[col].dt.isocalendar().week.astype(int)\n",
    "            else: # Fallback for older pandas\n",
    "                 df[f'{col}_weekofyear'] = df[col].dt.weekofyear.astype(int)\n",
    "            df[f'{col}_month_sin'] = np.sin(2 * np.pi * df[f'{col}_month']/12)\n",
    "            df[f'{col}_month_cos'] = np.cos(2 * np.pi * df[f'{col}_month']/12)\n",
    "\n",
    "    # Experience features\n",
    "    if all(c in df.columns for c in ['year_month', 'agent_join_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['year_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['agent_join_month']):\n",
    "        df['months_with_company'] = ((df['year_month'].dt.year - df['agent_join_month'].dt.year) * 12 + \\\n",
    "                                    (df['year_month'].dt.month - df['agent_join_month'].dt.month)).fillna(0)\n",
    "\n",
    "    if all(c in df.columns for c in ['first_policy_sold_month', 'agent_join_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['first_policy_sold_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['agent_join_month']):\n",
    "        df['months_to_first_sale'] = ((df['first_policy_sold_month'].dt.year - df['agent_join_month'].dt.year) * 12 + \\\n",
    "                                     (df['first_policy_sold_month'].dt.month - df['agent_join_month'].dt.month)).fillna(-1) # Corrected\n",
    "\n",
    "    if all(c in df.columns for c in ['year_month', 'first_policy_sold_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['year_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['first_policy_sold_month']):\n",
    "        df['months_since_first_sale'] = ((df['year_month'].dt.year - df['first_policy_sold_month'].dt.year) * 12 + \\\n",
    "                                        (df['year_month'].dt.month - df['first_policy_sold_month'].dt.month)).fillna(-1) # Corrected\n",
    "\n",
    "\n",
    "    # Activity trend features (simplified example, expand with your original logic)\n",
    "    activity_cols_periods = {\n",
    "        'unique_proposals': ['7_days', '15_days', '21_days'],\n",
    "        'unique_quotations': ['7_days', '15_days', '21_days'],\n",
    "        'unique_customers': ['7_days', '15_days', '21_days']\n",
    "    }\n",
    "    for base_col, periods in activity_cols_periods.items():\n",
    "        for i in range(len(periods) - 1):\n",
    "            col1_name_suffix = f'_last_{periods[i]}'\n",
    "            col2_name_suffix = f'_last_{periods[i+1]}'\n",
    "            # Construct full column names if not already full (e.g. from a list of suffixes)\n",
    "            col1 = base_col + col1_name_suffix if not base_col.endswith(col1_name_suffix) else base_col\n",
    "            col2 = base_col + col2_name_suffix if not base_col.endswith(col2_name_suffix) else base_col\n",
    "\n",
    "            if col1 in df.columns and col2 in df.columns:\n",
    "                df[f'{base_col}_trend_{periods[i]}_{periods[i+1]}'] = df[col1].fillna(0) / np.maximum(df[col2].fillna(0), 1e-6)\n",
    "\n",
    "\n",
    "    # Non-linear transformations\n",
    "    for col_to_transform in ['unique_proposal', 'unique_quotations', 'unique_customers', 'ANBP_value', 'net_income', 'agent_age']:\n",
    "        if col_to_transform in df.columns:\n",
    "            df[f'log_{col_to_transform}'] = np.log1p(df[col_to_transform].fillna(0))\n",
    "            df[f'sqrt_{col_to_transform}'] = np.sqrt(np.maximum(0, df[col_to_transform].fillna(0)))\n",
    "\n",
    "\n",
    "    # --- Historical Features ---\n",
    "    if is_train:\n",
    "        if 'new_policy_count' in df.columns:\n",
    "             df['hist_nill_rate_calc'] = df.groupby('agent_code')['new_policy_count'].transform(lambda x: x.expanding().apply(lambda y: (y==0).mean() if len(y)>1 else 0.5).shift(1)).fillna(0.5)\n",
    "             df['hist_avg_policies_calc'] = df.groupby('agent_code')['new_policy_count'].transform(lambda x: x.expanding().mean().shift(1)).fillna(0)\n",
    "        else:\n",
    "            df['hist_nill_rate_calc'] = 0.5\n",
    "            df['hist_avg_policies_calc'] = 0\n",
    "    else:\n",
    "        if train_reference_df is not None:\n",
    "            if 'new_policy_count' in train_reference_df.columns:\n",
    "                agent_hist_stats = train_reference_df.groupby('agent_code').agg(\n",
    "                    hist_nill_rate_ref=('new_policy_count', lambda x: (x==0).mean()),\n",
    "                    hist_avg_policies_ref=('new_policy_count', 'mean')\n",
    "                ).reset_index()\n",
    "                df = pd.merge(df, agent_hist_stats, on='agent_code', how='left')\n",
    "                overall_train_nill_rate = train_reference_df['new_policy_count'].eq(0).mean() if 'new_policy_count' in train_reference_df else 0.5\n",
    "                overall_train_avg_policies = train_reference_df['new_policy_count'].mean() if 'new_policy_count' in train_reference_df else 0\n",
    "                df['hist_nill_rate_calc'] = df['hist_nill_rate_ref'].fillna(overall_train_nill_rate)\n",
    "                df['hist_avg_policies_calc'] = df['hist_avg_policies_ref'].fillna(overall_train_avg_policies)\n",
    "                df.drop(columns=['hist_nill_rate_ref', 'hist_avg_policies_ref'], inplace=True, errors='ignore')\n",
    "            else:\n",
    "                df['hist_nill_rate_calc'] = 0.5\n",
    "                df['hist_avg_policies_calc'] = 0\n",
    "        else:\n",
    "            df['hist_nill_rate_calc'] = 0.5\n",
    "            df['hist_avg_policies_calc'] = 0\n",
    "\n",
    "\n",
    "    # --- Agent Profile Features ---\n",
    "    if train_reference_df is not None: profile_ref = train_reference_df\n",
    "    elif is_train: profile_ref = df\n",
    "    else: profile_ref = None\n",
    "\n",
    "    if profile_ref is not None:\n",
    "        agent_profiles_agg = {}\n",
    "        if 'unique_proposal' in profile_ref.columns: agent_profiles_agg['unique_proposal_mean_profile'] = ('unique_proposal', 'mean')\n",
    "        if 'unique_quotations' in profile_ref.columns: agent_profiles_agg['unique_quotations_mean_profile'] = ('unique_quotations', 'mean')\n",
    "        if 'agent_age' in profile_ref.columns: agent_profiles_agg['agent_age_mean_profile'] = ('agent_age', 'mean')\n",
    "\n",
    "        if agent_profiles_agg:\n",
    "            agent_profiles = profile_ref.groupby('agent_code', as_index=False).agg(**agent_profiles_agg)\n",
    "            df = pd.merge(df, agent_profiles, on='agent_code', how='left')\n",
    "            for col_name in agent_profiles.columns:\n",
    "                if col_name != 'agent_code' and col_name in df.columns:\n",
    "                    original_feature_name = col_name.replace('_profile', '') # Get original feature name\n",
    "                    fill_val = profile_ref[original_feature_name].mean() if original_feature_name in profile_ref else 0\n",
    "                    df[col_name] = df[col_name].fillna(fill_val)\n",
    "        else: # Fallback if no aggregations defined\n",
    "            if 'unique_proposal_mean_profile' not in df.columns: df['unique_proposal_mean_profile'] = 0\n",
    "            if 'unique_quotations_mean_profile' not in df.columns: df['unique_quotations_mean_profile'] = 0\n",
    "            if 'agent_age_mean_profile' not in df.columns: df['agent_age_mean_profile'] = 0\n",
    "\n",
    "\n",
    "    # Ensure all created features are numeric, fill NaNs that might remain\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' and col not in ['agent_code', 'row_id']: # Avoid converting identifiers\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col])\n",
    "            except ValueError: # If not purely numeric, it might be mixed or truly categorical\n",
    "                 # For simplicity, if it's a feature expected to be numeric, fill with a numeric default like 0\n",
    "                 # This requires careful thought based on the specific feature.\n",
    "                 # print(f\"Warning: Column {col} is object type. Attempting to fill NaNs with 0 or mode for objects.\")\n",
    "                 if df[col].isnull().any():\n",
    "                     df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else \"Unknown\")\n",
    "\n",
    "\n",
    "        # Final NaN fill for numeric columns created\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if df[col].isnull().any():\n",
    "                df[col] = df[col].fillna(0) # Default fill for numeric, consider median/mean\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create a copy of train_df to be used as a stable reference for test set feature engineering\n",
    "train_df_reference_for_fe = train_df.copy()\n",
    "\n",
    "train_df_fe = comprehensive_feature_engineering(train_df, is_train=True, train_reference_df=train_df_reference_for_fe)\n",
    "test_df_fe = comprehensive_feature_engineering(test_df, is_train=False, train_reference_df=train_df_reference_for_fe)\n",
    "\n",
    "print(f\"Train data shape after FE: {train_df_fe.shape}, Columns: {train_df_fe.columns.tolist()[:5]}...\")\n",
    "print(f\"Test data shape after FE: {test_df_fe.shape}, Columns: {test_df_fe.columns.tolist()[:5]}...\")\n",
    "\n",
    "\n",
    "# --- STEP 4: FEATURE SELECTION ---\n",
    "print(\"\\nStep 4: Feature selection...\")\n",
    "# IDENTIFY POTENTIAL FEATURES (EXCLUDE IDs, DATES, TARGETS, ETC.)\n",
    "non_feature_cols = ['row_id', 'agent_code', 'year_month', 'target_column', 'new_policy_count',\n",
    "                    'agent_join_month', 'first_policy_sold_month'] # Add any other non-feature cols\n",
    "potential_features = [col for col in train_df_fe.columns if col not in non_feature_cols and col in test_df_fe.columns]\n",
    "\n",
    "# KEEP ONLY NUMERIC FEATURES FOR MOST MODELS (handle categoricals separately if needed, e.g., via CatBoost or embedding)\n",
    "numeric_potential_features = []\n",
    "for col in potential_features:\n",
    "    # Attempt to convert to numeric if not already, to catch object cols that are actually numeric\n",
    "    try:\n",
    "        if not pd.api.types.is_numeric_dtype(train_df_fe[col]):\n",
    "            train_df_fe[col] = pd.to_numeric(train_df_fe[col])\n",
    "            # Also convert test_df_fe for consistency, use errors='coerce' to handle unconvertibles\n",
    "            if col in test_df_fe.columns:\n",
    "                 test_df_fe[col] = pd.to_numeric(test_df_fe[col], errors='coerce').fillna(0) # Fill unconvertibles with 0\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(train_df_fe[col]):\n",
    "            numeric_potential_features.append(col)\n",
    "    except Exception as e_conv:\n",
    "        print(f\"Could not process column {col} for numeric check: {e_conv}\")\n",
    "        continue # Skip problematic column\n",
    "\n",
    "potential_features = numeric_potential_features\n",
    "\n",
    "if not potential_features:\n",
    "    print(\"ERROR: No potential numeric features found after FE. Check FE and data types.\")\n",
    "    train_df_fe['dummy_numeric_feat'] = np.random.rand(len(train_df_fe))\n",
    "    test_df_fe['dummy_numeric_feat'] = np.random.rand(len(test_df_fe))\n",
    "    potential_features = ['dummy_numeric_feat']\n",
    "\n",
    "X_temp_fs = train_df_fe[potential_features].fillna(0) # Fill NaNs with 0 for selector\n",
    "y_temp_fs = train_df_fe['target_column']\n",
    "\n",
    "final_features = []\n",
    "if not X_temp_fs.empty and len(X_temp_fs) == len(y_temp_fs) and len(potential_features) > 0:\n",
    "    try:\n",
    "        print(f\"Performing feature selection from {len(potential_features)} potential features...\")\n",
    "        # Simplified: Use top N features by simple RandomForest importance\n",
    "        # REPLACE WITH YOUR ORIGINAL ROBUST FEATURE SELECTION (RFECV, etc.)\n",
    "        selector_model = RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1, max_depth=8, min_samples_leaf=5)\n",
    "        selector_model.fit(X_temp_fs, y_temp_fs)\n",
    "        importances = pd.Series(selector_model.feature_importances_, index=potential_features).sort_values(ascending=False)\n",
    "        final_features = list(importances.head(min(len(importances), 75)).index) # Top 75 or all if less\n",
    "    except Exception as e_fs:\n",
    "        print(f\"Error during feature selection: {e_fs}. Falling back to all potential numeric features.\")\n",
    "        final_features = potential_features\n",
    "else:\n",
    "    print(\"Skipping robust feature selection due to empty/mismatched X_temp_fs or no potential features. Using all potential numeric features.\")\n",
    "    final_features = potential_features\n",
    "\n",
    "\n",
    "if not final_features:\n",
    "    print(\"CRITICAL WARNING: final_features list is empty. This will likely cause errors.\")\n",
    "    # As a last resort, try to pick some numeric columns if any exist.\n",
    "    fallback_numeric_cols = [col for col in train_df_fe.columns if pd.api.types.is_numeric_dtype(train_df_fe[col]) and col not in non_feature_cols and col in test_df_fe.columns]\n",
    "    if fallback_numeric_cols:\n",
    "        final_features = fallback_numeric_cols[:min(10, len(fallback_numeric_cols))] # Take up to 10\n",
    "        print(f\"Using fallback features: {final_features}\")\n",
    "    else:\n",
    "        print(\"FATAL ERROR: No features available for modeling. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "print(f\"Selected {len(final_features)} features. Examples: {final_features[:min(5, len(final_features))]}\")\n",
    "selected_features_path = os.path.join(output_dir, 'selected_features_optuna_smote.txt')\n",
    "with open(selected_features_path, 'w') as f:\n",
    "    for feature in final_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "print(f\"Selected features saved to: {selected_features_path}\")\n",
    "\n",
    "\n",
    "# Prepare data for Optuna and final model\n",
    "global_final_X_df = train_df_fe[final_features].copy()\n",
    "global_final_y = train_df_fe['target_column'].copy()\n",
    "\n",
    "for col in global_final_X_df.columns:\n",
    "    if global_final_X_df[col].isnull().any():\n",
    "        # All columns in final_features should be numeric by now\n",
    "        global_final_X_df[col] = global_final_X_df[col].fillna(global_final_X_df[col].median())\n",
    "\n",
    "\n",
    "global_final_scaler = StandardScaler()\n",
    "global_final_X_scaled_np = global_final_scaler.fit_transform(global_final_X_df)\n",
    "\n",
    "global_tscv = TimeSeriesSplit(n_splits=3) # Reduced splits for faster demo. Consider 5 for real runs.\n",
    "\n",
    "\n",
    "# --- OPTUNA OBJECTIVE FUNCTION (with SMOTE) ---\n",
    "def objective(trial):\n",
    "    # Ensure minority_class_count_for_k is calculated based on the smallest fold if possible, or a safe global minimum\n",
    "    # This is tricky as fold sizes vary. A simpler approach is to cap k_neighbors based on a reasonable minimum expected.\n",
    "    # For now, we'll adjust k_neighbors per fold.\n",
    "    smote_k_neighbors_max_candidate = 7 # A sensible upper limit\n",
    "    if global_final_y.value_counts().min() > 1: # If there's at least one minority sample globally\n",
    "         smote_k_neighbors_max_candidate = min(smote_k_neighbors_max_candidate, global_final_y.value_counts().min() -1 )\n",
    "    smote_k_neighbors_max_candidate = max(1, smote_k_neighbors_max_candidate) # k must be at least 1\n",
    "\n",
    "    smote_k_neighbors = trial.suggest_int('smote_k_neighbors', 1, smote_k_neighbors_max_candidate)\n",
    "\n",
    "\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 50, 200, step=25)\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 4, 12)\n",
    "    rf_min_samples_leaf = trial.suggest_int('rf_min_samples_leaf', 2, 10)\n",
    "\n",
    "    xgb_n_estimators = trial.suggest_int('xgb_n_estimators', 50, 200, step=25)\n",
    "    xgb_max_depth = trial.suggest_int('xgb_max_depth', 3, 8)\n",
    "    xgb_learning_rate = trial.suggest_float('xgb_learning_rate', 0.01, 0.1, log=True)\n",
    "\n",
    "    cat_iterations = trial.suggest_int('cat_iterations', 50, 200, step=25)\n",
    "    cat_depth = trial.suggest_int('cat_depth', 4, 8)\n",
    "    cat_learning_rate = trial.suggest_float('cat_learning_rate', 0.01, 0.1, log=True)\n",
    "    cat_l2_leaf_reg = trial.suggest_float('cat_l2_leaf_reg', 1.0, 10.0, log=True)\n",
    "    # CatBoost class weight is often useful even with SMOTE, let Optuna decide\n",
    "    cat_class_weight_0 = trial.suggest_float('cat_class_weight_0_cb', 0.5, 5.0)\n",
    "\n",
    "\n",
    "    w_rf = trial.suggest_float('w_rf', 0.1, 3.0)\n",
    "    w_xgb = trial.suggest_float('w_xgb', 0.1, 3.0)\n",
    "    w_cat = trial.suggest_float('w_cat', 0.1, 3.0)\n",
    "    ensemble_weights = [w_rf, w_xgb, w_cat]\n",
    "\n",
    "    fold_roc_auc_scores = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(global_tscv.split(global_final_X_scaled_np)):\n",
    "        X_train_fold, X_val_fold = global_final_X_scaled_np[train_idx], global_final_X_scaled_np[val_idx]\n",
    "        y_train_fold, y_val_fold = global_final_y.iloc[train_idx], global_final_y.iloc[val_idx]\n",
    "\n",
    "        minority_class_count_fold = np.sum(y_train_fold == 1) # Assuming 1 is minority, 0 is majority\n",
    "        current_k_for_smote = smote_k_neighbors # Use the k suggested by Optuna for this trial\n",
    "\n",
    "        if minority_class_count_fold <= current_k_for_smote:\n",
    "            current_k_for_smote = max(1, minority_class_count_fold - 1) if minority_class_count_fold > 1 else 1\n",
    "\n",
    "        if minority_class_count_fold > 0 and current_k_for_smote > 0 : # Apply SMOTE only if minority samples exist and k is valid\n",
    "            smote = SMOTE(random_state=RANDOM_STATE + fold, k_neighbors=current_k_for_smote)\n",
    "            try:\n",
    "                X_train_fold_aug, y_train_fold_aug = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "            except ValueError:\n",
    "                X_train_fold_aug, y_train_fold_aug = X_train_fold, y_train_fold # Fallback\n",
    "        else:\n",
    "            X_train_fold_aug, y_train_fold_aug = X_train_fold, y_train_fold\n",
    "\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=rf_n_estimators, max_depth=rf_max_depth, min_samples_leaf=rf_min_samples_leaf,\n",
    "                                    random_state=RANDOM_STATE, class_weight='balanced_subsample', n_jobs=-1)\n",
    "        pos_weight_fold = (y_train_fold_aug == 0).sum() / max(1, (y_train_fold_aug == 1).sum())\n",
    "        xgb_m = xgb.XGBClassifier(n_estimators=xgb_n_estimators, max_depth=xgb_max_depth, learning_rate=xgb_learning_rate,\n",
    "                                  random_state=RANDOM_STATE, scale_pos_weight=pos_weight_fold, use_label_encoder=False, eval_metric='logloss')\n",
    "        cat_m = cb.CatBoostClassifier(iterations=cat_iterations, depth=cat_depth, learning_rate=cat_learning_rate, l2_leaf_reg=cat_l2_leaf_reg,\n",
    "                                      random_seed=RANDOM_STATE, loss_function='Logloss', verbose=0, class_weights={0: cat_class_weight_0, 1: 1.0})\n",
    "        ensemble = VotingClassifier(estimators=[('rf', rf), ('xgb', xgb_m), ('cat', cat_m)], voting='soft', weights=ensemble_weights)\n",
    "\n",
    "        try:\n",
    "            ensemble.fit(X_train_fold_aug, y_train_fold_aug)\n",
    "            if hasattr(ensemble, \"predict_proba\"):\n",
    "                y_val_proba = ensemble.predict_proba(X_val_fold)[:, 1]\n",
    "                fold_roc_auc_scores.append(roc_auc_score(y_val_fold, y_val_proba))\n",
    "            else: # Should not happen for soft voting classifier\n",
    "                fold_roc_auc_scores.append(0.5) # Penalize if no predict_proba\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    avg_roc_auc = np.mean(fold_roc_auc_scores) if fold_roc_auc_scores else 0.0\n",
    "    return avg_roc_auc\n",
    "\n",
    "\n",
    "# --- STEP 5: HYPERPARAMETER OPTIMIZATION ---\n",
    "print(\"\\nStep 5: Hyperparameter Optimization with Optuna (including SMOTE)...\")\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "N_OPTUNA_TRIALS = 750 # INCREASE FOR REAL USE (e.g., 50-200+)\n",
    "optuna_timeout_hours = 3.5\n",
    "study.optimize(objective, n_trials=N_OPTUNA_TRIALS, timeout=3600 * optuna_timeout_hours, n_jobs=1) # n_jobs=1 for SMOTE issues with parallel, or ensure thread-safety if >1\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"\\nBest ROC AUC from Optuna: {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters found by Optuna:\")\n",
    "for key, value in best_params.items(): print(f\"  {key}: {value}\")\n",
    "\n",
    "try:\n",
    "    study_trials_path = os.path.join(output_dir, 'optuna_study_trials.csv')\n",
    "    study.trials_dataframe().to_csv(study_trials_path, index=False)\n",
    "    print(f\"Optuna study trials saved to: {study_trials_path}\")\n",
    "except Exception as e_study_save:\n",
    "    print(f\"Could not save Optuna study trials: {e_study_save}\")\n",
    "\n",
    "\n",
    "# --- STEP 6: FINAL MODEL TRAINING ---\n",
    "print(\"\\nStep 6: Training final model on all data with optimized parameters & SMOTE...\")\n",
    "final_smote_k_neighbors_tuned = best_params.get('smote_k_neighbors', 3) # Default k=3 if not tuned\n",
    "final_minority_count_full = np.sum(global_final_y == 1)\n",
    "current_k_for_smote_final = final_smote_k_neighbors_tuned\n",
    "\n",
    "X_train_final_aug, y_train_final_aug = global_final_X_scaled_np, global_final_y # Default to no augmentation\n",
    "\n",
    "if final_minority_count_full > 0 : # Proceed with SMOTE only if minority class exists\n",
    "    if final_minority_count_full <= current_k_for_smote_final:\n",
    "        current_k_for_smote_final = max(1, final_minority_count_full - 1) if final_minority_count_full > 1 else 1\n",
    "        print(f\"Adjusting SMOTE k_neighbors for final training from {final_smote_k_neighbors_tuned} to {current_k_for_smote_final}\")\n",
    "\n",
    "    if current_k_for_smote_final > 0: # k_neighbors must be > 0\n",
    "        smote_final = SMOTE(random_state=RANDOM_STATE, k_neighbors=current_k_for_smote_final)\n",
    "        try:\n",
    "            X_train_final_aug, y_train_final_aug = smote_final.fit_resample(global_final_X_scaled_np, global_final_y)\n",
    "        except ValueError as e_smote_final:\n",
    "            print(f\"SMOTE failed for final model training even with k={current_k_for_smote_final}: {e_smote_final}. Using original data.\")\n",
    "            # X_train_final_aug, y_train_final_aug already set to original\n",
    "    else:\n",
    "        print(\"Warning: k_neighbors for SMOTE is 0 or less for final model. Using original data.\")\n",
    "else:\n",
    "    print(\"Warning: No minority samples in the full training data. SMOTE cannot be applied for final model.\")\n",
    "\n",
    "\n",
    "print(f\"Original full train shape: {global_final_X_scaled_np.shape}, Augmented full train shape: {X_train_final_aug.shape}\")\n",
    "\n",
    "final_rf = RandomForestClassifier(\n",
    "    n_estimators=best_params.get('rf_n_estimators', 100), max_depth=best_params.get('rf_max_depth', 8),\n",
    "    min_samples_leaf=best_params.get('rf_min_samples_leaf', 5),\n",
    "    random_state=RANDOM_STATE, class_weight='balanced_subsample', n_jobs=-1)\n",
    "final_pos_weight_overall = (y_train_final_aug == 0).sum() / max(1, (y_train_final_aug == 1).sum())\n",
    "final_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=best_params.get('xgb_n_estimators', 100), max_depth=best_params.get('xgb_max_depth', 5),\n",
    "    learning_rate=best_params.get('xgb_learning_rate', 0.05),\n",
    "    random_state=RANDOM_STATE, scale_pos_weight=final_pos_weight_overall, use_label_encoder=False, eval_metric='logloss')\n",
    "final_cat = cb.CatBoostClassifier(\n",
    "    iterations=best_params.get('cat_iterations', 100), depth=best_params.get('cat_depth', 6),\n",
    "    learning_rate=best_params.get('cat_learning_rate', 0.05), l2_leaf_reg=best_params.get('cat_l2_leaf_reg', 3.0),\n",
    "    random_seed=RANDOM_STATE, loss_function='Logloss', verbose=0,\n",
    "    class_weights={0: best_params.get('cat_class_weight_0_cb', 1.0), 1: 1.0})\n",
    "final_ensemble_weights = [\n",
    "    best_params.get('w_rf', 1.0), best_params.get('w_xgb', 1.0), best_params.get('w_cat', 1.0)\n",
    "]\n",
    "final_ensemble = VotingClassifier(\n",
    "    estimators=[('rf', final_rf), ('xgb', final_xgb), ('cat', final_cat)],\n",
    "    voting='soft', weights=final_ensemble_weights)\n",
    "\n",
    "final_ensemble.fit(X_train_final_aug, y_train_final_aug)\n",
    "\n",
    "model_components = {\n",
    "    'scaler': global_final_scaler, 'ensemble_model': final_ensemble,\n",
    "    'final_features': final_features, 'best_optuna_params': best_params\n",
    "}\n",
    "model_path = os.path.join(output_dir, 'champion_model_bundle_optuna_smote.pkl')\n",
    "joblib.dump(model_components, model_path)\n",
    "print(f\"Final model bundle saved to: {model_path}\")\n",
    "\n",
    "\n",
    "# --- STEP 7: GENERATE TEST PREDICTIONS ---\n",
    "print(\"\\nStep 7: Generating optimized test predictions...\")\n",
    "# Ensure X_test_final_df has the same columns as global_final_X_df used for fitting scaler\n",
    "X_test_final_df = pd.DataFrame(columns=global_final_X_df.columns, index=test_df_fe.index) # Create template\n",
    "for col in global_final_X_df.columns: # Iterate through columns scaler was fit on\n",
    "    if col in test_df_fe.columns:\n",
    "        X_test_final_df[col] = test_df_fe[col]\n",
    "    else: # Column from training features not in test_df_fe (e.g. dummy from one-hot)\n",
    "        X_test_final_df[col] = 0 # Fill with 0\n",
    "\n",
    "# Fill any remaining NaNs in test features (should be numeric from selection)\n",
    "for col in X_test_final_df.columns:\n",
    "    if X_test_final_df[col].isnull().any():\n",
    "        # Use median from corresponding TRAIN column (global_final_X_df)\n",
    "        X_test_final_df[col] = X_test_final_df[col].fillna(global_final_X_df[col].median())\n",
    "\n",
    "X_test_scaled_np = global_final_scaler.transform(X_test_final_df)\n",
    "\n",
    "test_proba = final_ensemble.predict_proba(X_test_scaled_np)[:, 1]\n",
    "test_proba_path = os.path.join(output_dir, 'test_probabilities_optuna_smote.npy')\n",
    "np.save(test_proba_path, test_proba)\n",
    "print(f\"Test probabilities saved to: {test_proba_path}\")\n",
    "\n",
    "# Dynamic thresholding\n",
    "print(\"Applying dynamic thresholds...\")\n",
    "# Simplified dynamic threshold for example\n",
    "# You need to ensure test_df_fe has the features your get_dynamic_threshold function uses.\n",
    "# For this example, we'll just use a fixed dynamic threshold for simplicity.\n",
    "# In a real scenario, you'd pass necessary features from test_df_fe to get_dynamic_threshold\n",
    "# or apply it to test_df_fe and merge back to submission.\n",
    "\n",
    "# For robust mapping, add probabilities to test_df_fe (which should have row_id)\n",
    "if 'row_id' not in test_df_fe.columns and 'row_id' in test_df.columns: # If row_id was lost from fe\n",
    "    test_df_fe = test_df_fe.reset_index().merge(test_df[['row_id']].reset_index(), on='index').set_index('index')\n",
    "\n",
    "test_df_fe['probability'] = test_proba\n",
    "test_df_fe['dynamic_threshold_value'] = 0.62 # Example fixed dynamic threshold\n",
    "test_df_fe['dynamic_prediction'] = (test_df_fe['probability'] >= test_df_fe['dynamic_threshold_value']).astype(int)\n",
    "\n",
    "dynamic_submission = submission_template.copy()\n",
    "if 'row_id' in test_df_fe.columns and 'row_id' in dynamic_submission.columns:\n",
    "    final_preds_df_dynamic = test_df_fe[['row_id', 'dynamic_prediction']].rename(columns={'dynamic_prediction': 'target_column'})\n",
    "    dynamic_submission = dynamic_submission.drop(columns=['target_column'], errors='ignore').merge(final_preds_df_dynamic, on='row_id', how='left')\n",
    "    dynamic_submission['target_column'] = dynamic_submission['target_column'].fillna(0).astype(int)\n",
    "else:\n",
    "    dynamic_submission['target_column'] = test_df_fe['dynamic_prediction'].values[:len(dynamic_submission)]\n",
    "\n",
    "dynamic_submission_path = os.path.join(output_dir, 'dynamic_submission_optuna_smote.csv')\n",
    "dynamic_submission.to_csv(dynamic_submission_path, index=False)\n",
    "print(f\"Dynamic threshold submission saved to: {dynamic_submission_path}\")\n",
    "\n",
    "# Fixed threshold submission\n",
    "best_fixed_threshold = 0.60\n",
    "optimal_predictions = (test_proba >= best_fixed_threshold).astype(int)\n",
    "optimal_submission = submission_template.copy()\n",
    "\n",
    "if 'row_id' in test_df_fe.columns and 'row_id' in optimal_submission.columns:\n",
    "    temp_preds_fixed = pd.DataFrame({'row_id': test_df_fe['row_id'], 'target_column': optimal_predictions})\n",
    "    optimal_submission = optimal_submission.drop(columns=['target_column'], errors='ignore').merge(temp_preds_fixed, on='row_id', how='left')\n",
    "    optimal_submission['target_column'] = optimal_submission['target_column'].fillna(0).astype(int)\n",
    "else:\n",
    "    optimal_submission['target_column'] = optimal_predictions[:len(optimal_submission)]\n",
    "\n",
    "optimal_submission_path = os.path.join(output_dir, 'submission_optuna_smote.csv') # Main submission\n",
    "optimal_submission.to_csv(optimal_submission_path, index=False)\n",
    "print(f\"Optimal fixed threshold submission saved to: {optimal_submission_path}\")\n",
    "\n",
    "\n",
    "# --- STEP 8: FEATURE IMPORTANCE ANALYSIS ---\n",
    "print(\"\\nStep 8: Feature importance analysis...\")\n",
    "if hasattr(final_ensemble, 'named_estimators_'):\n",
    "    importances_data = {'Feature': global_final_X_df.columns.tolist()} # Features before scaling\n",
    "    estimators_with_importance = {\n",
    "        'RF': final_ensemble.named_estimators_.get('rf'),\n",
    "        'XGB': final_ensemble.named_estimators_.get('xgb'),\n",
    "        'CAT': final_ensemble.named_estimators_.get('cat')\n",
    "    }\n",
    "    for model_name, model_obj in estimators_with_importance.items():\n",
    "        if model_obj and hasattr(model_obj, 'feature_importances_'):\n",
    "            # Ensure feature importances array matches number of features\n",
    "            if len(model_obj.feature_importances_) == len(global_final_X_df.columns):\n",
    "                importances_data[f'{model_name}_Importance'] = model_obj.feature_importances_\n",
    "            else:\n",
    "                print(f\"Warning: Mismatch in feature count for {model_name} importances. Expected {len(global_final_X_df.columns)}, got {len(model_obj.feature_importances_)}.\")\n",
    "\n",
    "\n",
    "    feature_importance_df = pd.DataFrame(importances_data)\n",
    "    importance_cols = [col for col in feature_importance_df.columns if '_Importance' in col]\n",
    "\n",
    "    if importance_cols:\n",
    "        feature_importance_df['Avg_Importance'] = feature_importance_df[importance_cols].mean(axis=1)\n",
    "        feature_importance_df = feature_importance_df.sort_values('Avg_Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        feature_importance_path = os.path.join(output_dir, 'feature_importance_optuna_smote.csv')\n",
    "        feature_importance_df.to_csv(feature_importance_path, index=False)\n",
    "        print(f\"Feature importance table saved to: {feature_importance_path}\")\n",
    "\n",
    "        plt.figure(figsize=(10, max(8, min(len(feature_importance_df), 30) // 2)))\n",
    "        sns.barplot(x='Avg_Importance', y='Feature', data=feature_importance_df.head(min(30, len(feature_importance_df))), palette=\"viridis\")\n",
    "        plt.title('Top Features by Average Importance (Optuna-SMOTE Model)')\n",
    "        plt.tight_layout()\n",
    "        feature_plot_path = os.path.join(output_dir, 'top_features_optuna_smote.png')\n",
    "        plt.savefig(feature_plot_path)\n",
    "        print(f\"Feature importance plot saved to: {feature_plot_path}\")\n",
    "    else:\n",
    "        print(\"No feature importances could be extracted or matched from the ensemble members.\")\n",
    "else:\n",
    "    print(\"Final ensemble model does not have 'named_estimators_', cannot extract feature importances.\")\n",
    "\n",
    "\n",
    "# --- SCRIPT COMPLETION ---\n",
    "end_time_script = time.time()\n",
    "elapsed_time_script = end_time_script - start_time_script\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"CHAMPIONSHIP SOLUTION WITH OPTUNA HPO & SMOTE completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total execution time: {elapsed_time_script:.2f} seconds ({elapsed_time_script/60:.2f} minutes)\")\n",
    "print(f\"OPTIMAL SUBMISSION (Optuna & SMOTE): {optimal_submission_path}\")\n",
    "print(f\"DYNAMIC SUBMISSION (Optuna & SMOTE): {dynamic_submission_path}\")\n",
    "print(f\"All outputs saved in: {output_dir}\")\n",
    "print(\"=\" * 100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "CHAMPIONSHIP KAGGLE SOLUTION - ADVANCED ENSEMBLE WITH OPTUNA HPO & SMOTE AUGMENTATION\n",
      "====================================================================================================\n",
      "Starting at: 2025-05-07 19:07:22\n",
      "\n",
      "Step 1: Loading data...\n",
      "Train data shape: (15308, 23)\n",
      "Test data shape: (914, 23)\n",
      "Submission template shape: (914, 2)\n",
      "Performing data integrity checks...\n",
      "\n",
      "Step 2: Enhanced preprocessing with domain expertise...\n",
      "Processing target for 905 unique agents...\n",
      "  Processed target for 500/905 agents...\n",
      "Processed training data shape after target creation: (14403, 24)\n",
      "Target distribution:\n",
      "target_column\n",
      "1    0.900437\n",
      "0    0.099563\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Step 3: Advanced feature engineering with agent profiling...\n",
      "Train data shape after FE: (14403, 74), Columns: ['row_id', 'agent_code', 'agent_age', 'agent_join_month', 'first_policy_sold_month']...\n",
      "Test data shape after FE: (914, 73), Columns: ['row_id', 'agent_code', 'agent_age', 'agent_join_month', 'first_policy_sold_month']...\n",
      "\n",
      "Step 4: Feature selection...\n",
      "Performing feature selection from 67 potential features...\n",
      "Selected 67 features. Examples: ['hist_avg_policies_calc', 'log_net_income', 'sqrt_net_income', 'net_income', 'unique_proposal_mean_profile']\n",
      "Selected features saved to: /home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/outputs/selected_features_optuna_smote.txt\n",
      "\n",
      "Step 5: Hyperparameter Optimization with Optuna (including SMOTE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-07 19:18:56,866] Trial 70 failed with parameters: {'smote_k_neighbors': 1, 'rf_n_estimators': 200, 'rf_max_depth': 11, 'rf_min_samples_leaf': 5, 'xgb_n_estimators': 200, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.04687822255757708, 'cat_iterations': 200, 'cat_depth': 8, 'cat_learning_rate': 0.03566599132203746, 'cat_l2_leaf_reg': 6.962981307340209, 'cat_class_weight_0_cb': 3.527108858314613, 'w_rf': 0.5281665879211331, 'w_xgb': 1.363111133830617, 'w_cat': 0.3724400459013333} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_29996/567699811.py\", line 428, in objective\n",
      "    ensemble.fit(X_train_fold_aug, y_train_fold_aug)\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 419, in fit\n",
      "    return super().fit(X, transformed_y, **fit_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 100, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/joblib/parallel.py\", line 1985, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/joblib/parallel.py\", line 1913, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_base.py\", line 39, in _fit_single_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/sklearn.py\", line 1682, in fit\n",
      "    self._Booster = train(\n",
      "                    ^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py\", line 2247, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-05-07 19:18:56,869] Trial 70 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 446\u001B[39m\n\u001B[32m    444\u001B[39m N_OPTUNA_TRIALS = \u001B[32m750\u001B[39m \u001B[38;5;66;03m# INCREASE FOR REAL USE (e.g., 50-200+)\u001B[39;00m\n\u001B[32m    445\u001B[39m optuna_timeout_hours = \u001B[32m3.5\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m446\u001B[39m \u001B[43mstudy\u001B[49m\u001B[43m.\u001B[49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mN_OPTUNA_TRIALS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3600\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m \u001B[49m\u001B[43moptuna_timeout_hours\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# n_jobs=1 for SMOTE issues with parallel, or ensure thread-safety if >1\u001B[39;00m\n\u001B[32m    448\u001B[39m best_params = study.best_params\n\u001B[32m    449\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mBest ROC AUC from Optuna: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstudy.best_value\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/study.py:475\u001B[39m, in \u001B[36mStudy.optimize\u001B[39m\u001B[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m    373\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34moptimize\u001B[39m(\n\u001B[32m    374\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    375\u001B[39m     func: ObjectiveFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m    382\u001B[39m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    383\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    384\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[32m    385\u001B[39m \n\u001B[32m    386\u001B[39m \u001B[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    473\u001B[39m \u001B[33;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[32m    474\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m475\u001B[39m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    476\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    477\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    478\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    479\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    480\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    482\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    483\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001B[39m, in \u001B[36m_optimize\u001B[39m\u001B[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     62\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs == \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     72\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     76\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs == -\u001B[32m1\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001B[39m, in \u001B[36m_optimize_sequential\u001B[39m\u001B[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[39m\n\u001B[32m    157\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m160\u001B[39m     frozen_trial = \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[32m    163\u001B[39m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[32m    164\u001B[39m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[32m    165\u001B[39m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    241\u001B[39m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mShould not reach.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    243\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    244\u001B[39m     frozen_trial.state == TrialState.FAIL\n\u001B[32m    245\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    246\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[32m    247\u001B[39m ):\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    195\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001B[32m    196\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m197\u001B[39m         value_or_values = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    198\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions.TrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    199\u001B[39m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[32m    200\u001B[39m         state = TrialState.PRUNED\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 428\u001B[39m, in \u001B[36mobjective\u001B[39m\u001B[34m(trial)\u001B[39m\n\u001B[32m    425\u001B[39m ensemble = VotingClassifier(estimators=[(\u001B[33m'\u001B[39m\u001B[33mrf\u001B[39m\u001B[33m'\u001B[39m, rf), (\u001B[33m'\u001B[39m\u001B[33mxgb\u001B[39m\u001B[33m'\u001B[39m, xgb_m), (\u001B[33m'\u001B[39m\u001B[33mcat\u001B[39m\u001B[33m'\u001B[39m, cat_m)], voting=\u001B[33m'\u001B[39m\u001B[33msoft\u001B[39m\u001B[33m'\u001B[39m, weights=ensemble_weights)\n\u001B[32m    427\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m428\u001B[39m     \u001B[43mensemble\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_fold_aug\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_fold_aug\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    429\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(ensemble, \u001B[33m\"\u001B[39m\u001B[33mpredict_proba\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    430\u001B[39m         y_val_proba = ensemble.predict_proba(X_val_fold)[:, \u001B[32m1\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:63\u001B[39m, in \u001B[36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     61\u001B[39m extra_args = \u001B[38;5;28mlen\u001B[39m(args) - \u001B[38;5;28mlen\u001B[39m(all_args)\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m extra_args <= \u001B[32m0\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[38;5;66;03m# extra_args > 0\u001B[39;00m\n\u001B[32m     66\u001B[39m args_msg = [\n\u001B[32m     67\u001B[39m     \u001B[33m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(name, arg)\n\u001B[32m     68\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m name, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(kwonly_args[:extra_args], args[-extra_args:])\n\u001B[32m     69\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py:419\u001B[39m, in \u001B[36mVotingClassifier.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, **fit_params)\u001B[39m\n\u001B[32m    416\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    417\u001B[39m     fit_params[\u001B[33m\"\u001B[39m\u001B[33msample_weight\u001B[39m\u001B[33m\"\u001B[39m] = sample_weight\n\u001B[32m--> \u001B[39m\u001B[32m419\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransformed_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py:100\u001B[39m, in \u001B[36m_BaseVoting.fit\u001B[39m\u001B[34m(self, X, y, **fit_params)\u001B[39m\n\u001B[32m     95\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33msample_weight\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m fit_params:\n\u001B[32m     96\u001B[39m             routed_params[name].fit[\u001B[33m\"\u001B[39m\u001B[33msample_weight\u001B[39m\u001B[33m\"\u001B[39m] = fit_params[\n\u001B[32m     97\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33msample_weight\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     98\u001B[39m             ]\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m \u001B[38;5;28mself\u001B[39m.estimators_ = \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    101\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_single_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    102\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    103\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    104\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    105\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfit\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    106\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessage_clsname\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mVoting\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_log_message\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mclfs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    108\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    109\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnames\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclfs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    110\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclf\u001B[49m\u001B[43m \u001B[49m\u001B[43m!=\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdrop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m    111\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    113\u001B[39m \u001B[38;5;28mself\u001B[39m.named_estimators_ = Bunch()\n\u001B[32m    115\u001B[39m \u001B[38;5;66;03m# Uses 'drop' as placeholder for dropped estimators\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:77\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     72\u001B[39m config = get_config()\n\u001B[32m     73\u001B[39m iterable_with_config = (\n\u001B[32m     74\u001B[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     76\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/joblib/parallel.py:1985\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1983\u001B[39m     output = \u001B[38;5;28mself\u001B[39m._get_sequential_output(iterable)\n\u001B[32m   1984\u001B[39m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m1985\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[32m   1987\u001B[39m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[32m   1988\u001B[39m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[32m   1989\u001B[39m \u001B[38;5;66;03m# reused, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[32m   1990\u001B[39m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[32m   1991\u001B[39m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[32m   1992\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._lock:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/joblib/parallel.py:1913\u001B[39m, in \u001B[36mParallel._get_sequential_output\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1911\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_batches += \u001B[32m1\u001B[39m\n\u001B[32m   1912\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_tasks += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1913\u001B[39m res = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1914\u001B[39m \u001B[38;5;28mself\u001B[39m.n_completed_tasks += \u001B[32m1\u001B[39m\n\u001B[32m   1915\u001B[39m \u001B[38;5;28mself\u001B[39m.print_progress()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/utils/parallel.py:139\u001B[39m, in \u001B[36m_FuncWrapper.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    137\u001B[39m     config = {}\n\u001B[32m    138\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(**config):\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/sklearn/ensemble/_base.py:39\u001B[39m, in \u001B[36m_fit_single_estimator\u001B[39m\u001B[34m(estimator, X, y, fit_params, message_clsname, message)\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     38\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m         \u001B[43mestimator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m estimator\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py:729\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    727\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig.parameters, args):\n\u001B[32m    728\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/sklearn.py:1682\u001B[39m, in \u001B[36mXGBClassifier.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001B[39m\n\u001B[32m   1660\u001B[39m model, metric, params, feature_weights = \u001B[38;5;28mself\u001B[39m._configure_fit(\n\u001B[32m   1661\u001B[39m     xgb_model, params, feature_weights\n\u001B[32m   1662\u001B[39m )\n\u001B[32m   1663\u001B[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001B[32m   1664\u001B[39m     missing=\u001B[38;5;28mself\u001B[39m.missing,\n\u001B[32m   1665\u001B[39m     X=X,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1679\u001B[39m     feature_types=\u001B[38;5;28mself\u001B[39m.feature_types,\n\u001B[32m   1680\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1682\u001B[39m \u001B[38;5;28mself\u001B[39m._Booster = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1683\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1684\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1685\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_num_boosting_rounds\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1686\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevals\u001B[49m\u001B[43m=\u001B[49m\u001B[43mevals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1687\u001B[39m \u001B[43m    \u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mearly_stopping_rounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1688\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevals_result\u001B[49m\u001B[43m=\u001B[49m\u001B[43mevals_result\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1689\u001B[39m \u001B[43m    \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1690\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcustom_metric\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1691\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1692\u001B[39m \u001B[43m    \u001B[49m\u001B[43mxgb_model\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1693\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1694\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1696\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m.objective):\n\u001B[32m   1697\u001B[39m     \u001B[38;5;28mself\u001B[39m.objective = params[\u001B[33m\"\u001B[39m\u001B[33mobjective\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py:729\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    727\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig.parameters, args):\n\u001B[32m    728\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/training.py:183\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001B[39m\n\u001B[32m    181\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001B[32m    182\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m183\u001B[39m \u001B[43mbst\u001B[49m\u001B[43m.\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miteration\u001B[49m\u001B[43m=\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m=\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    184\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001B[32m    185\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/xgboost/core.py:2247\u001B[39m, in \u001B[36mBooster.update\u001B[39m\u001B[34m(self, dtrain, iteration, fobj)\u001B[39m\n\u001B[32m   2243\u001B[39m \u001B[38;5;28mself\u001B[39m._assign_dmatrix_features(dtrain)\n\u001B[32m   2245\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m fobj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2246\u001B[39m     _check_call(\n\u001B[32m-> \u001B[39m\u001B[32m2247\u001B[39m         \u001B[43m_LIB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mXGBoosterUpdateOneIter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2248\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctypes\u001B[49m\u001B[43m.\u001B[49m\u001B[43mc_int\u001B[49m\u001B[43m(\u001B[49m\u001B[43miteration\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtrain\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle\u001B[49m\n\u001B[32m   2249\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2250\u001B[39m     )\n\u001B[32m   2251\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2252\u001B[39m     pred = \u001B[38;5;28mself\u001B[39m.predict(dtrain, output_margin=\u001B[38;5;28;01mTrue\u001B[39;00m, training=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
