{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T09:24:42.694667Z",
     "start_time": "2025-05-07T09:24:40.138033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define a function for preprocessing and feature engineering\n",
    "def preprocess_data(df_train_raw, df_test_raw):\n",
    "    \"\"\"\n",
    "    Preprocesses training and testing data, performs feature engineering.\n",
    "    \"\"\"\n",
    "    # Store test row_ids for submission\n",
    "    test_row_ids_func = df_test_raw['row_id'].copy()\n",
    "\n",
    "    # Add a temporary target column to test_df to match columns for concatenation\n",
    "    # and to identify test rows later.\n",
    "    df_test_with_target_placeholder = df_test_raw.copy()\n",
    "    df_test_with_target_placeholder['new_policy_count'] = -1 # Placeholder value\n",
    "\n",
    "    # Combine train and test data for consistent transformations\n",
    "    combined_df = pd.concat([df_train_raw, df_test_with_target_placeholder], ignore_index=True)\n",
    "\n",
    "    # --- Feature Engineering ---\n",
    "\n",
    "    # 1. agent_code: Ensure it's treated as a categorical string\n",
    "    combined_df['agent_code'] = combined_df['agent_code'].astype(str)\n",
    "\n",
    "    # 2. Date columns processing\n",
    "    date_cols_to_convert = ['agent_join_month', 'first_policy_sold_month', 'year_month']\n",
    "    date_cols_dt = [col + '_dt' for col in date_cols_to_convert] # e.g., 'agent_join_month_dt'\n",
    "\n",
    "    for col_orig, col_dt in zip(date_cols_to_convert, date_cols_dt):\n",
    "        combined_df[col_dt] = pd.to_datetime(combined_df[col_orig], format='%m/%d/%Y')\n",
    "\n",
    "    # Create new features from date differences\n",
    "    combined_df['agent_tenure_days'] = (combined_df['year_month_dt'] - combined_df['agent_join_month_dt']).dt.days\n",
    "    combined_df['days_join_to_first_sale'] = (combined_df['first_policy_sold_month_dt'] - combined_df['agent_join_month_dt']).dt.days\n",
    "    combined_df['days_report_to_first_sale'] = (combined_df['first_policy_sold_month_dt'] - combined_df['year_month_dt']).dt.days\n",
    "\n",
    "    # Extract components from 'year_month_dt' (the reporting period)\n",
    "    combined_df['report_year'] = combined_df['year_month_dt'].dt.year\n",
    "    combined_df['report_month'] = combined_df['year_month_dt'].dt.month\n",
    "    combined_df['report_dayofyear'] = combined_df['year_month_dt'].dt.dayofyear\n",
    "    combined_df['report_dayofweek'] = combined_df['year_month_dt'].dt.dayofweek\n",
    "\n",
    "    # Drop original string date columns and intermediate datetime columns\n",
    "    cols_to_drop_after_feature_eng = date_cols_to_convert + date_cols_dt\n",
    "    combined_df = combined_df.drop(columns=cols_to_drop_after_feature_eng)\n",
    "\n",
    "    # 3. Label encode all object type columns (this will include 'agent_code')\n",
    "    categorical_cols = combined_df.select_dtypes(include='object').columns.tolist()\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        combined_df[col] = le.fit_transform(combined_df[col])\n",
    "\n",
    "    # Separate train and test data based on the placeholder\n",
    "    train_processed = combined_df[combined_df['new_policy_count'] != -1].copy()\n",
    "    test_processed = combined_df[combined_df['new_policy_count'] == -1].copy()\n",
    "\n",
    "    # Prepare final datasets for modeling\n",
    "    y_train_processed = train_processed['new_policy_count'].astype(float) # Target variable\n",
    "    train_processed = train_processed.drop(columns=['new_policy_count'])\n",
    "    test_processed = test_processed.drop(columns=['new_policy_count'])\n",
    "\n",
    "    return train_processed, y_train_processed, test_processed, test_row_ids_func\n",
    "\n",
    "\n",
    "# --- Main script execution ---\n",
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    try:\n",
    "        train_df_raw = pd.read_csv(\"train_storming_round.csv\")\n",
    "        test_df_raw = pd.read_csv(\"test_storming_round.csv\")\n",
    "        # sample_submission_df = pd.read_csv(\"sample_submission_storming_round.csv\") # Not strictly needed for script logic\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}. Make sure the CSV files are in the same directory as the script.\")\n",
    "        exit()\n",
    "\n",
    "    # Preprocess data and engineer features\n",
    "    print(\"Preprocessing data...\")\n",
    "    X_train, y_train, X_test, test_row_ids = preprocess_data(train_df_raw, test_df_raw)\n",
    "    print(\"Preprocessing complete.\")\n",
    "\n",
    "    # 'row_id' is an identifier, not a feature for training.\n",
    "    # It was already handled for test_row_ids, but ensure it's not in X_train/X_test.\n",
    "    if 'row_id' in X_train.columns:\n",
    "        X_train = X_train.drop(columns=['row_id'])\n",
    "    if 'row_id' in X_test.columns:\n",
    "        X_test = X_test.drop(columns=['row_id'])\n",
    "\n",
    "    features = X_train.columns.tolist()\n",
    "\n",
    "    # --- Model Training (LightGBM) ---\n",
    "    # LightGBM parameters (can be tuned for better performance)\n",
    "    lgb_params = {\n",
    "        'objective': 'regression_l1',  # Mean Absolute Error (robust to outliers)\n",
    "        'metric': 'mae',               # Metric to monitor\n",
    "        'n_estimators': 2000,          # Number of boosting rounds\n",
    "        'learning_rate': 0.01,         # Step size shrinkage\n",
    "        'feature_fraction': 0.8,       # Fraction of features to use for each tree\n",
    "        'bagging_fraction': 0.8,       # Fraction of data to use for each tree (requires bagging_freq > 0)\n",
    "        'bagging_freq': 1,             # Frequency for bagging\n",
    "        'lambda_l1': 0.1,              # L1 regularization\n",
    "        'lambda_l2': 0.1,              # L2 regularization\n",
    "        'num_leaves': 31,              # Max number of leaves in one tree (default: 31)\n",
    "        'verbose': -1,                 # Suppress LightGBM's own verbosity\n",
    "        'n_jobs': -1,                  # Use all available cores\n",
    "        'seed': 42,                    # Random seed for reproducibility\n",
    "        'boosting_type': 'gbdt',       # Gradient Boosting Decision Tree\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**lgb_params)\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    # Using early stopping. eval_set ideally would be a separate validation set.\n",
    "    # Here, for simplicity, it monitors performance on the training data itself.\n",
    "    # This helps to stop if the model stops improving on train data, given large n_estimators.\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_train, y_train)],\n",
    "              eval_metric='mae',\n",
    "              callbacks=[lgb.early_stopping(100, verbose=100)]) # Stop if MAE doesn't improve for 100 rounds, print every 100 rounds.\n",
    "    print(\"Model training finished.\")\n",
    "\n",
    "    # --- Prediction ---\n",
    "    print(\"Making predictions on the test set...\")\n",
    "    predictions_test = model.predict(X_test)\n",
    "\n",
    "    # Post-processing predictions:\n",
    "    # 1. Ensure non-negativity (policy count cannot be negative)\n",
    "    predictions_test = np.maximum(0, predictions_test)\n",
    "    # 2. Round to nearest integer and convert to int type (policy count is a whole number)\n",
    "    predictions_test = np.round(predictions_test).astype(int)\n",
    "    print(\"Predictions made and post-processed.\")\n",
    "\n",
    "    # --- Create Submission File ---\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': test_row_ids,\n",
    "        'new_policy_count': predictions_test\n",
    "    })\n",
    "\n",
    "    submission_filename = \"submission.csv\"\n",
    "    submission_df.to_csv(submission_filename, index=False)\n",
    "    print(f\"Submission file created successfully: {submission_filename}\")\n",
    "\n",
    "    # --- Optional: Display Feature Importances ---\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "        print(\"\\nTop 20 Feature Importances:\")\n",
    "        print(feature_importance_df.head(20))"
   ],
   "id": "7f61990686133a32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Preprocessing complete.\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/randitha/Desktop/IT/Personal/DataStormV6/data-storm/venv/lib/python3.11/site-packages/lightgbm/callback.py:347: UserWarning: Only training set found, disabling early stopping.\n",
      "  _log_warning(\"Only training set found, disabling early stopping.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training finished.\n",
      "Making predictions on the test set...\n",
      "Predictions made and post-processed.\n",
      "Submission file created successfully: submission.csv\n",
      "\n",
      "Top 20 Feature Importances:\n",
      "                            feature  importance\n",
      "17  number_of_cash_payment_policies       10765\n",
      "5                   unique_proposal        6381\n",
      "14                       ANBP_value        5100\n",
      "15                       net_income        4134\n",
      "0                        agent_code        3266\n",
      "18                agent_tenure_days        3101\n",
      "20        days_report_to_first_sale        2780\n",
      "4     unique_proposals_last_21_days        2731\n",
      "19          days_join_to_first_sale        2419\n",
      "1                         agent_age        2338\n",
      "16         number_of_policy_holders        2171\n",
      "9                 unique_quotations        1849\n",
      "3     unique_proposals_last_15_days        1807\n",
      "13                 unique_customers        1511\n",
      "12    unique_customers_last_21_days        1253\n",
      "11    unique_customers_last_15_days        1113\n",
      "8    unique_quotations_last_21_days        1093\n",
      "22                     report_month        1021\n",
      "24                 report_dayofweek         959\n",
      "2      unique_proposals_last_7_days         958\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
