{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-07T16:34:31.914111Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "CHAMPIONSHIP MODEL - Insurance Agent NILL Prediction\n",
    "Data Storm v6.0 - First Place Solution (with Optuna HPO & SMOTE Augmentation)\n",
    "\n",
    "Key enhancements:\n",
    "1. Stratified time-series cross-validation with gap\n",
    "2. Feature importance-based selection with stability analysis (Now with RFECV)\n",
    "3. CatBoost integration with custom loss function\n",
    "4. Agent-specific dynamic thresholding (Conceptually improved, OOF-optimized fixed threshold)\n",
    "5. Recursive feature elimination with stability scores (RFECV implemented)\n",
    "6. Optuna for Hyperparameter Optimization (with Pruning)\n",
    "7. SMOTE Data Augmentation for minority class\n",
    "8. Enhanced Feature Engineering (Lags, Rolling Windows, Interactions, Time Since Event)\n",
    "9. LightGBM added to ensemble\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, f1_score as f1_score_metric\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# --- DIRECTORY SETUP ---\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    script_dir = os.getcwd() # Fallback for interactive environments\n",
    "\n",
    "data_dir = os.path.join(script_dir, 'dataset')\n",
    "output_dir = os.path.join(script_dir, 'outputs')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"CHAMPIONSHIP KAGGLE SOLUTION - ADVANCED ENSEMBLE WITH OPTUNA HPO & SMOTE - ENHANCED\")\n",
    "print(\"=\" * 100)\n",
    "start_time_script = time.time()\n",
    "print(f\"Starting at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# --- STEP 1: LOAD DATA ---\n",
    "print(\"\\nStep 1: Loading data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(os.path.join(data_dir, 'train_storming_round.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(data_dir, 'test_storming_round.csv'))\n",
    "    submission_template = pd.read_csv(os.path.join(data_dir, 'sample_submission_storming_round.csv'))\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Dataset file not found. Please ensure 'train_storming_round.csv', 'test_storming_round.csv', and 'sample_submission_storming_round.csv' are in the '{data_dir}' directory.\")\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "print(f\"Train data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Submission template shape: {submission_template.shape}\")\n",
    "\n",
    "print(\"Performing data integrity checks...\")\n",
    "assert len(test_df) == len(submission_template), \"Test and submission template row counts don't match!\"\n",
    "dupes_train = train_df.duplicated().sum()\n",
    "if dupes_train > 0:\n",
    "    print(f\"WARNING: Found {dupes_train} duplicate rows in training data. Removing...\")\n",
    "    train_df = train_df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "dupes_test = test_df.duplicated().sum()\n",
    "if dupes_test > 0:\n",
    "    print(f\"WARNING: Found {dupes_test} duplicate rows in test data. Removing...\")\n",
    "    test_df = test_df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "    if len(test_df) != len(submission_template):\n",
    "        print(\"WARNING: Test data size changed after deduplication. Submission template might not align.\")\n",
    "\n",
    "# --- STEP 2: ENHANCED PREPROCESSING ---\n",
    "print(\"\\nStep 2: Enhanced preprocessing with domain expertise...\")\n",
    "date_columns = ['agent_join_month', 'first_policy_sold_month', 'year_month']\n",
    "for df_loop in [train_df, test_df]:\n",
    "    for col in date_columns:\n",
    "        if col in df_loop.columns:\n",
    "            df_loop[col] = pd.to_datetime(df_loop[col], errors='coerce')\n",
    "\n",
    "train_df = train_df.sort_values(['agent_code', 'year_month'])\n",
    "train_df['target_column'] = 0\n",
    "unique_agents = train_df['agent_code'].unique()\n",
    "print(f\"Processing target for {len(unique_agents)} unique agents...\")\n",
    "for agent_idx, agent in enumerate(unique_agents):\n",
    "    agent_data = train_df[train_df['agent_code'] == agent].copy().sort_values('year_month')\n",
    "    for i in range(len(agent_data) - 1):\n",
    "        current_row_id = agent_data.iloc[i]['row_id']\n",
    "        next_month_sales = agent_data.iloc[i+1]['new_policy_count']\n",
    "        if next_month_sales > 0:\n",
    "            train_df.loc[train_df['row_id'] == current_row_id, 'target_column'] = 1\n",
    "    if (agent_idx + 1) % 1000 == 0: print(f\"  Processed target for {agent_idx+1}/{len(unique_agents)} agents...\")\n",
    "last_month_indices = train_df.groupby('agent_code')['year_month'].idxmax()\n",
    "train_df = train_df.drop(last_month_indices)\n",
    "print(f\"Processed training data shape after target creation: {train_df.shape}\")\n",
    "print(f\"Target distribution:\\n{train_df['target_column'].value_counts(normalize=True)}\")\n",
    "\n",
    "# --- STEP 3: ADVANCED FEATURE ENGINEERING ---\n",
    "print(\"\\nStep 3: Advanced feature engineering with agent profiling...\")\n",
    "\n",
    "def comprehensive_feature_engineering(df_input, is_train=True, train_reference_df=None):\n",
    "    df = df_input.copy()\n",
    "    df = df.sort_values(['agent_code', 'year_month']).reset_index(drop=True) # CRITICAL for shift/rolling\n",
    "\n",
    "    # Extract time-based features\n",
    "    for col in date_columns:\n",
    "        if col in df.columns and pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[f'{col}_month'] = df[col].dt.month\n",
    "            df[f'{col}_year'] = df[col].dt.year\n",
    "            df[f'{col}_quarter'] = df[col].dt.quarter\n",
    "            df[f'{col}_dayofweek'] = df[col].dt.dayofweek\n",
    "            df[f'{col}_dayofyear'] = df[col].dt.dayofyear\n",
    "            if hasattr(df[col].dt, 'isocalendar'):\n",
    "                 df[f'{col}_weekofyear'] = df[col].dt.isocalendar().week.astype(int)\n",
    "            else:\n",
    "                 df[f'{col}_weekofyear'] = df[col].dt.weekofyear.astype(int)\n",
    "            df[f'{col}_month_sin'] = np.sin(2 * np.pi * df[f'{col}_month']/12)\n",
    "            df[f'{col}_month_cos'] = np.cos(2 * np.pi * df[f'{col}_month']/12)\n",
    "\n",
    "    if all(c in df.columns for c in ['year_month', 'agent_join_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['year_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['agent_join_month']):\n",
    "        df['months_with_company'] = ((df['year_month'].dt.year - df['agent_join_month'].dt.year) * 12 + \\\n",
    "                                    (df['year_month'].dt.month - df['agent_join_month'].dt.month)).fillna(0)\n",
    "    if all(c in df.columns for c in ['first_policy_sold_month', 'agent_join_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['first_policy_sold_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['agent_join_month']):\n",
    "        df['months_to_first_sale'] = ((df['first_policy_sold_month'].dt.year - df['agent_join_month'].dt.year) * 12 + \\\n",
    "                                     (df['first_policy_sold_month'].dt.month - df['agent_join_month'].dt.month)).fillna(-1)\n",
    "    if all(c in df.columns for c in ['year_month', 'first_policy_sold_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['year_month']) and \\\n",
    "       pd.api.types.is_datetime64_any_dtype(df['first_policy_sold_month']):\n",
    "        df['months_since_first_sale'] = ((df['year_month'].dt.year - df['first_policy_sold_month'].dt.year) * 12 + \\\n",
    "                                        (df['year_month'].dt.month - df['first_policy_sold_month'].dt.month)).fillna(-1)\n",
    "\n",
    "    # --- Enhanced FE: Lagged and Rolling Features ---\n",
    "    # These features are calculated directly on 'df' (which is train_df or test_df)\n",
    "    # Ensure 'df' is sorted by 'agent_code', 'year_month' before these calculations.\n",
    "    # This is done at the start of the function.\n",
    "\n",
    "    # Define features for lags/rolling (excluding new_policy_count initially, handle separately if needed for test)\n",
    "    # Using features that are typically available for both train and test instances.\n",
    "    # Note: If 'new_policy_count' from previous months is needed as a feature for test, it must be part of test_df's historical data or merged.\n",
    "    # For this problem, 'new_policy_count' is the actuals of the current month in train_df.\n",
    "    # So, new_policy_count and its lags/rolling stats (shifted) are valid features.\n",
    "    key_numeric_features = ['new_policy_count', # Available in train_df, represents current month sales\n",
    "                            'unique_proposals', 'unique_quotations', 'unique_customers',\n",
    "                            'ANBP_value', 'net_income']\n",
    "    lag_periods = [1, 2, 3, 6]\n",
    "    rolling_windows = [3, 6, 12]\n",
    "\n",
    "    for col in key_numeric_features:\n",
    "        if col in df.columns:\n",
    "            # Lagged Features\n",
    "            for k in lag_periods:\n",
    "                df[f'{col}_lag_{k}'] = df.groupby('agent_code')[col].shift(k).fillna(0)\n",
    "\n",
    "            # Rolling Window Statistics (shifted to avoid leakage)\n",
    "            for w in rolling_windows:\n",
    "                grouped_col = df.groupby('agent_code')[col]\n",
    "                # .reset_index after rolling().operation() is important to align for .shift()\n",
    "                df[f'{col}_roll_mean_{w}m'] = grouped_col.rolling(window=w, min_periods=1).mean().reset_index(level=0, drop=True).shift(1).fillna(0)\n",
    "                df[f'{col}_roll_median_{w}m'] = grouped_col.rolling(window=w, min_periods=1).median().reset_index(level=0, drop=True).shift(1).fillna(0)\n",
    "                df[f'{col}_roll_std_{w}m'] = grouped_col.rolling(window=w, min_periods=1).std().reset_index(level=0, drop=True).shift(1).fillna(0)\n",
    "                df[f'{col}_roll_sum_{w}m'] = grouped_col.rolling(window=w, min_periods=1).sum().reset_index(level=0, drop=True).shift(1).fillna(0)\n",
    "\n",
    "    # Difference Features\n",
    "    for col in key_numeric_features:\n",
    "        if col in df.columns:\n",
    "            if f'{col}_lag_1' in df.columns:\n",
    "                df[f'{col}_diff_lag1'] = df[col].fillna(0) - df[f'{col}_lag_1'].fillna(0)\n",
    "            if f'{col}_roll_mean_3m' in df.columns: # Uses the 3m rolling mean (already shifted)\n",
    "                df[f'{col}_diff_roll_mean_3m'] = df[col].fillna(0) - df[f'{col}_roll_mean_3m'].fillna(0)\n",
    "\n",
    "\n",
    "    activity_cols_periods = {\n",
    "        'unique_proposals': ['7_days', '15_days', '21_days'],\n",
    "        'unique_quotations': ['7_days', '15_days', '21_days'],\n",
    "        'unique_customers': ['7_days', '15_days', '21_days']\n",
    "    } # Original trend features\n",
    "    for base_col, periods in activity_cols_periods.items():\n",
    "        for i in range(len(periods) - 1):\n",
    "            col1_name_suffix = f'_last_{periods[i]}'\n",
    "            col2_name_suffix = f'_last_{periods[i+1]}'\n",
    "            col1 = base_col + col1_name_suffix if not base_col.endswith(col1_name_suffix) else base_col\n",
    "            col2 = base_col + col2_name_suffix if not base_col.endswith(col2_name_suffix) else base_col\n",
    "            if col1 in df.columns and col2 in df.columns:\n",
    "                df[f'{base_col}_trend_{periods[i]}_{periods[i+1]}'] = df[col1].fillna(0) / np.maximum(df[col2].fillna(0), 1e-6)\n",
    "\n",
    "    for col_to_transform in ['unique_proposals', 'unique_quotations', 'unique_customers', 'ANBP_value', 'net_income', 'agent_age']: # unique_proposal -> unique_proposals\n",
    "        if col_to_transform in df.columns:\n",
    "            df[f'log_{col_to_transform}'] = np.log1p(df[col_to_transform].fillna(0))\n",
    "            df[f'sqrt_{col_to_transform}'] = np.sqrt(np.maximum(0, df[col_to_transform].fillna(0)))\n",
    "\n",
    "    # --- Historical Features (NILL rate, Avg Policies) ---\n",
    "    # These depend on 'new_policy_count' and need careful handling for train/test split\n",
    "    if is_train:\n",
    "        if 'new_policy_count' in df.columns:\n",
    "             df['hist_nill_rate_calc'] = df.groupby('agent_code')['new_policy_count'].transform(lambda x: x.expanding().apply(lambda y: (y==0).mean() if len(y)>1 else 0.5).shift(1)).fillna(0.5)\n",
    "             df['hist_avg_policies_calc'] = df.groupby('agent_code')['new_policy_count'].transform(lambda x: x.expanding().mean().shift(1)).fillna(0)\n",
    "        else: # Should not happen if train_df is correctly passed\n",
    "            df['hist_nill_rate_calc'] = 0.5\n",
    "            df['hist_avg_policies_calc'] = 0\n",
    "    else: # For test data\n",
    "        if train_reference_df is not None and 'new_policy_count' in train_reference_df.columns:\n",
    "            agent_hist_stats = train_reference_df.groupby('agent_code').agg(\n",
    "                hist_nill_rate_ref=('new_policy_count', lambda x: (x==0).mean()),\n",
    "                hist_avg_policies_ref=('new_policy_count', 'mean')\n",
    "            ).reset_index()\n",
    "            df = pd.merge(df, agent_hist_stats, on='agent_code', how='left')\n",
    "\n",
    "            overall_train_nill_rate = train_reference_df['new_policy_count'].eq(0).mean()\n",
    "            overall_train_avg_policies = train_reference_df['new_policy_count'].mean()\n",
    "\n",
    "            df['hist_nill_rate_calc'] = df['hist_nill_rate_ref'].fillna(overall_train_nill_rate)\n",
    "            df['hist_avg_policies_calc'] = df['hist_avg_policies_ref'].fillna(overall_train_avg_policies)\n",
    "            df.drop(columns=['hist_nill_rate_ref', 'hist_avg_policies_ref'], inplace=True, errors='ignore')\n",
    "        else: # Fallback if train_reference_df is not available or lacks new_policy_count\n",
    "            df['hist_nill_rate_calc'] = 0.5 # Global default\n",
    "            df['hist_avg_policies_calc'] = 0 # Global default\n",
    "\n",
    "\n",
    "    # --- Enhanced FE: Time Since Last Sale ---\n",
    "    # This feature also depends on 'new_policy_count'\n",
    "    temp_calc_df_mssf = None\n",
    "    if is_train and 'new_policy_count' in df.columns:\n",
    "        temp_calc_df_mssf = df.copy() # df is train_df, has new_policy_count and is sorted\n",
    "    elif not is_train and train_reference_df is not None and 'new_policy_count' in train_reference_df.columns:\n",
    "        # For test data, compute on the full history from train_reference_df\n",
    "        temp_calc_df_mssf = train_reference_df.copy().sort_values(['agent_code', 'year_month']).reset_index(drop=True)\n",
    "\n",
    "    if temp_calc_df_mssf is not None:\n",
    "        temp_calc_df_mssf['had_sale_mssf'] = (temp_calc_df_mssf['new_policy_count'] > 0).astype(int)\n",
    "        temp_calc_df_mssf['month_idx_asc_mssf'] = temp_calc_df_mssf.groupby('agent_code').cumcount()\n",
    "        temp_calc_df_mssf['sale_month_idx_mssf'] = temp_calc_df_mssf['month_idx_asc_mssf'].where(temp_calc_df_mssf['had_sale_mssf'] == 1)\n",
    "        temp_calc_df_mssf['last_sale_month_idx_ffill_mssf'] = temp_calc_df_mssf.groupby('agent_code')['sale_month_idx_mssf'].ffill()\n",
    "        temp_calc_df_mssf['current_months_since_last_sale_mssf'] = temp_calc_df_mssf['month_idx_asc_mssf'] - temp_calc_df_mssf['last_sale_month_idx_ffill_mssf']\n",
    "        # Shift by 1 to represent \"months since last sale as of START of current month\"\n",
    "        temp_calc_df_mssf['months_since_last_sale_feat'] = temp_calc_df_mssf.groupby('agent_code')['current_months_since_last_sale_mssf'].shift(1).fillna(999) # 999 for never sold / very long time / first month\n",
    "\n",
    "        if is_train:\n",
    "            df['months_since_last_sale_feat'] = temp_calc_df_mssf['months_since_last_sale_feat']\n",
    "        else: # is_test, merge from the computed values on train_reference_df\n",
    "            df = pd.merge(df, temp_calc_df_mssf[['agent_code', 'year_month', 'months_since_last_sale_feat']],\n",
    "                          on=['agent_code', 'year_month'], how='left')\n",
    "            df['months_since_last_sale_feat'] = df['months_since_last_sale_feat'].fillna(999) # Fallback for any test rows not covered\n",
    "    else:\n",
    "        df['months_since_last_sale_feat'] = 999\n",
    "\n",
    "\n",
    "    # --- Agent Profile Features ---\n",
    "    profile_ref_df = train_reference_df if train_reference_df is not None else (df if is_train else None)\n",
    "    if profile_ref_df is not None:\n",
    "        agent_profiles_agg = {}\n",
    "        # Use unique_proposals instead of unique_proposal\n",
    "        if 'unique_proposals' in profile_ref_df.columns: agent_profiles_agg['unique_proposals_mean_profile'] = ('unique_proposals', 'mean')\n",
    "        if 'unique_quotations' in profile_ref_df.columns: agent_profiles_agg['unique_quotations_mean_profile'] = ('unique_quotations', 'mean')\n",
    "        if 'agent_age' in profile_ref_df.columns: agent_profiles_agg['agent_age_mean_profile'] = ('agent_age', 'mean')\n",
    "        # Add more profile features if sensible (e.g. std dev, sum of ANBP, etc.)\n",
    "\n",
    "        if agent_profiles_agg:\n",
    "            agent_profiles = profile_ref_df.groupby('agent_code', as_index=False).agg(**agent_profiles_agg)\n",
    "            df = pd.merge(df, agent_profiles, on='agent_code', how='left')\n",
    "            for col_name in agent_profiles.columns:\n",
    "                if col_name != 'agent_code' and col_name in df.columns:\n",
    "                    original_feature_name = col_name.replace('_mean_profile', '') # Adjusted based on new naming\n",
    "                    fill_val = profile_ref_df[original_feature_name].mean() if original_feature_name in profile_ref_df else 0\n",
    "                    df[col_name] = df[col_name].fillna(fill_val)\n",
    "    # Fallback if profiles couldn't be created\n",
    "    for prof_col in ['unique_proposals_mean_profile', 'unique_quotations_mean_profile', 'agent_age_mean_profile']:\n",
    "        if prof_col not in df.columns: df[prof_col] = 0\n",
    "\n",
    "\n",
    "    # --- Interaction Features ---\n",
    "    if 'agent_age' in df.columns and 'hist_avg_policies_calc' in df.columns:\n",
    "        df['inter_age_x_hist_avg_policies'] = df['agent_age'].fillna(0) * df['hist_avg_policies_calc'].fillna(0)\n",
    "    if 'months_with_company' in df.columns and 'unique_proposals_mean_profile' in df.columns:\n",
    "        df['inter_exp_x_prop_profile'] = df['months_with_company'].fillna(0) * df['unique_proposals_mean_profile'].fillna(0)\n",
    "    if 'months_since_last_sale_feat' in df.columns and 'hist_nill_rate_calc' in df.columns:\n",
    "        df['inter_msls_x_nill_rate'] = df['months_since_last_sale_feat'].replace(999, 50).fillna(50) * df['hist_nill_rate_calc'].fillna(0.5)\n",
    "\n",
    "\n",
    "    # Final NaN fill for numeric columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' and col not in ['agent_code', 'row_id']:\n",
    "            try: df[col] = pd.to_numeric(df[col])\n",
    "            except ValueError:\n",
    "                 if df[col].isnull().any():\n",
    "                     df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else \"Unknown\")\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if df[col].isnull().any():\n",
    "                df[col] = df[col].fillna(0) # Consider median/mean for specific features if 0 is not appropriate\n",
    "    return df\n",
    "\n",
    "train_df_reference_for_fe = train_df.copy() # This has new_policy_count\n",
    "train_df_fe = comprehensive_feature_engineering(train_df, is_train=True, train_reference_df=train_df_reference_for_fe)\n",
    "test_df_fe = comprehensive_feature_engineering(test_df, is_train=False, train_reference_df=train_df_reference_for_fe)\n",
    "\n",
    "print(f\"Train data shape after FE: {train_df_fe.shape}\")\n",
    "print(f\"Test data shape after FE: {test_df_fe.shape}\")\n",
    "\n",
    "\n",
    "# --- STEP 4: FEATURE SELECTION (with RFECV) ---\n",
    "print(\"\\nStep 4: Feature selection with RFECV...\")\n",
    "non_feature_cols = ['row_id', 'agent_code', 'year_month', 'target_column', 'new_policy_count',\n",
    "                    'agent_join_month', 'first_policy_sold_month']\n",
    "potential_features = [col for col in train_df_fe.columns if col not in non_feature_cols and col in test_df_fe.columns]\n",
    "numeric_potential_features = []\n",
    "for col in potential_features:\n",
    "    try:\n",
    "        if not pd.api.types.is_numeric_dtype(train_df_fe[col]):\n",
    "            train_df_fe[col] = pd.to_numeric(train_df_fe[col], errors='coerce').fillna(0)\n",
    "            if col in test_df_fe.columns:\n",
    "                 test_df_fe[col] = pd.to_numeric(test_df_fe[col], errors='coerce').fillna(0)\n",
    "        if pd.api.types.is_numeric_dtype(train_df_fe[col]):\n",
    "            numeric_potential_features.append(col)\n",
    "    except Exception as e_conv:\n",
    "        print(f\"Could not process column {col} for numeric check: {e_conv}\")\n",
    "        continue\n",
    "potential_features = numeric_potential_features\n",
    "\n",
    "if not potential_features:\n",
    "    print(\"ERROR: No potential numeric features found. Adding dummy.\")\n",
    "    train_df_fe['dummy_numeric_feat'] = np.random.rand(len(train_df_fe))\n",
    "    test_df_fe['dummy_numeric_feat'] = np.random.rand(len(test_df_fe))\n",
    "    potential_features = ['dummy_numeric_feat']\n",
    "\n",
    "X_temp_fs = train_df_fe[potential_features].copy()\n",
    "y_temp_fs = train_df_fe['target_column'].copy()\n",
    "\n",
    "# Handle potential inf values and NaNs more robustly for RFECV\n",
    "X_temp_fs = X_temp_fs.replace([np.inf, -np.inf], np.nan)\n",
    "for col in X_temp_fs.columns:\n",
    "    if X_temp_fs[col].isnull().any():\n",
    "        X_temp_fs[col] = X_temp_fs[col].fillna(X_temp_fs[col].median()) # Use median for filling\n",
    "X_temp_fs = X_temp_fs.fillna(0) # Final fallback for columns that were all NaN\n",
    "\n",
    "\n",
    "final_features = []\n",
    "global_tscv_for_fs = TimeSeriesSplit(n_splits=3) # Use 3 splits for FS speed\n",
    "\n",
    "if not X_temp_fs.empty and len(X_temp_fs) == len(y_temp_fs) and len(potential_features) > 1:\n",
    "    try:\n",
    "        print(f\"Performing RFECV from {len(potential_features)} potential features...\")\n",
    "        estimator_for_rfecv = lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "        # Scale data for RFECV for estimators that might be sensitive (though LGBM is fairly robust)\n",
    "        scaler_rfecv = StandardScaler()\n",
    "        X_temp_fs_scaled = scaler_rfecv.fit_transform(X_temp_fs)\n",
    "\n",
    "        rfecv_selector = RFECV(estimator=estimator_for_rfecv,\n",
    "                               step=0.1, # Remove 10% of features at each step\n",
    "                               cv=global_tscv_for_fs,\n",
    "                               scoring='roc_auc',\n",
    "                               min_features_to_select=max(1, int(min(len(potential_features),150)*0.2)), # Select at least 20% of up to 150 features\n",
    "                               n_jobs=-1,\n",
    "                               verbose=0) # Set to 1 for more verbosity\n",
    "\n",
    "        rfecv_selector.fit(X_temp_fs_scaled, y_temp_fs)\n",
    "        final_features = X_temp_fs.columns[rfecv_selector.support_].tolist()\n",
    "\n",
    "        if not final_features:\n",
    "            print(\"WARNING: RFECV selected 0 features. Falling back to top N from RandomForest.\")\n",
    "            selector_model_fallback = RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1, max_depth=8, min_samples_leaf=5)\n",
    "            selector_model_fallback.fit(X_temp_fs, y_temp_fs)\n",
    "            importances_fallback = pd.Series(selector_model_fallback.feature_importances_, index=X_temp_fs.columns).sort_values(ascending=False)\n",
    "            final_features = list(importances_fallback.head(min(len(importances_fallback), 75)).index)\n",
    "\n",
    "    except Exception as e_fs:\n",
    "        print(f\"Error during RFECV: {e_fs}. Falling back to simpler feature selection.\")\n",
    "        selector_model_fallback = RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1, max_depth=8, min_samples_leaf=5)\n",
    "        selector_model_fallback.fit(X_temp_fs, y_temp_fs)\n",
    "        importances_fallback = pd.Series(selector_model_fallback.feature_importances_, index=X_temp_fs.columns).sort_values(ascending=False)\n",
    "        final_features = list(importances_fallback.head(min(len(importances_fallback), 75)).index)\n",
    "else:\n",
    "    print(\"Skipping RFECV due to insufficient features/data. Using all potential numeric features or fallback.\")\n",
    "    final_features = potential_features if potential_features else X_temp_fs.columns.tolist()\n",
    "\n",
    "\n",
    "if not final_features:\n",
    "    print(\"CRITICAL WARNING: final_features list is empty. Using fallback.\")\n",
    "    fallback_numeric_cols = [col for col in train_df_fe.columns if pd.api.types.is_numeric_dtype(train_df_fe[col]) and col not in non_feature_cols and col in test_df_fe.columns]\n",
    "    final_features = fallback_numeric_cols[:min(10, len(fallback_numeric_cols))]\n",
    "    if not final_features: print(\"FATAL ERROR: No features available. Exiting.\"); exit()\n",
    "\n",
    "print(f\"Selected {len(final_features)} features. Examples: {final_features[:min(5, len(final_features))]}\")\n",
    "selected_features_path = os.path.join(output_dir, 'selected_features_optuna_smote_enhanced.txt')\n",
    "with open(selected_features_path, 'w') as f:\n",
    "    for feature in final_features: f.write(f\"{feature}\\n\")\n",
    "print(f\"Selected features saved to: {selected_features_path}\")\n",
    "\n",
    "\n",
    "global_final_X_df = train_df_fe[final_features].copy()\n",
    "global_final_y = train_df_fe['target_column'].copy()\n",
    "for col in global_final_X_df.columns: # Impute any remaining NaNs in selected features\n",
    "    if global_final_X_df[col].isnull().any():\n",
    "        global_final_X_df[col] = global_final_X_df[col].fillna(global_final_X_df[col].median())\n",
    "global_final_X_df = global_final_X_df.fillna(0) # Final safety net\n",
    "\n",
    "global_final_scaler = StandardScaler()\n",
    "global_final_X_scaled_np = global_final_scaler.fit_transform(global_final_X_df)\n",
    "global_tscv = TimeSeriesSplit(n_splits=3) # For HPO and OOF. Can increase to 5 if time allows.\n",
    "\n",
    "\n",
    "# --- OPTUNA OBJECTIVE FUNCTION (with SMOTE, LGBM, Pruning) ---\n",
    "def objective(trial):\n",
    "    smote_k_neighbors_max_candidate = 7\n",
    "    if global_final_y.value_counts().min() > 1:\n",
    "         smote_k_neighbors_max_candidate = min(smote_k_neighbors_max_candidate, global_final_y.value_counts().min() -1 )\n",
    "    smote_k_neighbors_max_candidate = max(1, smote_k_neighbors_max_candidate)\n",
    "    smote_k_neighbors = trial.suggest_int('smote_k_neighbors', 1, smote_k_neighbors_max_candidate)\n",
    "\n",
    "    rf_n_estimators = trial.suggest_int('rf_n_estimators', 50, 250, step=25) # Range extended\n",
    "    rf_max_depth = trial.suggest_int('rf_max_depth', 4, 15) # Range extended\n",
    "    rf_min_samples_leaf = trial.suggest_int('rf_min_samples_leaf', 2, 15) # Range extended\n",
    "\n",
    "    xgb_n_estimators = trial.suggest_int('xgb_n_estimators', 50, 250, step=25)\n",
    "    xgb_max_depth = trial.suggest_int('xgb_max_depth', 3, 10)\n",
    "    xgb_learning_rate = trial.suggest_float('xgb_learning_rate', 0.005, 0.15, log=True) # Range extended\n",
    "    xgb_scale_pos_weight = trial.suggest_float('xgb_scale_pos_weight', 0.5, 10.0)\n",
    "\n",
    "\n",
    "    cat_iterations = trial.suggest_int('cat_iterations', 50, 250, step=25)\n",
    "    cat_depth = trial.suggest_int('cat_depth', 4, 10)\n",
    "    cat_learning_rate = trial.suggest_float('cat_learning_rate', 0.005, 0.15, log=True)\n",
    "    cat_l2_leaf_reg = trial.suggest_float('cat_l2_leaf_reg', 0.5, 15.0, log=True) # Range extended\n",
    "    cat_class_weight_0 = trial.suggest_float('cat_class_weight_0_cb', 0.2, 8.0) # Range extended\n",
    "\n",
    "    lgbm_n_estimators = trial.suggest_int('lgbm_n_estimators', 50, 250, step=25)\n",
    "    lgbm_max_depth = trial.suggest_int('lgbm_max_depth', 3, 10)\n",
    "    lgbm_learning_rate = trial.suggest_float('lgbm_learning_rate', 0.005, 0.15, log=True)\n",
    "    lgbm_num_leaves = trial.suggest_int('lgbm_num_leaves', 10, 150) # Wider range\n",
    "    lgbm_reg_alpha = trial.suggest_float('lgbm_reg_alpha', 1e-4, 10.0, log=True) # Wider range\n",
    "    lgbm_reg_lambda = trial.suggest_float('lgbm_reg_lambda', 1e-4, 10.0, log=True) # Wider range\n",
    "    lgbm_colsample_bytree = trial.suggest_float('lgbm_colsample_bytree', 0.5, 1.0) # Wider range\n",
    "    lgbm_scale_pos_weight = trial.suggest_float('lgbm_scale_pos_weight', 0.5, 10.0)\n",
    "\n",
    "\n",
    "    w_rf = trial.suggest_float('w_rf', 0.05, 2.0) # Range adjusted\n",
    "    w_xgb = trial.suggest_float('w_xgb', 0.05, 2.0)\n",
    "    w_cat = trial.suggest_float('w_cat', 0.05, 2.0)\n",
    "    w_lgbm = trial.suggest_float('w_lgbm', 0.05, 2.0)\n",
    "    ensemble_weights = [w_rf, w_xgb, w_cat, w_lgbm]\n",
    "\n",
    "    fold_roc_auc_scores = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(global_tscv.split(global_final_X_scaled_np)):\n",
    "        X_train_fold, X_val_fold = global_final_X_scaled_np[train_idx], global_final_X_scaled_np[val_idx]\n",
    "        y_train_fold, y_val_fold = global_final_y.iloc[train_idx], global_final_y.iloc[val_idx]\n",
    "\n",
    "        minority_class_count_fold = np.sum(y_train_fold == 1)\n",
    "        current_k_for_smote = smote_k_neighbors\n",
    "        if minority_class_count_fold <= current_k_for_smote:\n",
    "            current_k_for_smote = max(1, minority_class_count_fold - 1) if minority_class_count_fold > 1 else 1\n",
    "\n",
    "        X_train_fold_aug, y_train_fold_aug = X_train_fold, y_train_fold\n",
    "        if minority_class_count_fold > 0 and current_k_for_smote > 0 :\n",
    "            smote = SMOTE(random_state=RANDOM_STATE + fold, k_neighbors=current_k_for_smote)\n",
    "            try: X_train_fold_aug, y_train_fold_aug = smote.fit_resample(X_train_fold, y_train_fold)\n",
    "            except ValueError: pass # Fallback to original if SMOTE fails\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators=rf_n_estimators, max_depth=rf_max_depth, min_samples_leaf=rf_min_samples_leaf,\n",
    "                                    random_state=RANDOM_STATE, class_weight='balanced_subsample', n_jobs=-1)\n",
    "        xgb_m = xgb.XGBClassifier(n_estimators=xgb_n_estimators, max_depth=xgb_max_depth, learning_rate=xgb_learning_rate,\n",
    "                                  random_state=RANDOM_STATE, scale_pos_weight=xgb_scale_pos_weight, use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "        cat_m = cb.CatBoostClassifier(iterations=cat_iterations, depth=cat_depth, learning_rate=cat_learning_rate, l2_leaf_reg=cat_l2_leaf_reg,\n",
    "                                      random_seed=RANDOM_STATE, loss_function='Logloss', verbose=0, class_weights={0: cat_class_weight_0, 1: 1.0}, thread_count=-1)\n",
    "        lgbm_m = lgb.LGBMClassifier(n_estimators=lgbm_n_estimators, max_depth=lgbm_max_depth, learning_rate=lgbm_learning_rate,\n",
    "                                    num_leaves=lgbm_num_leaves, reg_alpha=lgbm_reg_alpha, reg_lambda=lgbm_reg_lambda,\n",
    "                                    colsample_bytree=lgbm_colsample_bytree, random_state=RANDOM_STATE,\n",
    "                                    scale_pos_weight=lgbm_scale_pos_weight, n_jobs=-1)\n",
    "\n",
    "        ensemble = VotingClassifier(estimators=[('rf', rf), ('xgb', xgb_m), ('cat', cat_m), ('lgbm', lgbm_m)], voting='soft', weights=ensemble_weights, n_jobs=-1)\n",
    "\n",
    "        try:\n",
    "            ensemble.fit(X_train_fold_aug, y_train_fold_aug)\n",
    "            if hasattr(ensemble, \"predict_proba\"):\n",
    "                y_val_proba = ensemble.predict_proba(X_val_fold)[:, 1]\n",
    "                current_fold_auc = roc_auc_score(y_val_fold, y_val_proba)\n",
    "                fold_roc_auc_scores.append(current_fold_auc)\n",
    "\n",
    "                # Pruning integration\n",
    "                trial.report(current_fold_auc, step=fold)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "            else:\n",
    "                fold_roc_auc_scores.append(0.5) # Penalize\n",
    "        except Exception as e_model_fit:\n",
    "            # print(f\"Trial {trial.number}, Fold {fold} failed: {e_model_fit}\")\n",
    "            return 0.0 # Low score for failed trials\n",
    "\n",
    "    avg_roc_auc = np.mean(fold_roc_auc_scores) if fold_roc_auc_scores else 0.0\n",
    "    return avg_roc_auc\n",
    "\n",
    "# --- STEP 5: HYPERPARAMETER OPTIMIZATION ---\n",
    "print(\"\\nStep 5: Hyperparameter Optimization with Optuna (SMOTE, LGBM, Pruning)...\")\n",
    "# Using MedianPruner: n_startup_trials runs without pruning, n_warmup_steps is number of intermediate steps (folds) before pruning is active.\n",
    "study = optuna.create_study(direction='maximize',\n",
    "                            sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE),\n",
    "                            pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=1, interval_steps=1)) # Prune after 2nd fold (step=1)\n",
    "N_OPTUNA_TRIALS = 250 # ADJUST AS NEEDED (Original was 750 - very long! Using 100 for reasonable demo time)\n",
    "optuna_timeout_hours = 1.0 # ADJUST (Original was 3.5)\n",
    "study.optimize(objective, n_trials=N_OPTUNA_TRIALS, timeout=3600 * optuna_timeout_hours, n_jobs=1) # n_jobs=1 for SMOTE safety\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"\\nBest ROC AUC from Optuna: {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters found by Optuna:\")\n",
    "for key, value in best_params.items(): print(f\"  {key}: {value}\")\n",
    "try:\n",
    "    study_trials_path = os.path.join(output_dir, 'optuna_study_trials_enhanced.csv')\n",
    "    study.trials_dataframe().to_csv(study_trials_path, index=False)\n",
    "    print(f\"Optuna study trials saved to: {study_trials_path}\")\n",
    "except Exception as e_study_save: print(f\"Could not save Optuna study trials: {e_study_save}\")\n",
    "\n",
    "\n",
    "# --- STEP 5.5: OPTIMIZE FIXED THRESHOLD USING OOF FROM BEST PARAMS ---\n",
    "print(\"\\nStep 5.5: Optimizing fixed threshold on OOF predictions...\")\n",
    "oof_true_labels_list = []\n",
    "oof_probas_list = []\n",
    "smote_k_for_oof = best_params.get('smote_k_neighbors', 3)\n",
    "\n",
    "# Initialize models with best_params\n",
    "best_rf_oof = RandomForestClassifier(n_estimators=best_params.get('rf_n_estimators', 100), max_depth=best_params.get('rf_max_depth', 8),\n",
    "    min_samples_leaf=best_params.get('rf_min_samples_leaf', 5), random_state=RANDOM_STATE, class_weight='balanced_subsample', n_jobs=-1)\n",
    "best_xgb_oof = xgb.XGBClassifier(n_estimators=best_params.get('xgb_n_estimators', 100), max_depth=best_params.get('xgb_max_depth', 5),\n",
    "    learning_rate=best_params.get('xgb_learning_rate', 0.05), random_state=RANDOM_STATE, scale_pos_weight=best_params.get('xgb_scale_pos_weight', 1.0),\n",
    "    use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "best_cat_oof = cb.CatBoostClassifier(iterations=best_params.get('cat_iterations', 100), depth=best_params.get('cat_depth', 6),\n",
    "    learning_rate=best_params.get('cat_learning_rate', 0.05), l2_leaf_reg=best_params.get('cat_l2_leaf_reg', 3.0),\n",
    "    random_seed=RANDOM_STATE, loss_function='Logloss', verbose=0, class_weights={0: best_params.get('cat_class_weight_0_cb', 1.0), 1: 1.0}, thread_count=-1)\n",
    "best_lgbm_oof = lgb.LGBMClassifier(n_estimators=best_params.get('lgbm_n_estimators', 100), max_depth=best_params.get('lgbm_max_depth', 7),\n",
    "    learning_rate=best_params.get('lgbm_learning_rate', 0.05), num_leaves=best_params.get('lgbm_num_leaves', 31),\n",
    "    reg_alpha=best_params.get('lgbm_reg_alpha', 0.0), reg_lambda=best_params.get('lgbm_reg_lambda', 0.0),\n",
    "    colsample_bytree=best_params.get('lgbm_colsample_bytree', 0.8), random_state=RANDOM_STATE,\n",
    "    scale_pos_weight=best_params.get('lgbm_scale_pos_weight', 1.0), n_jobs=-1)\n",
    "\n",
    "best_ensemble_oof_weights = [best_params.get('w_rf', 1.0), best_params.get('w_xgb', 1.0), best_params.get('w_cat', 1.0), best_params.get('w_lgbm', 1.0)]\n",
    "best_ensemble_for_oof = VotingClassifier(estimators=[('rf', best_rf_oof), ('xgb', best_xgb_oof), ('cat', best_cat_oof), ('lgbm', best_lgbm_oof)],\n",
    "                                       voting='soft', weights=best_ensemble_oof_weights, n_jobs=-1)\n",
    "\n",
    "all_val_indices = [] # To map OOF preds back to original train_df_fe if needed for error analysis\n",
    "for fold, (train_idx, val_idx) in enumerate(global_tscv.split(global_final_X_scaled_np)):\n",
    "    X_train_fold, X_val_fold = global_final_X_scaled_np[train_idx], global_final_X_scaled_np[val_idx]\n",
    "    y_train_fold, y_val_fold = global_final_y.iloc[train_idx], global_final_y.iloc[val_idx]\n",
    "    all_val_indices.extend(val_idx)\n",
    "\n",
    "\n",
    "    minority_class_count_fold = np.sum(y_train_fold == 1)\n",
    "    current_k_for_smote_oof = smote_k_for_oof\n",
    "    if minority_class_count_fold <= current_k_for_smote_oof:\n",
    "        current_k_for_smote_oof = max(1, minority_class_count_fold - 1) if minority_class_count_fold > 1 else 1\n",
    "\n",
    "    X_train_fold_aug, y_train_fold_aug = X_train_fold, y_train_fold\n",
    "    if minority_class_count_fold > 0 and current_k_for_smote_oof > 0:\n",
    "        smote_oof = SMOTE(random_state=RANDOM_STATE + fold, k_neighbors=current_k_for_smote_oof)\n",
    "        try: X_train_fold_aug, y_train_fold_aug = smote_oof.fit_resample(X_train_fold, y_train_fold)\n",
    "        except ValueError: pass\n",
    "\n",
    "    best_ensemble_for_oof.fit(X_train_fold_aug, y_train_fold_aug)\n",
    "    y_val_proba_oof = best_ensemble_for_oof.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    oof_true_labels_list.extend(y_val_fold.tolist())\n",
    "    oof_probas_list.extend(y_val_proba_oof.tolist())\n",
    "\n",
    "oof_true_labels_np = np.array(oof_true_labels_list)\n",
    "oof_probas_np = np.array(oof_probas_list)\n",
    "optimized_fixed_threshold = 0.5 # Default\n",
    "\n",
    "if len(oof_probas_np) > 0:\n",
    "    precisions_pr, recalls_pr, thresholds_pr = precision_recall_curve(oof_true_labels_np, oof_probas_np)\n",
    "    # Ensure thresholds_pr is not empty and handle edge cases for f1_scores_pr calculation\n",
    "    if len(thresholds_pr) > 0:\n",
    "        f1_scores_pr_calc = np.zeros_like(thresholds_pr)\n",
    "        # Valid indices: precision and recall are for thresholds_pr, but len(precisions/recalls) = len(thresholds) + 1\n",
    "        # We need to compare elements of same length for division.\n",
    "        # (2 * precisions[:-1] * recalls[:-1]) / (precisions[:-1] + recalls[:-1])\n",
    "        numerator = 2 * precisions_pr[:-1] * recalls_pr[:-1]\n",
    "        denominator = precisions_pr[:-1] + recalls_pr[:-1]\n",
    "        # Avoid division by zero\n",
    "        valid_idx_f1 = denominator > 0\n",
    "        f1_scores_pr_calc[valid_idx_f1] = numerator[valid_idx_f1] / denominator[valid_idx_f1]\n",
    "\n",
    "        if len(f1_scores_pr_calc) > 0:\n",
    "            best_f1_idx = np.argmax(f1_scores_pr_calc)\n",
    "            optimized_fixed_threshold = thresholds_pr[best_f1_idx]\n",
    "            print(f\"Optimized fixed threshold (max F1 on OOF): {optimized_fixed_threshold:.4f} (F1: {f1_scores_pr_calc[best_f1_idx]:.4f})\")\n",
    "        else:\n",
    "            print(\"Warning: Could not calculate F1 scores for threshold optimization. Using default 0.5.\")\n",
    "    else:\n",
    "        print(\"Warning: No thresholds found from precision_recall_curve. Using default 0.5.\")\n",
    "else:\n",
    "    print(\"Warning: OOF predictions not generated for threshold optimization. Using default 0.5.\")\n",
    "\n",
    "\n",
    "# --- STEP 6: FINAL MODEL TRAINING ---\n",
    "print(\"\\nStep 6: Training final model on all data with optimized parameters & SMOTE...\")\n",
    "final_smote_k_neighbors_tuned = best_params.get('smote_k_neighbors', 3)\n",
    "final_minority_count_full = np.sum(global_final_y == 1)\n",
    "current_k_for_smote_final = final_smote_k_neighbors_tuned\n",
    "X_train_final_aug, y_train_final_aug = global_final_X_scaled_np, global_final_y\n",
    "\n",
    "if final_minority_count_full > 0 :\n",
    "    if final_minority_count_full <= current_k_for_smote_final:\n",
    "        current_k_for_smote_final = max(1, final_minority_count_full - 1) if final_minority_count_full > 1 else 1\n",
    "        print(f\"Adjusting SMOTE k_neighbors for final training to {current_k_for_smote_final}\")\n",
    "    if current_k_for_smote_final > 0:\n",
    "        smote_final = SMOTE(random_state=RANDOM_STATE, k_neighbors=current_k_for_smote_final)\n",
    "        try: X_train_final_aug, y_train_final_aug = smote_final.fit_resample(global_final_X_scaled_np, global_final_y)\n",
    "        except ValueError as e_smote_final: print(f\"SMOTE failed for final model: {e_smote_final}. Using original data.\")\n",
    "    else: print(\"Warning: k_neighbors for SMOTE is <=0. Using original data.\")\n",
    "else: print(\"Warning: No minority samples. SMOTE not applied.\")\n",
    "print(f\"Original full train shape: {global_final_X_scaled_np.shape}, Augmented full train shape: {X_train_final_aug.shape}\")\n",
    "\n",
    "final_rf = best_rf_oof # Reuse model instances from OOF thresholding step, already configured with best_params\n",
    "final_xgb = best_xgb_oof\n",
    "final_cat = best_cat_oof\n",
    "final_lgbm = best_lgbm_oof\n",
    "final_ensemble_weights = best_ensemble_oof_weights # Reuse weights\n",
    "\n",
    "final_ensemble = VotingClassifier(\n",
    "    estimators=[('rf', final_rf), ('xgb', final_xgb), ('cat', final_cat), ('lgbm', final_lgbm)],\n",
    "    voting='soft', weights=final_ensemble_weights, n_jobs=-1)\n",
    "\n",
    "final_ensemble.fit(X_train_final_aug, y_train_final_aug)\n",
    "model_components = {\n",
    "    'scaler': global_final_scaler, 'ensemble_model': final_ensemble,\n",
    "    'final_features': final_features, 'best_optuna_params': best_params,\n",
    "    'optimized_threshold': optimized_fixed_threshold\n",
    "}\n",
    "model_path = os.path.join(output_dir, 'champion_model_bundle_optuna_smote_enhanced.pkl')\n",
    "joblib.dump(model_components, model_path)\n",
    "print(f\"Final model bundle saved to: {model_path}\")\n",
    "\n",
    "\n",
    "# --- STEP 7: GENERATE TEST PREDICTIONS ---\n",
    "print(\"\\nStep 7: Generating optimized test predictions...\")\n",
    "X_test_final_df = pd.DataFrame(columns=global_final_X_df.columns, index=test_df_fe.index)\n",
    "for col in global_final_X_df.columns:\n",
    "    X_test_final_df[col] = test_df_fe[col] if col in test_df_fe.columns else 0\n",
    "for col in X_test_final_df.columns: # Impute using TRAIN median\n",
    "    if X_test_final_df[col].isnull().any():\n",
    "        X_test_final_df[col] = X_test_final_df[col].fillna(global_final_X_df[col].median())\n",
    "X_test_final_df = X_test_final_df.fillna(0) # Final safety net\n",
    "\n",
    "X_test_scaled_np = global_final_scaler.transform(X_test_final_df)\n",
    "test_proba = final_ensemble.predict_proba(X_test_scaled_np)[:, 1]\n",
    "test_proba_path = os.path.join(output_dir, 'test_probabilities_optuna_smote_enhanced.npy')\n",
    "np.save(test_proba_path, test_proba)\n",
    "print(f\"Test probabilities saved to: {test_proba_path}\")\n",
    "\n",
    "# Dynamic thresholding (using optimized fixed threshold as a better placeholder)\n",
    "print(\"Applying dynamic thresholds (using OOF-optimized fixed threshold as example)...\")\n",
    "# True dynamic thresholding requires a strategy to vary the threshold per instance/agent.\n",
    "# Examples:\n",
    "# 1. Segment agents (e.g., by tenure, past performance) and find optimal thresholds for each segment on CV OOF data.\n",
    "# 2. Train a small model to predict an optimal threshold for each test instance based on its features.\n",
    "# For this script, the OOF-optimized fixed threshold is used.\n",
    "dynamic_threshold_to_apply = optimized_fixed_threshold\n",
    "# Ensure test_df_fe has row_id if it was lost (it should have it from initial load)\n",
    "if 'row_id' not in test_df_fe.columns and 'row_id' in test_df.columns:\n",
    "    test_df_fe = test_df_fe.reset_index().merge(test_df[['row_id']].reset_index(drop=True), left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "test_df_fe['probability'] = test_proba # Add probability to test_df_fe for submission mapping\n",
    "test_df_fe['dynamic_prediction'] = (test_df_fe['probability'] >= dynamic_threshold_to_apply).astype(int)\n",
    "\n",
    "dynamic_submission = submission_template.copy()\n",
    "if 'row_id' in test_df_fe.columns and 'row_id' in dynamic_submission.columns:\n",
    "    # Ensure test_df_fe['row_id'] is compatible with submission_template['row_id']\n",
    "    # This merge is safer if test_df_fe was re-indexed or row order changed.\n",
    "    final_preds_df_dynamic = test_df_fe[['row_id', 'dynamic_prediction']].rename(columns={'dynamic_prediction': 'target_column'})\n",
    "    dynamic_submission = dynamic_submission.drop(columns=['target_column'], errors='ignore').merge(final_preds_df_dynamic, on='row_id', how='left')\n",
    "    dynamic_submission['target_column'] = dynamic_submission['target_column'].fillna(0).astype(int) # Fill if any row_id mismatch\n",
    "else: # Fallback if row_id mapping is problematic\n",
    "    print(\"Warning: row_id not available for precise mapping in dynamic submission. Using direct assignment.\")\n",
    "    dynamic_submission['target_column'] = test_df_fe['dynamic_prediction'].values[:len(dynamic_submission)]\n",
    "\n",
    "dynamic_submission_path = os.path.join(output_dir, 'dynamic_submission_optuna_smote_enhanced.csv')\n",
    "dynamic_submission.to_csv(dynamic_submission_path, index=False)\n",
    "print(f\"Dynamic threshold submission saved to: {dynamic_submission_path}\")\n",
    "\n",
    "\n",
    "# Fixed threshold submission (using the OOF optimized threshold)\n",
    "best_fixed_threshold = optimized_fixed_threshold\n",
    "optimal_predictions = (test_proba >= best_fixed_threshold).astype(int)\n",
    "optimal_submission = submission_template.copy()\n",
    "\n",
    "if 'row_id' in test_df_fe.columns and 'row_id' in optimal_submission.columns:\n",
    "    temp_preds_fixed = pd.DataFrame({'row_id': test_df_fe['row_id'], 'target_column': optimal_predictions})\n",
    "    optimal_submission = optimal_submission.drop(columns=['target_column'], errors='ignore').merge(temp_preds_fixed, on='row_id', how='left')\n",
    "    optimal_submission['target_column'] = optimal_submission['target_column'].fillna(0).astype(int)\n",
    "else:\n",
    "    print(\"Warning: row_id not available for precise mapping in optimal submission. Using direct assignment.\")\n",
    "    optimal_submission['target_column'] = optimal_predictions[:len(optimal_submission)]\n",
    "\n",
    "optimal_submission_path = os.path.join(output_dir, 'submission_optuna_smote_enhanced.csv') # Main submission\n",
    "optimal_submission.to_csv(optimal_submission_path, index=False)\n",
    "print(f\"Optimal fixed threshold submission saved to: {optimal_submission_path}\")\n",
    "\n",
    "\n",
    "# --- STEP 8: FEATURE IMPORTANCE ANALYSIS ---\n",
    "print(\"\\nStep 8: Feature importance analysis...\")\n",
    "if hasattr(final_ensemble, 'named_estimators_'):\n",
    "    importances_data = {'Feature': global_final_X_df.columns.tolist()}\n",
    "    estimators_with_importance = {\n",
    "        'RF': final_ensemble.named_estimators_.get('rf'),\n",
    "        'XGB': final_ensemble.named_estimators_.get('xgb'),\n",
    "        'CAT': final_ensemble.named_estimators_.get('cat'),\n",
    "        'LGBM': final_ensemble.named_estimators_.get('lgbm')\n",
    "    }\n",
    "    for model_name, model_obj in estimators_with_importance.items():\n",
    "        if model_obj and hasattr(model_obj, 'feature_importances_'):\n",
    "            if len(model_obj.feature_importances_) == len(global_final_X_df.columns):\n",
    "                importances_data[f'{model_name}_Importance'] = model_obj.feature_importances_\n",
    "            else:\n",
    "                print(f\"Warning: Mismatch in feature count for {model_name} importances.\")\n",
    "\n",
    "    feature_importance_df = pd.DataFrame(importances_data)\n",
    "    importance_cols = [col for col in feature_importance_df.columns if '_Importance' in col]\n",
    "    if importance_cols:\n",
    "        feature_importance_df['Avg_Importance'] = feature_importance_df[importance_cols].mean(axis=1)\n",
    "        feature_importance_df = feature_importance_df.sort_values('Avg_Importance', ascending=False).reset_index(drop=True)\n",
    "        feature_importance_path = os.path.join(output_dir, 'feature_importance_optuna_smote_enhanced.csv')\n",
    "        feature_importance_df.to_csv(feature_importance_path, index=False)\n",
    "        print(f\"Feature importance table saved to: {feature_importance_path}\")\n",
    "\n",
    "        plt.figure(figsize=(12, max(8, min(len(feature_importance_df), 30) // 1.5))) # Adjusted figure size\n",
    "        sns.barplot(x='Avg_Importance', y='Feature', data=feature_importance_df.head(min(30, len(feature_importance_df))), palette=\"viridis\")\n",
    "        plt.title(f'Top {min(30, len(feature_importance_df))} Features by Average Importance (Enhanced Model)')\n",
    "        plt.tight_layout()\n",
    "        feature_plot_path = os.path.join(output_dir, 'top_features_optuna_smote_enhanced.png')\n",
    "        plt.savefig(feature_plot_path); plt.close()\n",
    "        print(f\"Feature importance plot saved to: {feature_plot_path}\")\n",
    "    else: print(\"No feature importances could be extracted.\")\n",
    "else: print(\"Final ensemble model structure issue for feature importances.\")\n",
    "\n",
    "\n",
    "# --- STEP 9: ERROR ANALYSIS (Conceptual on OOF) ---\n",
    "print(\"\\nStep 9: Error Analysis (Conceptual)...\")\n",
    "# This section provides a template for analyzing misclassifications from OOF predictions.\n",
    "# It requires 'oof_true_labels_np', 'oof_probas_np', and 'all_val_indices' from Step 5.5.\n",
    "\n",
    "# if len(oof_probas_np) > 0 and len(oof_true_labels_np) == len(oof_probas_np) and len(all_val_indices) == len(oof_true_labels_np):\n",
    "#     oof_preds_for_analysis = (oof_probas_np >= optimized_fixed_threshold).astype(int)\n",
    "#\n",
    "#     # Get original indices from train_df_fe that correspond to the OOF predictions\n",
    "#     # This assumes all_val_indices were collected in the same order as oof_true_labels_np/oof_probas_np\n",
    "#     oof_original_indices = train_df_fe.iloc[all_val_indices].index\n",
    "#\n",
    "#     misclassified_mask = (oof_preds_for_analysis != oof_true_labels_np)\n",
    "#     misclassified_original_indices = oof_original_indices[misclassified_mask]\n",
    "#\n",
    "#     if not misclassified_original_indices.empty:\n",
    "#         print(f\"Analyzing {len(misclassified_original_indices)} misclassified samples from OOF...\")\n",
    "#         misclassified_samples_df = train_df_fe.loc[misclassified_original_indices].copy()\n",
    "#         misclassified_samples_df['oof_probability'] = oof_probas_np[misclassified_mask]\n",
    "#         misclassified_samples_df['oof_prediction'] = oof_preds_for_analysis[misclassified_mask]\n",
    "#         misclassified_samples_df['true_target_actual'] = oof_true_labels_np[misclassified_mask] # Renamed to avoid conflict if 'target_column' exists\n",
    "#\n",
    "#         fp_df = misclassified_samples_df[(misclassified_samples_df['true_target_actual'] == 0) & (misclassified_samples_df['oof_prediction'] == 1)]\n",
    "#         print(f\"False Positives ({len(fp_df)}):\")\n",
    "#         # print(fp_df[['agent_code', 'year_month', 'oof_probability'] + final_features[:3]].head())\n",
    "#\n",
    "#         fn_df = misclassified_samples_df[(misclassified_samples_df['true_target_actual'] == 1) & (misclassified_samples_df['oof_prediction'] == 0)]\n",
    "#         print(f\"False Negatives ({len(fn_df)}):\")\n",
    "#         # print(fn_df[['agent_code', 'year_month', 'oof_probability'] + final_features[:3]].head())\n",
    "#\n",
    "#         # Example: Save misclassified samples for external review\n",
    "#         # misclassified_samples_df.to_csv(os.path.join(output_dir, 'misclassified_oof_samples.csv'), index=False)\n",
    "#     else:\n",
    "#         print(\"No misclassified samples found in OOF data with the optimized threshold.\")\n",
    "# else:\n",
    "#     print(\"OOF data not available for error analysis.\")\n",
    "print(\"Error analysis section is conceptual. Uncomment and adapt as needed with OOF data.\")\n",
    "\n",
    "\n",
    "# --- SCRIPT COMPLETION ---\n",
    "end_time_script = time.time()\n",
    "elapsed_time_script = end_time_script - start_time_script\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"CHAMPIONSHIP SOLUTION (ENHANCED) completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total execution time: {elapsed_time_script:.2f} seconds ({elapsed_time_script/60:.2f} minutes)\")\n",
    "print(f\"OPTIMAL SUBMISSION (ENHANCED): {optimal_submission_path}\")\n",
    "print(f\"DYNAMIC SUBMISSION (ENHANCED): {dynamic_submission_path}\")\n",
    "print(f\"All outputs saved in: {output_dir}\")\n",
    "print(\"=\" * 100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "CHAMPIONSHIP KAGGLE SOLUTION - ADVANCED ENSEMBLE WITH OPTUNA HPO & SMOTE - ENHANCED\n",
      "====================================================================================================\n",
      "Starting at: 2025-05-07 22:04:32\n",
      "\n",
      "Step 1: Loading data...\n",
      "Train data shape: (15308, 23)\n",
      "Test data shape: (914, 23)\n",
      "Submission template shape: (914, 2)\n",
      "Performing data integrity checks...\n",
      "\n",
      "Step 2: Enhanced preprocessing with domain expertise...\n",
      "Processing target for 905 unique agents...\n",
      "Processed training data shape after target creation: (14403, 24)\n",
      "Target distribution:\n",
      "target_column\n",
      "1    0.900437\n",
      "0    0.099563\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Step 3: Advanced feature engineering with agent profiling...\n",
      "Train data shape after FE: (14403, 166)\n",
      "Test data shape after FE: (914, 165)\n",
      "\n",
      "Step 4: Feature selection with RFECV...\n",
      "Performing RFECV from 159 potential features...\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
